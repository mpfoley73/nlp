# Sentiment Analysis {#sentimentanalysis}

Sentiment analysis is the extraction of the emotional intent of text. You can classify the *polarity* (positive | negative) or sentiment (angry | sad | happy | ...) at the document, sentence, or feature level. This section continues with the hotel data from \@ref(data-prep).

```{r include=FALSE}
library(tidyverse)
library(tidymodels)
library(sentimentr)
library(tidytext)
library(scales)
library(janitor)
```

```{r}
load("input/hotel_prepped.Rdata")

hotel_0 <- prepped_hotel

glimpse(token)
```

## Subjectivity Lexicons

A subjectivity lexicon is a predefined list of words associated with emotional context such as positive/negative. Subjectivity lexicons are typically short (a few thousand words), but work because of Zipf's law which holds that the nth-ranked item in a frequency table has a frequency count equal to 1/n of the top-ranked item. So infrequently used words are used *very* infrequently.

There are three common sentiment lexicons. **Bing** is common for polarity scoring, **AFINN** for emotion classification. **NRC** is a less common option for emotion classification.

Bing classifies words as *positive* or *negative*.

```{r}
bing <- tidytext::get_sentiments("bing") %>%
  # remove dups
  filter(!word %in% c("envious", "enviously", "enviousness"))

bing %>% 
  count(sentiment) %>% 
  adorn_totals() %>%
  flextable::flextable() %>% 
  flextable::autofit()
```

AFINN, by Finn Arup Nielsen, associates words with a manually rated valence integer between -5 (negative) and +5 (positive).

```{r}
afinn <- tidytext::get_sentiments("afinn")

afinn %>%
  count(value) %>% 
  adorn_totals() %>%
  flextable::flextable() %>% 
  flextable::autofit()
```

NRC lexicon associates words with eight emotions corresponding to the second level of [Plutchik's Wheel of Emotions](https://www.6seconds.org/2020/08/11/plutchik-wheel-emotions/) and two sentiments (negative and positive). NRC was created by manual annotation on a crowd sourcing platform (see [this](https://nrc.canada.ca/en/research-development/products-services/technical-advisory-services/sentiment-emotion-lexicons)).

```{r}
nrc <- tidytext::get_sentiments("nrc")

nrc %>%
  count(sentiment) %>% 
  adorn_totals() %>%
  flextable::flextable() %>% 
  flextable::autofit()
```

## Polarity Scoring

Two packages measure text polarity. The simpler one **tidytext**: unnest tokens, join to the Bing lexicon, and calculate the net of positive minus negative polarity counts. **sentimentr** is more sophisticated in that it takes into account *valence shifters*, surrounding words that change the intensity of a sentiment (e.g., "very") or switch its direction (e.g., "not").^[There is a third package called **qdap**, but the **sentimentr** [Read Me](https://cran.r-project.org/web/packages/sentimentr/readme/README.html) explains **sentimentr** is an improved version that better balances accuracy and speed.] 

### tidytext {-}

The **tidytext** way to score polarity is to tag words as "positive" and "negative" using the Bing lexicon, then calculate the difference in counts. The **qdap** and **sentimentr** packages correct for text length by dividing by $\sqrt{n}$. It is useful to capture the positive and negative words back in the main data frame for explaining how the polarity score was calculated.

```{r}
polarity_bing <- 
  token %>%
  left_join(bing, by = "word", relationship = "many-to-one") %>%
  summarize(.by = c(review_id, sentiment), n = n(), words = list(word)) %>%
  pivot_wider(names_from = sentiment, values_from = c(n, words), 
              values_fill = list(n = 0)) %>%
  select(-c(n_NA, words_NA)) %>%
  inner_join(hotel_0 %>% select(review_id, prepped_wrdcnt), by = "review_id") %>%
  mutate(
    polarity = (n_positive - n_negative) / sqrt(prepped_wrdcnt),
    polarity_desc = if_else(polarity >= 0, "Positive", "Negative")
  )

polarity_afinn <- 
  token %>%
  inner_join(afinn, by = "word", relationship = "many-to-one") %>%
  summarize(.by = review_id, sentiment = sum(value), words = list(word)) %>%
  inner_join(hotel_0 %>% select(review_id, prepped_wrdcnt), by = "review_id") %>%
  mutate(
    polarity = sentiment / sqrt(prepped_wrdcnt),
    polarity_desc = if_else(polarity >= 0, "Positive", "Negative")
  )

# Attach to main data frame
hotel_1 <-
  hotel_0 %>%
  left_join(polarity_bing %>% 
              select(review_id, polarity, words_positive, words_negative) %>%
              rename_with(~paste0("bing_", .x)),
            by = join_by(review_id == bing_review_id)) %>%
  left_join(polarity_afinn %>% 
              select(review_id, polarity) %>%
              rename_with(~paste0("afinn_", .x)),
            by = join_by(review_id == afinn_review_id))
```

Let's see how the polarity scores compare.

```{r}
hotel_1 %>%
  pivot_longer(cols = c(bing_polarity, afinn_polarity), 
               names_to = "lexicon", values_to = "polarity") %>%
  filter(!is.na(polarity)) %>%
  ggplot(aes(x = polarity, y = fct_rev(hotel))) +
  geom_boxplot() +
  facet_wrap(facets = vars(lexicon)) +
  labs(title = "Review polarity", x = NULL, y = NULL,
       caption = glue::glue("Bing Polarity = (n_pos - n_neg) / sqrt(n_words)\n",
                      "AFINN Polarity = sentiment / sqrt(n_words)"))
```

The two lexicons are similar. The data set includes a rating (1-5). I'll stick with Bing going forward for convenience. The polarity score should correlate with the rating.

```{r}
hotel_1 %>% 
  filter(!is.na(bing_polarity)) %>%
  ggplot(aes(x = as_factor(rating), y = bing_polarity)) +
  geom_jitter(width = 0.2, alpha = 0.3, color = "#5DA5DA", size = 1) +
  geom_boxplot(alpha = 0) +
  theme_minimal() +
  labs(title = "Polarity is associated with overall Likert score",
       x = "Overall Likert Rating", y = "Polarity Score")
```

Sentiment increases with Likert rating, but there are many reviews with a rating of 5 and a polarity score <0. In some cases this is because the reviewer interpreted the scale incorrectly. You can use polarity scores to identify problematic reviews like these.

```{r}
hotel_1 %>%
  mutate(
    problematic = case_when(
      (rating == "1-2" & bing_polarity > 0.5) ~ "Too Low",
      (rating == "5" & bing_polarity < -.5) ~ "Too High",
      TRUE ~ "Other"
    )
  ) %>%
  filter(problematic %in% c("Too High", "Too Low")) %>%
  group_by(problematic) %>%
  slice_max(order_by = abs(bing_polarity), n = 1) %>%
  select(problematic, rating, bing_polarity, review) %>%
  flextable::flextable() %>% 
  flextable::autofit() %>%
  flextable::valign(valign = "top")
```

The polarity words can help explain why some hotels rated poor or excellent.

```{r}
token %>%
  inner_join(hotel_1 %>% filter(rating %in% c("1-2", "5")), by = join_by(review_id)) %>%
  filter(!word %in% c("hotel", "stay", "night")) %>%
  filter((rating == "5" & bing_polarity > 0) | 
         (rating == "1-2" & bing_polarity < 0)) %>%
  count(rating, word) %>%
  mutate(.by = rating, pct = n / sum(n)) %>%
  group_by(rating) %>%
  slice_max(order_by = pct, n = 10) %>%
  ggplot(aes(x = pct, y = reorder_within(word, by = pct, within = rating))) +
  geom_col() +
  scale_y_reordered() +
  scale_x_continuous(labels = percent_format(1)) +
  labs(y = NULL, x = NULL) +
  facet_wrap(facets = vars(rating), scales = "free_y")
```

Word clouds are a nice way to get an overview of the data.

```{r fig.height=5}
token %>%
  inner_join(hotel_1 %>% filter(rating %in% c("1-2", "5")), by = join_by(review_id)) %>%
  filter(!word %in% c("hotel", "stay", "night", "london"),
         !is.na(bing_polarity)) %>%
  mutate(polarity_desc = if_else(bing_polarity > 0, "Positive", "Negative")) %>%
  count(word, polarity_desc, wt = prepped_wrdcnt) %>%
  pivot_wider(names_from = polarity_desc, values_from = n, values_fill = 0) %>%
  data.table::data.table() %>%
  as.matrix(rownames = "word") %>%
  wordcloud::comparison.cloud(max.words = 30, title.size = 1.5, scale = c(1, 3.5))
```

### sentimentr {-}

**sentimentr** calculates polarity at the sentence level. It improves on **tidytext** in that it takes into account the context in which the sentiment words occur by incorporating *valence shifters*. 

* A *negator* flips the direction of a polarizing word (e.g., "I do ***not*** like it."). `lexicon::hash_valence_shifters[y==1]`. 
* An *amplifier* intensifies the impact (e.g., "I ***really*** like it."). `lexicon::hash_valence_shifters[y==2]`. 
* A *de-amplifier* (downtoner) reduces the impact (e.g., "I **hardly** like it."). `lexicon::hash_valence_shifters[y==3]`. 
* An *adversative conjunction* overrules the previous clause containing a polarized word (e.g., "I like it ***but*** it's not worth it."). `lexicon::hash_valence_shifters[y==4]`.

**sentimentr** uses a lexicon package combined from the **syuzhet** and **lexicon** packages. Positive words are scored +1 and negative words are scored -1. **sentimentr** identifies clusters of words within sentences of the text. The 4 words before and 2 words after are candidate valence shifters. Polarized words are weighted by the valence shifter weights: negators = -1; amplifiers and de-amplifiers = 1.8; adversative conjunctions decrease the value of the prior cluster and increase the value of the following cluster. Neutral words hold no value, but do affect the word count. 

```{r}
hotel_sentimentr <- 
  sentimentr::get_sentences(hotel_1$review) %>%
  sentimentr::sentiment() %>% 
  summarize(.by = element_id, sentimentr_polarity = mean(sentiment))

hotel_2 <-
  hotel_1 %>%
  mutate(element_id = row_number()) %>%
  inner_join(hotel_sentimentr, by = join_by(element_id)) %>%
  select(-element_id)
```

Let's see a few examples where **sentimentr** differed from **tidytext**. Looks like **bing** did a better job on the first one, but **sentimentr** was better on the next two.

```{r}
hotel_2 %>%
  filter((bing_polarity > 0.2 & sentimentr_polarity < -0.2) | 
           (bing_polarity < -0.2 & sentimentr_polarity > 0.2)) %>%
  select(review, bing_polarity, sentimentr_polarity) %>%
  head(3) %>%
  flextable::flextable() %>%
  flextable::autofit()
```

## Statistical Test

You can fit an ordinal logistic regression model to predict the rating based on the review sentiment. Which performs better, **tidytext** or **sentimentr**? Start with an intercept-only model for a baseline and review of ordinal logistic regression.

```{r}
fit_intercept <- ordinal::clm(rating ~ 1, data = hotel_2)

summary(fit_intercept)
```

The threshold coefficients in the summary table are the log-odds of the outcome variable having a level at or below vs above. Below, 10.3% of ratings were <=3 and 89.7% were >3 for a log-odds of log(.103/.897) = `r log(0.10320745/0.8967926)`, corresponding to the `3|4` line in the regression summary.

```{r}
hotel_2 %>% tabyl(rating) %>% mutate(cum = cumsum(percent), `1-cum` = 1 - cum)
```

Now fit the **bing** and **sentimentr** models. The **bing** model has the higher -2 * log-likelihood.

```{r}
fit_bing <- ordinal::clm(rating ~ bing_polarity, data = hotel_2)

fit_sentimentr <- ordinal::clm(rating ~ sentimentr_polarity, data = hotel_2)

anova(fit_bing, fit_sentimentr, fit_intercept)
```

How about predictive performance? They performed about the same.

```{r warning=FALSE}
bing_conf <- 
  fit_bing %>% 
  augment(type = "class") %>% 
  conf_mat(truth = rating, estimate = .fitted)

sentimentr_conf <- 
  fit_sentimentr %>% 
  augment(type = "class") %>% 
  conf_mat(truth = rating, estimate = .fitted)

bind_rows(
  bing = as_tibble(bing_conf$table),
  sentimentr = as_tibble(sentimentr_conf$table), 
  .id = "lexicon"
) %>%
  pivot_wider(names_from = Truth, values_from = n) %>%
  flextable::flextable() %>%
  flextable::merge_v(j = 1) %>%
  flextable::valign(j = 1, valign = "top")

bind_rows(
  bing = summary(bing_conf),
  sentimentr = summary(sentimentr_conf),
  .id = "lexicon"
) %>%
  pivot_wider(names_from = lexicon, values_from = .estimate)
```

