<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.1 LDA | Natural Language Processing in R</title>
  <meta name="description" content="Background and tutorial on natural language processing in R (topic modeling, sentiment analysis) using R." />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="2.1 LDA | Natural Language Processing in R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Background and tutorial on natural language processing in R (topic modeling, sentiment analysis) using R." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.1 LDA | Natural Language Processing in R" />
  
  <meta name="twitter:description" content="Background and tutorial on natural language processing in R (topic modeling, sentiment analysis) using R." />
  

<meta name="author" content="Michael Foley" />


<meta name="date" content="2023-12-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="topicmodeling.html"/>
<link rel="next" href="stm.html"/>
<script src="assets/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="assets/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="assets/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="assets/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="assets/tabwid-1.1.3/tabwid.css" rel="stylesheet" />
<script src="assets/tabwid-1.1.3/tabwid.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Intro</a></li>
<li class="chapter" data-level="1" data-path="data-prep.html"><a href="data-prep.html"><i class="fa fa-check"></i><b>1</b> Data Preparation</a>
<ul>
<li class="chapter" data-level="1.1" data-path="scrub.html"><a href="scrub.html"><i class="fa fa-check"></i><b>1.1</b> Scrub</a></li>
<li class="chapter" data-level="1.2" data-path="tokenize.html"><a href="tokenize.html"><i class="fa fa-check"></i><b>1.2</b> Tokenize</a></li>
<li class="chapter" data-level="1.3" data-path="spell-check.html"><a href="spell-check.html"><i class="fa fa-check"></i><b>1.3</b> Spell-check</a></li>
<li class="chapter" data-level="1.4" data-path="remove-stop-words.html"><a href="remove-stop-words.html"><i class="fa fa-check"></i><b>1.4</b> Remove Stop Words</a></li>
<li class="chapter" data-level="1.5" data-path="lemmatize.html"><a href="lemmatize.html"><i class="fa fa-check"></i><b>1.5</b> Lemmatize</a></li>
<li class="chapter" data-level="1.6" data-path="prepped-data.html"><a href="prepped-data.html"><i class="fa fa-check"></i><b>1.6</b> Prepped Data</a></li>
<li class="chapter" data-level="1.7" data-path="bigrams.html"><a href="bigrams.html"><i class="fa fa-check"></i><b>1.7</b> Bigrams</a></li>
<li class="chapter" data-level="1.8" data-path="save.html"><a href="save.html"><i class="fa fa-check"></i><b>1.8</b> Save</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="topicmodeling.html"><a href="topicmodeling.html"><i class="fa fa-check"></i><b>2</b> Topic Modeling</a>
<ul>
<li class="chapter" data-level="2.1" data-path="lda.html"><a href="lda.html"><i class="fa fa-check"></i><b>2.1</b> LDA</a>
<ul>
<li class="chapter" data-level="" data-path="lda.html"><a href="lda.html#preprocessing"><i class="fa fa-check"></i>Preprocessing</a></li>
<li class="chapter" data-level="" data-path="lda.html"><a href="lda.html#fit"><i class="fa fa-check"></i>Fit</a></li>
<li class="chapter" data-level="" data-path="lda.html"><a href="lda.html#topic-labeling-with-chatgpt"><i class="fa fa-check"></i>Topic Labeling with ChatGPT</a></li>
<li class="chapter" data-level="" data-path="lda.html"><a href="lda.html#todo"><i class="fa fa-check"></i>TODO</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="stm.html"><a href="stm.html"><i class="fa fa-check"></i><b>2.2</b> STM</a>
<ul>
<li class="chapter" data-level="" data-path="stm.html"><a href="stm.html#algorithm"><i class="fa fa-check"></i>Algorithm</a></li>
<li class="chapter" data-level="" data-path="stm.html"><a href="stm.html#data-preparation"><i class="fa fa-check"></i>Data Preparation</a></li>
<li class="chapter" data-level="" data-path="stm.html"><a href="stm.html#fit-1"><i class="fa fa-check"></i>Fit</a></li>
<li class="chapter" data-level="" data-path="stm.html"><a href="stm.html#interpret"><i class="fa fa-check"></i>Interpret</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="sentimentanalysis.html"><a href="sentimentanalysis.html"><i class="fa fa-check"></i><b>3</b> Sentiment Analysis</a>
<ul>
<li class="chapter" data-level="3.1" data-path="subjectivity-lexicons.html"><a href="subjectivity-lexicons.html"><i class="fa fa-check"></i><b>3.1</b> Subjectivity Lexicons</a></li>
<li class="chapter" data-level="3.2" data-path="polarity-scoring.html"><a href="polarity-scoring.html"><i class="fa fa-check"></i><b>3.2</b> Polarity Scoring</a>
<ul>
<li class="chapter" data-level="" data-path="polarity-scoring.html"><a href="polarity-scoring.html#tidytext"><i class="fa fa-check"></i>tidytext</a></li>
<li class="chapter" data-level="" data-path="polarity-scoring.html"><a href="polarity-scoring.html#sentimentr"><i class="fa fa-check"></i>sentimentr</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="statistical-test.html"><a href="statistical-test.html"><i class="fa fa-check"></i><b>3.3</b> Statistical Test</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Natural Language Processing in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lda" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> LDA<a href="lda.html#lda" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Latent Dirichlet allocation (LDA) is one of a family of mixed membership models that decompose data into latent components. <em>Latent</em> means <em>unidentified</em> topics and <em>Dirichlet</em> is the distribution followed by the words in topics and by topics in documents.</p>
<p>LDA presumes each document is created by a generative process in which topics are selected from a probability distribution and then words from that topic are selected from another distribution. LDA is an optimization algorithm to estimate those distributions by performing a random search through the parameter space to find the model with the largest log-likelihood. There are multiple search algorithms, but the preferred one appears to be Gibbs sampling, a type of Monte Carlo Markov Chain (MCMC) algorithm. The <a href="https://www.mygreatlearning.com/blog/understanding-latent-dirichlet-allocation/">algorithm</a> is:</p>
<ol style="list-style-type: decimal">
<li><p>For each document <span class="math inline">\(d = 1 \ldots D\)</span>, randomly assign each word <span class="math inline">\(w = 1 \ldots W\)</span> to one of <span class="math inline">\(k = 1\ldots K\)</span> topics.</p></li>
<li><p>Tabulate the number of words in each document and topic, a <span class="math inline">\(D \times K\)</span> matrix, and tabulate the number of occurrences of each word in each document, a <span class="math inline">\(W \times D\)</span> matrix.</p></li>
<li><p>Resample to remove a single instance of a word from the corpus, decrementing the document’s topic count and the word’s topic count.</p></li>
<li><p>Calculate the gamma matrix, <span class="math inline">\(\gamma\)</span>, and the beta matrix, <span class="math inline">\(\beta\)</span>.</p>
<ul>
<li>the gamma matrix (aka theta) is the topical prevalence, the probability distribution of topics for each document, <span class="math display">\[p(k|d) = \frac{n_{dk} + \alpha}{N_i + K \alpha}\]</span> were <span class="math inline">\(n_{dk}\)</span> is the number of words in document <span class="math inline">\(d\)</span> for topic <span class="math inline">\(k\)</span>, <span class="math inline">\(N_d\)</span> is the total number of words in <span class="math inline">\(d\)</span>, and <span class="math inline">\(\alpha\)</span> is a hyperparameter. For each <span class="math inline">\(d\)</span>, <span class="math inline">\(\sum_{k \in K} \gamma_{dk} = 1\)</span>.</li>
<li>the beta matrix (aka phi), is the topical content, the probability distribution of words for each topic, <span class="math display">\[p(w|k) = \frac{m_{w,k} + \beta}{\sum_{w \in W}m_{d,k} + W\beta}\]</span> where <span class="math inline">\(m_{w,k}\)</span> is the corpus-wide frequency count of word <span class="math inline">\(w\)</span> to topic <span class="math inline">\(k\)</span>, <span class="math inline">\(W\)</span> is the number of distinct words in the corpus, and <span class="math inline">\(\beta\)</span> is a hyperparameter. For each <span class="math inline">\(k\)</span>, <span class="math inline">\(\sum_{w \in W} \beta_{kw} = 1\)</span>.</li>
</ul></li>
<li><p>Calculate the joint probability distribution of words for each document and topic, <span class="math inline">\(p(w|k,d) = p(k|d)p(w|k)\)</span>. Assign each word, <span class="math inline">\(w\)</span>, to the topic with the maximum joint probability.</p></li>
<li><p>Repeat steps 3-6 for all of the words in all of the documents.</p></li>
<li><p>Repeat steps 3-7 for a pre-determined number of iterations.</p></li>
</ol>
<p>LDA thus has 3 hyperparameters: the document-topic density factor, <span class="math inline">\(\alpha\)</span>, the topic-word density factor, <span class="math inline">\(\beta\)</span>, and the topic count, <span class="math inline">\(K\)</span>. <span class="math inline">\(\alpha\)</span> controls the number of topics expected per document (large <span class="math inline">\(\alpha\)</span> = more topics). <span class="math inline">\(\beta\)</span> controls the distribution of words per topic (large <span class="math inline">\(\beta\)</span> = more words). Ideally, you want a few topics per document and a few words per topic, so <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are typically set below one. <span class="math inline">\(K\)</span> is set using a combination of domain knowledge, coherence, and exclusivity.</p>
<p>Notice that LDA is a “bag of words” method. It does not consider the order of the tokens in the text, so where tokens are located what other tokens are nearby do not factor into the output.</p>
<div id="preprocessing" class="section level3 unnumbered hasAnchor">
<h3>Preprocessing<a href="lda.html#preprocessing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We already cleaned the text in Chapter <a href="data-prep.html#data-prep">1</a>. The next step is to create a document-term matrix (DTM). A DTM has one row per per document, one column per term, and the cells are frequencies. The DTM contains mostly unhelpful infrequently used terms, so the pre-processing step removes sparse terms.</p>
<p>Keep only the decent sized reviews, ones with at least 25 words. If this is a predictive model, now is the time to create a train/test split. Consider weighting the split by the outcome variable (<code>rating</code> in this case) to ensure proportional coverage.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="lda.html#cb22-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb22-2"><a href="lda.html#cb22-2" tabindex="-1"></a></span>
<span id="cb22-3"><a href="lda.html#cb22-3" tabindex="-1"></a>hotel_lda <span class="ot">&lt;-</span> prepped_hotel <span class="sc">%&gt;%</span> <span class="fu">filter</span>(prepped_wrdcnt <span class="sc">&gt;=</span> <span class="dv">25</span>)</span>
<span id="cb22-4"><a href="lda.html#cb22-4" tabindex="-1"></a></span>
<span id="cb22-5"><a href="lda.html#cb22-5" tabindex="-1"></a><span class="co"># Parameter `strata` ensures proportional coverage of ratings.</span></span>
<span id="cb22-6"><a href="lda.html#cb22-6" tabindex="-1"></a>lda_split <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">initial_split</span>(hotel_lda, <span class="at">prop =</span> <span class="dv">3</span><span class="sc">/</span><span class="dv">4</span>, <span class="at">strata =</span> rating)</span>
<span id="cb22-7"><a href="lda.html#cb22-7" tabindex="-1"></a></span>
<span id="cb22-8"><a href="lda.html#cb22-8" tabindex="-1"></a>lda_train <span class="ot">&lt;-</span> token <span class="sc">%&gt;%</span> <span class="fu">semi_join</span>(<span class="fu">training</span>(lda_split), <span class="at">by =</span> <span class="fu">join_by</span>(review_id))</span>
<span id="cb22-9"><a href="lda.html#cb22-9" tabindex="-1"></a></span>
<span id="cb22-10"><a href="lda.html#cb22-10" tabindex="-1"></a>lda_test <span class="ot">&lt;-</span> token <span class="sc">%&gt;%</span> <span class="fu">semi_join</span>(<span class="fu">testing</span>(lda_split), <span class="at">by =</span> <span class="fu">join_by</span>(review_id))</span></code></pre></div>
<p>Most words add little value to a topic model because they appear infrequently or too frequently. Including them only wastes computing resources. The most common metric for removing sparse terms is the term frequency-inverse document frequency (TF-IDF). TF(t,d) is term t’s usage proportion in document d. IDF(t) the log of the inverse of the term t’s proportion of documents it appears in. For example, “savoy” appears in n = 1,873 of the N = 12,121 training documents. Its IDF is log(N/n) = 1.87. “savoy” appears in review #5 in 2 of 30 (6.67%) of the terms. The TF-IDF score is the product of the two numbers. Here is that prepped review.</p>
<blockquote>
<p>form moment arrive leave experience absolute perfection service excellence savoy staff famous personalise service rich heritage savoy win hotel world stay eat dine woud highly recommend kaspers restaurant wait return</p>
</blockquote>
<p><span class="citation">Nagelkerke (<a href="#ref-Nagelkerke2020b">2020b</a>)</span> suggests another route. You’ve already removed stop words, so you needn’t worry about the over-used words. The TF-IDF approach was developed for long documents. Smaller documents like online reviews have little TF variation (most words are used only once or twice in a review), and the IDF ends up dominating. Nagelkerke suggests using the overall word frequency instead.</p>
<p>In the end, you need to experiment to find the right cutoff. The elbow in the TD-IDF plot below is around .2. Using that as a threshold would throw out about about 90% of the vocabulary. The corpus frequency plot after that has an elbow around 5 occurrences. Using that as a threshold would retain about 20% of the vocabulary.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="lda.html#cb23-1" tabindex="-1"></a>lda_word_stats <span class="ot">&lt;-</span></span>
<span id="cb23-2"><a href="lda.html#cb23-2" tabindex="-1"></a>  lda_train <span class="sc">%&gt;%</span> </span>
<span id="cb23-3"><a href="lda.html#cb23-3" tabindex="-1"></a>  <span class="fu">count</span>(review_id, word, <span class="at">name =</span> <span class="st">&quot;doc_freq&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb23-4"><a href="lda.html#cb23-4" tabindex="-1"></a>  <span class="fu">bind_tf_idf</span>(word, review_id, doc_freq) <span class="sc">%&gt;%</span></span>
<span id="cb23-5"><a href="lda.html#cb23-5" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">.by =</span> word, <span class="at">corp_freq =</span> <span class="fu">sum</span>(doc_freq)) <span class="sc">%&gt;%</span></span>
<span id="cb23-6"><a href="lda.html#cb23-6" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">corp_pct =</span> corp_freq <span class="sc">/</span> <span class="fu">sum</span>(doc_freq))</span>
<span id="cb23-7"><a href="lda.html#cb23-7" tabindex="-1"></a></span>
<span id="cb23-8"><a href="lda.html#cb23-8" tabindex="-1"></a>lda_word_stats <span class="sc">%&gt;%</span> </span>
<span id="cb23-9"><a href="lda.html#cb23-9" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">tf_idf_bin =</span> <span class="fu">cut</span>(tf_idf, <span class="at">breaks =</span> <span class="dv">50</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb23-10"><a href="lda.html#cb23-10" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">.by =</span> tf_idf_bin, <span class="at">vocab =</span> <span class="fu">n_distinct</span>(word)) <span class="sc">%&gt;%</span></span>
<span id="cb23-11"><a href="lda.html#cb23-11" tabindex="-1"></a>  <span class="fu">arrange</span>(tf_idf_bin) <span class="sc">%&gt;%</span></span>
<span id="cb23-12"><a href="lda.html#cb23-12" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">pct =</span> vocab <span class="sc">/</span> <span class="fu">sum</span>(vocab), <span class="at">cumpct =</span> <span class="fu">cumsum</span>(pct)) <span class="sc">%&gt;%</span></span>
<span id="cb23-13"><a href="lda.html#cb23-13" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> tf_idf_bin)) <span class="sc">+</span> </span>
<span id="cb23-14"><a href="lda.html#cb23-14" tabindex="-1"></a>  <span class="fu">geom_col</span>(<span class="fu">aes</span>(<span class="at">y =</span> pct)) <span class="sc">+</span></span>
<span id="cb23-15"><a href="lda.html#cb23-15" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> cumpct, <span class="at">group =</span> <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb23-16"><a href="lda.html#cb23-16" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">9</span>, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb23-17"><a href="lda.html#cb23-17" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;vocabulary&quot;</span>, <span class="at">title =</span> <span class="st">&quot;TF-IDF Method&quot;</span>) <span class="sc">+</span></span>
<span id="cb23-18"><a href="lda.html#cb23-18" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">breaks =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, .<span class="dv">1</span>), <span class="at">labels =</span> <span class="fu">percent_format</span>(<span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb23-19"><a href="lda.html#cb23-19" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">90</span>, <span class="at">vjust =</span> .<span class="dv">5</span>))</span></code></pre></div>
<p><img src="nlp_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="lda.html#cb24-1" tabindex="-1"></a>lda_word_stats <span class="sc">%&gt;%</span> </span>
<span id="cb24-2"><a href="lda.html#cb24-2" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb24-3"><a href="lda.html#cb24-3" tabindex="-1"></a>    <span class="at">corp_freq_bin =</span> <span class="fu">if_else</span>(corp_freq <span class="sc">&gt;</span> <span class="dv">19</span>, <span class="st">&quot;20+&quot;</span>, <span class="fu">as.character</span>(corp_freq)),</span>
<span id="cb24-4"><a href="lda.html#cb24-4" tabindex="-1"></a>    <span class="at">corp_freq_bin =</span> <span class="fu">factor</span>(corp_freq_bin, <span class="at">levels =</span> <span class="fu">c</span>(<span class="fu">as.character</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">19</span>), <span class="st">&quot;20+&quot;</span>))</span>
<span id="cb24-5"><a href="lda.html#cb24-5" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb24-6"><a href="lda.html#cb24-6" tabindex="-1"></a>  <span class="co"># mutate(corp_pct_bin = cut(corp_pct, breaks = 100)) %&gt;%</span></span>
<span id="cb24-7"><a href="lda.html#cb24-7" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">.by =</span> corp_freq_bin, <span class="at">vocab =</span> <span class="fu">n_distinct</span>(word)) <span class="sc">%&gt;%</span></span>
<span id="cb24-8"><a href="lda.html#cb24-8" tabindex="-1"></a>  <span class="fu">arrange</span>(corp_freq_bin) <span class="sc">%&gt;%</span></span>
<span id="cb24-9"><a href="lda.html#cb24-9" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">pct =</span> vocab <span class="sc">/</span> <span class="fu">sum</span>(vocab), <span class="at">cumpct =</span> <span class="fu">cumsum</span>(pct)) <span class="sc">%&gt;%</span></span>
<span id="cb24-10"><a href="lda.html#cb24-10" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> corp_freq_bin, <span class="at">y =</span> cumpct)) <span class="sc">+</span> </span>
<span id="cb24-11"><a href="lda.html#cb24-11" tabindex="-1"></a>  <span class="fu">geom_col</span>(<span class="fu">aes</span>(<span class="at">y =</span> pct)) <span class="sc">+</span></span>
<span id="cb24-12"><a href="lda.html#cb24-12" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> cumpct, <span class="at">group =</span> <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb24-13"><a href="lda.html#cb24-13" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">5</span>, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb24-14"><a href="lda.html#cb24-14" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;vocabulary&quot;</span>, <span class="at">title =</span> <span class="st">&quot;Corpus Frequency Method&quot;</span>) <span class="sc">+</span></span>
<span id="cb24-15"><a href="lda.html#cb24-15" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">breaks =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, .<span class="dv">1</span>), <span class="at">labels =</span> <span class="fu">percent_format</span>(<span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb24-16"><a href="lda.html#cb24-16" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">90</span>, <span class="at">vjust =</span> .<span class="dv">5</span>))</span></code></pre></div>
<p><img src="nlp_files/figure-html/unnamed-chunk-16-2.png" width="672" /></p>
<p>Below is the DTM associated with both methods. The TF-IDF method removed half the documents while retaining 11K terms. The corpus frequency method retained all of the documents while reducing the vocabulary to 6K terms. Corpus frequency does indeed seem superior in this case, so I’ll move forward with it.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="lda.html#cb25-1" tabindex="-1"></a>(lda_dtm_tfidf <span class="ot">&lt;-</span></span>
<span id="cb25-2"><a href="lda.html#cb25-2" tabindex="-1"></a>  lda_word_stats <span class="sc">%&gt;%</span></span>
<span id="cb25-3"><a href="lda.html#cb25-3" tabindex="-1"></a>  <span class="fu">filter</span>(tf_idf <span class="sc">&gt;</span> .<span class="dv">2</span>) <span class="sc">%&gt;%</span></span>
<span id="cb25-4"><a href="lda.html#cb25-4" tabindex="-1"></a>  <span class="fu">cast_dtm</span>(<span class="at">document =</span> review_id, <span class="at">term =</span> word, <span class="at">value =</span> doc_freq))</span>
<span id="cb25-5"><a href="lda.html#cb25-5" tabindex="-1"></a><span class="do">## &lt;&lt;DocumentTermMatrix (documents: 5623, terms: 10922)&gt;&gt;</span></span>
<span id="cb25-6"><a href="lda.html#cb25-6" tabindex="-1"></a><span class="do">## Non-/sparse entries: 15782/61398624</span></span>
<span id="cb25-7"><a href="lda.html#cb25-7" tabindex="-1"></a><span class="do">## Sparsity           : 100%</span></span>
<span id="cb25-8"><a href="lda.html#cb25-8" tabindex="-1"></a><span class="do">## Maximal term length: 37</span></span>
<span id="cb25-9"><a href="lda.html#cb25-9" tabindex="-1"></a><span class="do">## Weighting          : term frequency (tf)</span></span>
<span id="cb25-10"><a href="lda.html#cb25-10" tabindex="-1"></a></span>
<span id="cb25-11"><a href="lda.html#cb25-11" tabindex="-1"></a>(lda_dtm_corpfreq <span class="ot">&lt;-</span></span>
<span id="cb25-12"><a href="lda.html#cb25-12" tabindex="-1"></a>  lda_word_stats <span class="sc">%&gt;%</span></span>
<span id="cb25-13"><a href="lda.html#cb25-13" tabindex="-1"></a>  <span class="fu">filter</span>(corp_freq <span class="sc">&gt;</span> <span class="dv">5</span>) <span class="sc">%&gt;%</span></span>
<span id="cb25-14"><a href="lda.html#cb25-14" tabindex="-1"></a>  <span class="fu">cast_dtm</span>(<span class="at">document =</span> review_id, <span class="at">term =</span> word, <span class="at">value =</span> doc_freq))</span>
<span id="cb25-15"><a href="lda.html#cb25-15" tabindex="-1"></a><span class="do">## &lt;&lt;DocumentTermMatrix (documents: 12121, terms: 5821)&gt;&gt;</span></span>
<span id="cb25-16"><a href="lda.html#cb25-16" tabindex="-1"></a><span class="do">## Non-/sparse entries: 606488/69949853</span></span>
<span id="cb25-17"><a href="lda.html#cb25-17" tabindex="-1"></a><span class="do">## Sparsity           : 99%</span></span>
<span id="cb25-18"><a href="lda.html#cb25-18" tabindex="-1"></a><span class="do">## Maximal term length: 16</span></span>
<span id="cb25-19"><a href="lda.html#cb25-19" tabindex="-1"></a><span class="do">## Weighting          : term frequency (tf)</span></span>
<span id="cb25-20"><a href="lda.html#cb25-20" tabindex="-1"></a></span>
<span id="cb25-21"><a href="lda.html#cb25-21" tabindex="-1"></a>lda_dtm <span class="ot">&lt;-</span> lda_dtm_corpfreq</span></code></pre></div>
<p>The pre-processing step sure pared down the corpus. The high frequency terms comprise only 20% of the vocabulary, but are still 95% of the total word usage.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="lda.html#cb26-1" tabindex="-1"></a><span class="fu">bind_rows</span>(</span>
<span id="cb26-2"><a href="lda.html#cb26-2" tabindex="-1"></a>  <span class="st">`</span><span class="at">high freq words</span><span class="st">`</span> <span class="ot">=</span> lda_word_stats <span class="sc">%&gt;%</span> <span class="fu">filter</span>(corp_freq <span class="sc">&gt;</span> <span class="dv">5</span>) <span class="sc">%&gt;%</span></span>
<span id="cb26-3"><a href="lda.html#cb26-3" tabindex="-1"></a>    <span class="fu">summarize</span>(<span class="at">distinct_words =</span> <span class="fu">n_distinct</span>(word), <span class="at">total_words =</span> <span class="fu">sum</span>(doc_freq)),</span>
<span id="cb26-4"><a href="lda.html#cb26-4" tabindex="-1"></a>  <span class="st">`</span><span class="at">low freq words</span><span class="st">`</span> <span class="ot">=</span> lda_word_stats <span class="sc">%&gt;%</span> <span class="fu">filter</span>(corp_freq <span class="sc">&lt;=</span> <span class="dv">5</span>) <span class="sc">%&gt;%</span></span>
<span id="cb26-5"><a href="lda.html#cb26-5" tabindex="-1"></a>    <span class="fu">summarize</span>(<span class="at">distinct_words =</span> <span class="fu">n_distinct</span>(word), <span class="at">total_words =</span> <span class="fu">sum</span>(doc_freq)),</span>
<span id="cb26-6"><a href="lda.html#cb26-6" tabindex="-1"></a>  <span class="at">.id =</span> <span class="st">&quot;partition&quot;</span></span>
<span id="cb26-7"><a href="lda.html#cb26-7" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb26-8"><a href="lda.html#cb26-8" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">total_pct =</span> total_words <span class="sc">/</span> <span class="fu">sum</span>(total_words) <span class="sc">*</span> <span class="dv">100</span>, </span>
<span id="cb26-9"><a href="lda.html#cb26-9" tabindex="-1"></a>         <span class="at">distinct_pct =</span> distinct_words <span class="sc">/</span> <span class="fu">sum</span>(distinct_words) <span class="sc">*</span> <span class="dv">100</span>) <span class="sc">%&gt;%</span></span>
<span id="cb26-10"><a href="lda.html#cb26-10" tabindex="-1"></a>  <span class="fu">select</span>(partition, distinct_words, distinct_pct, total_words, total_pct) <span class="sc">%&gt;%</span></span>
<span id="cb26-11"><a href="lda.html#cb26-11" tabindex="-1"></a>  janitor<span class="sc">::</span><span class="fu">adorn_totals</span>()</span></code></pre></div>
<pre><code>##        partition distinct_words distinct_pct total_words  total_pct
##  high freq words           5821     20.18377      723170  95.396707
##   low freq words          23019     79.81623       34896   4.603293
##            Total          28840    100.00000      758066 100.000000</code></pre>
</div>
<div id="fit" class="section level3 unnumbered hasAnchor">
<h3>Fit<a href="lda.html#fit" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are several parameters you might tweak for the model fit. The biggest surprise is that you set the number of topics, <code>k</code>. In general, you only want as many topics as are clearly distinct and that you can easily communicate to others. <a href="https://knowledger.rbind.io/post/topic-modeling-using-r/">knowledgeR</a> explains the harmonic mean method for optimization, but <span class="citation">Nagelkerke (<a href="#ref-Nagelkerke2020b">2020b</a>)</span> suggests sticking with the art vs science approach and picking your own <code>k</code>.</p>
<div class="rmdnote">
<p>Another option is to use the perplexity statistic to help identify a good value for <code>k</code>. Perplexity is a measure of how well a probability model fits a new set of data. Look for the elbow in a scree plot. The code below does that, but it runs forever, especially as you get to higher values of k. I abandoned it.</p>
</div>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="lda.html#cb28-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb28-2"><a href="lda.html#cb28-2" tabindex="-1"></a></span>
<span id="cb28-3"><a href="lda.html#cb28-3" tabindex="-1"></a>train_ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(lda_dtm), <span class="fu">floor</span>(<span class="fl">0.75</span><span class="sc">*</span><span class="fu">nrow</span>(lda_dtm)))</span>
<span id="cb28-4"><a href="lda.html#cb28-4" tabindex="-1"></a>k_train <span class="ot">&lt;-</span> lda_dtm[train_ind, ]</span>
<span id="cb28-5"><a href="lda.html#cb28-5" tabindex="-1"></a>k_test <span class="ot">&lt;-</span> lda_dtm[<span class="sc">-</span>train_ind, ]</span>
<span id="cb28-6"><a href="lda.html#cb28-6" tabindex="-1"></a></span>
<span id="cb28-7"><a href="lda.html#cb28-7" tabindex="-1"></a>k <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">2</span>, <span class="at">to =</span> <span class="dv">5</span>, <span class="at">by =</span> <span class="dv">1</span>))</span>
<span id="cb28-8"><a href="lda.html#cb28-8" tabindex="-1"></a></span>
<span id="cb28-9"><a href="lda.html#cb28-9" tabindex="-1"></a>perp <span class="ot">&lt;-</span> k <span class="sc">%&gt;%</span> </span>
<span id="cb28-10"><a href="lda.html#cb28-10" tabindex="-1"></a>  <span class="fu">map</span>(<span class="sc">~</span> <span class="fu">LDA</span>(k_train, <span class="at">k =</span> .x)) <span class="sc">%&gt;%</span></span>
<span id="cb28-11"><a href="lda.html#cb28-11" tabindex="-1"></a>  <span class="fu">map</span>(<span class="sc">~</span> <span class="fu">perplexity</span>(.x, <span class="at">newdata =</span> k_test)) <span class="sc">%&gt;%</span></span>
<span id="cb28-12"><a href="lda.html#cb28-12" tabindex="-1"></a>  <span class="fu">as.numeric</span>()</span>
<span id="cb28-13"><a href="lda.html#cb28-13" tabindex="-1"></a></span>
<span id="cb28-14"><a href="lda.html#cb28-14" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">k =</span> k, <span class="at">perplexity =</span> perp) <span class="sc">%&gt;%</span></span>
<span id="cb28-15"><a href="lda.html#cb28-15" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> k, <span class="at">y =</span> perplexity)) <span class="sc">+</span> </span>
<span id="cb28-16"><a href="lda.html#cb28-16" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb28-17"><a href="lda.html#cb28-17" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;loess&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb28-18"><a href="lda.html#cb28-18" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Perplexity Plot for LDM model&quot;</span>)</span></code></pre></div>
<p>I will stick with a simple k = 3 model.</p>
<div class="rmdnote">
<p>The model fit took a minute to run, so I ran it once then saved the result.</p>
</div>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="lda.html#cb29-1" tabindex="-1"></a>lda_fit <span class="ot">&lt;-</span> <span class="fu">LDA</span>(lda_dtm, <span class="at">k =</span> <span class="dv">3</span>)</span>
<span id="cb29-2"><a href="lda.html#cb29-2" tabindex="-1"></a></span>
<span id="cb29-3"><a href="lda.html#cb29-3" tabindex="-1"></a><span class="fu">saveRDS</span>(lda_fit, <span class="at">file =</span> <span class="st">&quot;input/lda_fit.RDS&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="lda.html#cb30-1" tabindex="-1"></a>lda_fit <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="at">file =</span> <span class="st">&quot;input/lda_fit.RDS&quot;</span>)</span></code></pre></div>
<p>The fitted object contains two matrices. The <strong>beta</strong> (aka “phi”) matrix is the distribution of tokens (cols) over topics (rows). The <strong>gamma</strong> (aka “theta”) matrix is the distribution of documents (rows) over topics (cols). The row sum is 1 for each matrix (sum of topic probabilities, some of document probabilities).</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="lda.html#cb31-1" tabindex="-1"></a>lda_beta_mtrx <span class="ot">&lt;-</span> <span class="fu">posterior</span>(lda_fit) <span class="sc">%&gt;%</span> <span class="fu">pluck</span>(<span class="st">&quot;terms&quot;</span>) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb31-2"><a href="lda.html#cb31-2" tabindex="-1"></a></span>
<span id="cb31-3"><a href="lda.html#cb31-3" tabindex="-1"></a><span class="co"># One row per topic, one col per token.</span></span>
<span id="cb31-4"><a href="lda.html#cb31-4" tabindex="-1"></a><span class="fu">dim</span>(lda_beta_mtrx)</span>
<span id="cb31-5"><a href="lda.html#cb31-5" tabindex="-1"></a><span class="do">## [1]    3 5821</span></span>
<span id="cb31-6"><a href="lda.html#cb31-6" tabindex="-1"></a></span>
<span id="cb31-7"><a href="lda.html#cb31-7" tabindex="-1"></a><span class="co"># Word probability distribution sums to 1 for each topic.</span></span>
<span id="cb31-8"><a href="lda.html#cb31-8" tabindex="-1"></a><span class="fu">sum</span>(lda_beta_mtrx[<span class="dv">1</span>, ])</span>
<span id="cb31-9"><a href="lda.html#cb31-9" tabindex="-1"></a><span class="do">## [1] 1</span></span>
<span id="cb31-10"><a href="lda.html#cb31-10" tabindex="-1"></a></span>
<span id="cb31-11"><a href="lda.html#cb31-11" tabindex="-1"></a>lda_gamma_mtrx <span class="ot">&lt;-</span> <span class="fu">posterior</span>(lda_fit) <span class="sc">%&gt;%</span> <span class="fu">pluck</span>(<span class="st">&quot;topics&quot;</span>) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb31-12"><a href="lda.html#cb31-12" tabindex="-1"></a></span>
<span id="cb31-13"><a href="lda.html#cb31-13" tabindex="-1"></a><span class="co"># One row per document, one col per topic</span></span>
<span id="cb31-14"><a href="lda.html#cb31-14" tabindex="-1"></a><span class="fu">dim</span>(lda_gamma_mtrx)</span>
<span id="cb31-15"><a href="lda.html#cb31-15" tabindex="-1"></a><span class="do">## [1] 12121     3</span></span>
<span id="cb31-16"><a href="lda.html#cb31-16" tabindex="-1"></a></span>
<span id="cb31-17"><a href="lda.html#cb31-17" tabindex="-1"></a><span class="co"># Topic probability distribution sums to 1 for each document.</span></span>
<span id="cb31-18"><a href="lda.html#cb31-18" tabindex="-1"></a><span class="fu">sum</span>(lda_gamma_mtrx[<span class="dv">1</span>, ])</span>
<span id="cb31-19"><a href="lda.html#cb31-19" tabindex="-1"></a><span class="do">## [1] 1</span></span></code></pre></div>
<p><code>tidytext::tidy()</code> pivots the beta matrix into a [topic, term, beta] data frame.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="lda.html#cb32-1" tabindex="-1"></a>lda_beta <span class="ot">&lt;-</span> <span class="fu">tidy</span>(lda_fit, <span class="at">matrix =</span> <span class="st">&quot;beta&quot;</span>)</span>
<span id="cb32-2"><a href="lda.html#cb32-2" tabindex="-1"></a></span>
<span id="cb32-3"><a href="lda.html#cb32-3" tabindex="-1"></a>lda_top_tokens <span class="ot">&lt;-</span> </span>
<span id="cb32-4"><a href="lda.html#cb32-4" tabindex="-1"></a>  lda_beta <span class="sc">%&gt;%</span></span>
<span id="cb32-5"><a href="lda.html#cb32-5" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">topic =</span> <span class="fu">factor</span>(<span class="fu">paste</span>(<span class="st">&quot;Topic&quot;</span>, topic))) <span class="sc">%&gt;%</span></span>
<span id="cb32-6"><a href="lda.html#cb32-6" tabindex="-1"></a>  <span class="fu">group_by</span>(topic) <span class="sc">%&gt;%</span></span>
<span id="cb32-7"><a href="lda.html#cb32-7" tabindex="-1"></a>  <span class="fu">slice_max</span>(<span class="at">order_by =</span> beta, <span class="at">n =</span> <span class="dv">10</span>) <span class="sc">%&gt;%</span></span>
<span id="cb32-8"><a href="lda.html#cb32-8" tabindex="-1"></a>  <span class="fu">ungroup</span>()</span>
<span id="cb32-9"><a href="lda.html#cb32-9" tabindex="-1"></a></span>
<span id="cb32-10"><a href="lda.html#cb32-10" tabindex="-1"></a>lda_top_tokens <span class="sc">%&gt;%</span></span>
<span id="cb32-11"><a href="lda.html#cb32-11" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> beta, <span class="at">y =</span> <span class="fu">reorder_within</span>(term, <span class="at">by =</span> beta, <span class="at">within =</span> topic))) <span class="sc">+</span></span>
<span id="cb32-12"><a href="lda.html#cb32-12" tabindex="-1"></a>  <span class="fu">geom_col</span>() <span class="sc">+</span></span>
<span id="cb32-13"><a href="lda.html#cb32-13" tabindex="-1"></a>  <span class="fu">scale_y_reordered</span>() <span class="sc">+</span></span>
<span id="cb32-14"><a href="lda.html#cb32-14" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(topic), <span class="at">scales =</span> <span class="st">&quot;free_y&quot;</span>) <span class="sc">+</span></span>
<span id="cb32-15"><a href="lda.html#cb32-15" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="cn">NULL</span>, <span class="at">title =</span> <span class="st">&quot;LDA Top 10 Terms&quot;</span>)</span></code></pre></div>
<p><img src="nlp_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>Word clouds tell you more or less the same thing.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="lda.html#cb33-1" tabindex="-1"></a>colors6 <span class="ot">&lt;-</span> RColorBrewer<span class="sc">::</span><span class="fu">brewer.pal</span>(<span class="at">n =</span> <span class="dv">3</span>, <span class="at">name =</span> <span class="st">&quot;Set2&quot;</span>)</span>
<span id="cb33-2"><a href="lda.html#cb33-2" tabindex="-1"></a></span>
<span id="cb33-3"><a href="lda.html#cb33-3" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">map</span>(</span>
<span id="cb33-4"><a href="lda.html#cb33-4" tabindex="-1"></a>  <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>), </span>
<span id="cb33-5"><a href="lda.html#cb33-5" tabindex="-1"></a>  <span class="sc">~</span> <span class="fu">with</span>(lda_beta <span class="sc">%&gt;%</span> <span class="fu">filter</span>(topic <span class="sc">==</span> .x), </span>
<span id="cb33-6"><a href="lda.html#cb33-6" tabindex="-1"></a>         wordcloud<span class="sc">::</span><span class="fu">wordcloud</span>(</span>
<span id="cb33-7"><a href="lda.html#cb33-7" tabindex="-1"></a>           term, </span>
<span id="cb33-8"><a href="lda.html#cb33-8" tabindex="-1"></a>           beta, </span>
<span id="cb33-9"><a href="lda.html#cb33-9" tabindex="-1"></a>           <span class="at">max.words =</span> <span class="dv">20</span>,</span>
<span id="cb33-10"><a href="lda.html#cb33-10" tabindex="-1"></a>           <span class="at">colors =</span> colors6[.x]</span>
<span id="cb33-11"><a href="lda.html#cb33-11" tabindex="-1"></a>         ))</span>
<span id="cb33-12"><a href="lda.html#cb33-12" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="nlp_files/figure-html/unnamed-chunk-24-1.png" width="672" /><img src="nlp_files/figure-html/unnamed-chunk-24-2.png" width="672" /><img src="nlp_files/figure-html/unnamed-chunk-24-3.png" width="672" /></p>
<p>There is a downside to this evaluation. Popular words like “room” and “stay” appear at or near the top in all three topics. You might want to look at <em>relative</em> popularity instead: the popularity within the topic divided by overall popularity. That’s problematic too because words that only appear in few reviews will pop to the top. What you want is a combination of both absolute term probability and relative term probability. <code>LDAvis::serVis()</code> can help you do that.</p>
<div class="rmdnote">
<p>Unfortunately, the plot from <code>LDAvis::serVis()</code> is interactive and does not render in the RMarkdown notebook html, so below is just a screenshot of the code chunk output.</p>
</div>
<p>The left side shows the topic sizes (documents) and topic distances. The right side shows the most important tokens.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="lda.html#cb34-1" tabindex="-1"></a><span class="co"># word count for each document</span></span>
<span id="cb34-2"><a href="lda.html#cb34-2" tabindex="-1"></a>doc_length <span class="ot">&lt;-</span> lda_word_stats <span class="sc">%&gt;%</span> <span class="fu">filter</span>(corp_freq <span class="sc">&gt;</span> <span class="dv">5</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb34-3"><a href="lda.html#cb34-3" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">.by =</span> review_id, <span class="at">n =</span> <span class="fu">sum</span>(doc_freq)) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(n)</span>
<span id="cb34-4"><a href="lda.html#cb34-4" tabindex="-1"></a></span>
<span id="cb34-5"><a href="lda.html#cb34-5" tabindex="-1"></a><span class="co"># vocabulary: unique tokens</span></span>
<span id="cb34-6"><a href="lda.html#cb34-6" tabindex="-1"></a>vocab <span class="ot">&lt;-</span> <span class="fu">colnames</span>(lda_beta_mtrx) </span>
<span id="cb34-7"><a href="lda.html#cb34-7" tabindex="-1"></a></span>
<span id="cb34-8"><a href="lda.html#cb34-8" tabindex="-1"></a><span class="co"># overall token frequency</span></span>
<span id="cb34-9"><a href="lda.html#cb34-9" tabindex="-1"></a>term_frequency <span class="ot">&lt;-</span> lda_word_stats <span class="sc">%&gt;%</span> <span class="fu">filter</span>(corp_freq <span class="sc">&gt;</span> <span class="dv">5</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb34-10"><a href="lda.html#cb34-10" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">.by =</span> word, <span class="at">n =</span> <span class="fu">sum</span>(doc_freq)) <span class="sc">%&gt;%</span> <span class="fu">arrange</span>(<span class="fu">match</span>(word, vocab)) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(n) </span>
<span id="cb34-11"><a href="lda.html#cb34-11" tabindex="-1"></a></span>
<span id="cb34-12"><a href="lda.html#cb34-12" tabindex="-1"></a><span class="co"># create JSON containing all needed elements</span></span>
<span id="cb34-13"><a href="lda.html#cb34-13" tabindex="-1"></a>json <span class="ot">&lt;-</span> LDAvis<span class="sc">::</span><span class="fu">createJSON</span>(lda_beta_mtrx, lda_gamma_mtrx, doc_length, vocab, term_frequency)</span>
<span id="cb34-14"><a href="lda.html#cb34-14" tabindex="-1"></a></span>
<span id="cb34-15"><a href="lda.html#cb34-15" tabindex="-1"></a>LDAvis<span class="sc">::</span><span class="fu">serVis</span>(json)</span></code></pre></div>
<p><img src="images/ldavis.png" /></p>
<p>The gamma matrix shows topic distributions. You can use it to see if topics vary by a covariate.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="lda.html#cb35-1" tabindex="-1"></a><span class="fu">tidy</span>(lda_fit, <span class="at">matrix =</span> <span class="st">&quot;gamma&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb35-2"><a href="lda.html#cb35-2" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">document =</span> <span class="fu">as.numeric</span>(document), <span class="at">topic =</span> <span class="fu">factor</span>(topic)) <span class="sc">%&gt;%</span></span>
<span id="cb35-3"><a href="lda.html#cb35-3" tabindex="-1"></a>  <span class="fu">inner_join</span>(prepped_hotel, <span class="at">by =</span> <span class="fu">join_by</span>(document <span class="sc">==</span> review_id)) <span class="sc">%&gt;%</span></span>
<span id="cb35-4"><a href="lda.html#cb35-4" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">.by =</span> <span class="fu">c</span>(hotel, topic), <span class="at">gamma =</span> <span class="fu">mean</span>(gamma)) <span class="sc">%&gt;%</span></span>
<span id="cb35-5"><a href="lda.html#cb35-5" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> gamma, <span class="at">y =</span> <span class="fu">fct_rev</span>(hotel), <span class="at">fill =</span> topic)) <span class="sc">+</span></span>
<span id="cb35-6"><a href="lda.html#cb35-6" tabindex="-1"></a>  <span class="fu">geom_col</span>() <span class="sc">+</span></span>
<span id="cb35-7"><a href="lda.html#cb35-7" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="cn">NULL</span>, <span class="at">title =</span> <span class="st">&quot;LDA Topic Distribution&quot;</span>)</span></code></pre></div>
<p><img src="nlp_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>Iterate through the model by tweaking <code>k</code>, excluding words that dominate and suppress the more interesting subdomains, and/or changing the minimal token frequency to focus on more/less dominant tokens. You can also change the document sampling strategy to promote interesting domains, like we did when we over sampled the low hotel ratings.</p>
</div>
<div id="topic-labeling-with-chatgpt" class="section level3 unnumbered hasAnchor">
<h3>Topic Labeling with ChatGPT<a href="lda.html#topic-labeling-with-chatgpt" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="rmdnote">
<p>To OpenAI’s ChatGPT API service requires an API token. I created one at <a href="https://platform.openai.com/api-keys" class="uri">https://platform.openai.com/api-keys</a> and saved it to .Renviron. See <code>usethis::edit_r_environ()</code>.</p>
</div>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="lda.html#cb36-1" tabindex="-1"></a><span class="co"># Create a function to send each list of topic words to Open AI as a separate request.</span></span>
<span id="cb36-2"><a href="lda.html#cb36-2" tabindex="-1"></a>get_topic_from_openai <span class="ot">&lt;-</span> <span class="cf">function</span>(prompt) {</span>
<span id="cb36-3"><a href="lda.html#cb36-3" tabindex="-1"></a>  my_resp <span class="ot">&lt;-</span></span>
<span id="cb36-4"><a href="lda.html#cb36-4" tabindex="-1"></a>    <span class="fu">request</span>(<span class="st">&quot;https://api.openai.com/v1/chat/completions&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb36-5"><a href="lda.html#cb36-5" tabindex="-1"></a>    <span class="fu">req_headers</span>(<span class="at">Authorization =</span> <span class="fu">paste</span>(<span class="st">&quot;Bearer&quot;</span>, <span class="fu">Sys.getenv</span>(<span class="st">&quot;OPENAI_API_KEY&quot;</span>))) <span class="sc">%&gt;%</span></span>
<span id="cb36-6"><a href="lda.html#cb36-6" tabindex="-1"></a>    <span class="fu">req_body_json</span>(<span class="fu">list</span>(</span>
<span id="cb36-7"><a href="lda.html#cb36-7" tabindex="-1"></a>      <span class="at">model =</span> <span class="st">&quot;gpt-3.5-turbo&quot;</span>,</span>
<span id="cb36-8"><a href="lda.html#cb36-8" tabindex="-1"></a>      <span class="co"># Temperature [0,2] controls creativity, predictable -&gt; variable. </span></span>
<span id="cb36-9"><a href="lda.html#cb36-9" tabindex="-1"></a>      <span class="at">temperature =</span> <span class="dv">1</span>,</span>
<span id="cb36-10"><a href="lda.html#cb36-10" tabindex="-1"></a>      <span class="at">messages =</span> <span class="fu">list</span>(</span>
<span id="cb36-11"><a href="lda.html#cb36-11" tabindex="-1"></a>        <span class="co"># System prompt sets repeated context. It is prefixed to prompts.</span></span>
<span id="cb36-12"><a href="lda.html#cb36-12" tabindex="-1"></a>        <span class="fu">list</span>(</span>
<span id="cb36-13"><a href="lda.html#cb36-13" tabindex="-1"></a>          <span class="at">role =</span> <span class="st">&quot;system&quot;</span>,</span>
<span id="cb36-14"><a href="lda.html#cb36-14" tabindex="-1"></a>          <span class="at">content =</span> <span class="fu">paste</span>(<span class="st">&quot;You are a topic modeling assistant. You accept lists &quot;</span>,</span>
<span id="cb36-15"><a href="lda.html#cb36-15" tabindex="-1"></a>                          <span class="st">&quot;of words in a topic and summarizes them into a salient &quot;</span>,</span>
<span id="cb36-16"><a href="lda.html#cb36-16" tabindex="-1"></a>                          <span class="st">&quot;topic label of five words or less. How would you &quot;</span>,</span>
<span id="cb36-17"><a href="lda.html#cb36-17" tabindex="-1"></a>                          <span class="st">&quot;summarize the following list? The list is in descending &quot;</span>,</span>
<span id="cb36-18"><a href="lda.html#cb36-18" tabindex="-1"></a>                          <span class="st">&quot;order of importance, so the first term in the list is most &quot;</span>,</span>
<span id="cb36-19"><a href="lda.html#cb36-19" tabindex="-1"></a>                          <span class="st">&quot;strongly tied to the topic. Return just the topic label and &quot;</span>,</span>
<span id="cb36-20"><a href="lda.html#cb36-20" tabindex="-1"></a>                          <span class="st">&quot;nothing else.&quot;</span>)</span>
<span id="cb36-21"><a href="lda.html#cb36-21" tabindex="-1"></a>        ),</span>
<span id="cb36-22"><a href="lda.html#cb36-22" tabindex="-1"></a>        <span class="fu">list</span>(</span>
<span id="cb36-23"><a href="lda.html#cb36-23" tabindex="-1"></a>          <span class="at">role =</span> <span class="st">&quot;user&quot;</span>,</span>
<span id="cb36-24"><a href="lda.html#cb36-24" tabindex="-1"></a>          <span class="at">content =</span> prompt</span>
<span id="cb36-25"><a href="lda.html#cb36-25" tabindex="-1"></a>        )</span>
<span id="cb36-26"><a href="lda.html#cb36-26" tabindex="-1"></a>      )</span>
<span id="cb36-27"><a href="lda.html#cb36-27" tabindex="-1"></a>    )) <span class="sc">%&gt;%</span></span>
<span id="cb36-28"><a href="lda.html#cb36-28" tabindex="-1"></a>    <span class="fu">req_perform</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb36-29"><a href="lda.html#cb36-29" tabindex="-1"></a>    <span class="fu">resp_body_json</span>() <span class="sc">%&gt;%</span></span>
<span id="cb36-30"><a href="lda.html#cb36-30" tabindex="-1"></a>    <span class="fu">pluck</span>(<span class="st">&quot;choices&quot;</span>, <span class="dv">1</span>, <span class="st">&quot;message&quot;</span>, <span class="st">&quot;content&quot;</span>)</span>
<span id="cb36-31"><a href="lda.html#cb36-31" tabindex="-1"></a>}</span>
<span id="cb36-32"><a href="lda.html#cb36-32" tabindex="-1"></a></span>
<span id="cb36-33"><a href="lda.html#cb36-33" tabindex="-1"></a>lda_topics <span class="ot">&lt;-</span> </span>
<span id="cb36-34"><a href="lda.html#cb36-34" tabindex="-1"></a>  lda_top_tokens <span class="sc">%&gt;%</span></span>
<span id="cb36-35"><a href="lda.html#cb36-35" tabindex="-1"></a>  <span class="fu">nest</span>(<span class="at">data =</span> term, <span class="at">.by =</span> topic) <span class="sc">%&gt;%</span></span>
<span id="cb36-36"><a href="lda.html#cb36-36" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb36-37"><a href="lda.html#cb36-37" tabindex="-1"></a>    <span class="at">token_str =</span> <span class="fu">map</span>(data, <span class="sc">~</span><span class="fu">paste</span>(.<span class="sc">$</span>term, <span class="at">collapse =</span> <span class="st">&quot;, &quot;</span>)),</span>
<span id="cb36-38"><a href="lda.html#cb36-38" tabindex="-1"></a>    <span class="at">topic_lbl =</span> <span class="fu">map_chr</span>(token_str, get_topic_from_openai),</span>
<span id="cb36-39"><a href="lda.html#cb36-39" tabindex="-1"></a>    <span class="at">topic_lbl =</span> <span class="fu">str_remove_all</span>(topic_lbl, <span class="st">&#39;</span><span class="sc">\\</span><span class="st">&quot;&#39;</span>),</span>
<span id="cb36-40"><a href="lda.html#cb36-40" tabindex="-1"></a>    <span class="at">topic_lbl =</span> snakecase<span class="sc">::</span><span class="fu">to_any_case</span>(topic_lbl, <span class="st">&quot;title&quot;</span>)</span>
<span id="cb36-41"><a href="lda.html#cb36-41" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb36-42"><a href="lda.html#cb36-42" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>data)</span>
<span id="cb36-43"><a href="lda.html#cb36-43" tabindex="-1"></a></span>
<span id="cb36-44"><a href="lda.html#cb36-44" tabindex="-1"></a><span class="co"># Save to file system to avoid regenerating.</span></span>
<span id="cb36-45"><a href="lda.html#cb36-45" tabindex="-1"></a><span class="fu">saveRDS</span>(lda_topics, <span class="at">file =</span> <span class="st">&quot;input/lda_topics.RDS&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="lda.html#cb37-1" tabindex="-1"></a>lda_topics <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="at">file =</span> <span class="st">&quot;input/lda_topics.RDS&quot;</span>)</span></code></pre></div>
<p>Let’s see the topic summary with the newly generated labels.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="lda.html#cb38-1" tabindex="-1"></a>lda_top_tokens <span class="sc">%&gt;%</span></span>
<span id="cb38-2"><a href="lda.html#cb38-2" tabindex="-1"></a>  <span class="fu">inner_join</span>(lda_topics, <span class="at">by =</span> <span class="fu">join_by</span>(topic)) <span class="sc">%&gt;%</span></span>
<span id="cb38-3"><a href="lda.html#cb38-3" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">topic_lbl =</span> <span class="fu">str_wrap</span>(topic_lbl, <span class="dv">25</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb38-4"><a href="lda.html#cb38-4" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> beta, <span class="at">y =</span> <span class="fu">reorder_within</span>(term, <span class="at">by =</span> beta, <span class="at">within =</span> topic_lbl))) <span class="sc">+</span></span>
<span id="cb38-5"><a href="lda.html#cb38-5" tabindex="-1"></a>  <span class="fu">geom_col</span>() <span class="sc">+</span></span>
<span id="cb38-6"><a href="lda.html#cb38-6" tabindex="-1"></a>  <span class="fu">scale_y_reordered</span>() <span class="sc">+</span></span>
<span id="cb38-7"><a href="lda.html#cb38-7" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="at">facets =</span> <span class="fu">vars</span>(topic_lbl), <span class="at">scales =</span> <span class="st">&quot;free_y&quot;</span>) <span class="sc">+</span></span>
<span id="cb38-8"><a href="lda.html#cb38-8" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="cn">NULL</span>, <span class="at">title =</span> <span class="st">&quot;LDA Top 10 Terms&quot;</span>)</span></code></pre></div>
<p><img src="nlp_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
</div>
<div id="todo" class="section level3 unnumbered hasAnchor">
<h3>TODO<a href="lda.html#todo" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>I still need to learn more about</p>
<ul>
<li>Held-out Likelihood (Wallach et al., 2009).</li>
<li>Semantic Coherence. The coherence measure evaluates topics.</li>
<li>Exclusivity. Generally, the greater the number of topics in a model, the lower the quality of the smallest topics. One way around this is hiding the low-quality topics.</li>
</ul>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Nagelkerke2020b" class="csl-entry">
———. 2020b. <span>“NLP with r Part 1: Topic Modeling to Identify Topics in Restaurant Reviews,”</span> November. <a href="https://medium.com/cmotions/nlp-with-r-part-1-topic-modeling-to-identify-topics-in-restaurant-reviews-3ee870e6cd8">https://medium.com/cmotions/nlp-with-r-part-1-topic-modeling-to-identify-topics-in-restaurant-reviews-3ee870e6cd8</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="topicmodeling.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="stm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
