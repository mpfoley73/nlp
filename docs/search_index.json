[["index.html", "Natural Language Processing in R Intro", " Natural Language Processing in R Michael Foley 2023-09-20 Intro These notes consolidate several resources I’ve encountered while working on text mining projects: Text Mining with R (Silge and Robinson 2017) Introduction to Natural Language Processing in R (DataCamp) Topic Modeling in R (DataCamp) Introduction to Text Analysis in R” (DataCamp) String Manipulation in R with stringr (DataCamp) Text Mining with Bag-of-Words in R (DataCamp) Sentiment Analysis in R (DataCamp) Tidy Sentiment Analysis in R (DataCamp) Julia Silge’s The game is afoot! Topic modeling of Sherlock Holmes stories Julia Silge’s Training, evaluating, and interpreting topic models Toward understanding 17th century English culture: A structural topic model of Francis Bacon’s ideas References "],["data-preparation.html", "Chapter 1 Data Preparation", " Chapter 1 Data Preparation This section covers how to prepare a corpus for text analysis. I’ll work with the customer reviews of London-based hotels data set hosted on data.world. hotel_raw contains 27K reviews of the top 10 most- and least-expensive hotels in London. The csv file is located online here. I saved it to my \\inputs directory. library(tidyverse) library(tidytext) library(scales) library(glue) hotel_0 &lt;- read_csv(&quot;input/london_hotel_reviews.csv&quot;) %&gt;% mutate( `Date Of Review` = lubridate::mdy(`Date Of Review`), `Property Name` = str_trim(str_remove(`Property Name`, &quot;Hotel&quot;)), `Property Name` = str_trim(str_remove(`Property Name`, &quot;The&quot;)), `Property Name` = case_when( str_detect(`Property Name`, &quot;^45 Park Lane&quot;) ~ &quot;45 Park Lane&quot;, str_detect(`Property Name`, &quot;^Apex&quot;) ~ &quot;Apex&quot;, str_detect(`Property Name`, &quot;^Bulgari&quot;) ~ &quot;Bulgari&quot;, str_detect(`Property Name`, &quot;^Corinthia&quot;) ~ &quot;Corinthia&quot;, str_detect(`Property Name`, &quot;^London Guest House&quot;) ~ &quot;Guest House&quot;, str_detect(`Property Name`, &quot;^Xenia&quot;) ~ &quot;Xenia&quot;, str_detect(`Property Name`, &quot;^Mandarin&quot;) ~ &quot;Mandarin&quot;, str_detect(`Property Name`, &quot;^Mondrian&quot;) ~ &quot;Mondrian&quot;, str_detect(`Property Name`, &quot;^Wellesley&quot;) ~ &quot;Wellesley&quot;, TRUE ~ `Property Name` ), `Property Name` = factor(`Property Name`), review_id = row_number() ) %&gt;% janitor::clean_names(case = &quot;snake&quot;) %&gt;% rename(review_dt = date_of_review, reviewer_loc = location_of_the_reviewer) %&gt;% select(review_id, everything()) hotel_0 contains 27,330 reviews of 20 hotels posted between 2002-04-01 and 2018-10-18. The raw data needs cleaned. One issue is tags like &lt;e9&gt; and unicode characters like &lt;U+0440&gt;. One way to get rid of unicode characters is to convert them to ASCII tags with iconv() and then remove the ASCII tags with str_remove(). E.g., iconv() converts &lt;U+0093&gt; to &lt;93&gt; which you can remove with regex \"\\\\&lt;[:alnum]+\\\\&gt;]\".1 hotel_1 &lt;- hotel_0 %&gt;% mutate( review_text = iconv(review_text, from = &quot;&quot;, to = &quot;ASCII&quot;, sub = &quot;byte&quot;), review_text = str_remove_all(review_text, &quot;\\\\&lt;[[:alnum:]]+\\\\&gt;&quot;) ) %&gt;% # Exclude reviews written in a foreign language. One heuristic to handle this # is to look for words common in other languages that do not also occur in English. filter(!str_detect(review_text, &quot;( das )|( der )|( und )|( en )&quot;)) %&gt;% # German filter(!str_detect(review_text, &quot;( et )|( de )|( le )|( les )&quot;)) %&gt;% # French filter(!str_detect(review_text, &quot;( di )|( e )|( la )&quot;)) %&gt;% # Italian filter(!str_detect(review_text, &quot;( un )|( y )&quot;)) # Spanish hotel_1 %&gt;% count(property_name, review_rating) %&gt;% mutate(review_rating = factor(review_rating)) %&gt;% ggplot(aes(y = fct_rev(property_name), x = n, fill = fct_rev(review_rating))) + geom_col(color = &quot;gray80&quot;) + scale_fill_brewer(type = &quot;div&quot;, direction = -1) + labs( y = NULL, fill = NULL, title = glue(&quot;{comma(nrow(hotel_1),1)} Reviews of 20 Hotels&quot;) ) That removes 3,557 rows. Tokenize the reviews. Even if you want bigrams, it is often helpful to tokenize into unigrams first to clean and regularize. # Get list of misspellings and their correction. Unfortunately, there are multiple # possible right spellings! (sigh) just choose one. spell_check &lt;- fuzzyjoin::misspellings %&gt;% distinct(misspelling, .keep_all = TRUE) # Create a list of stop words. Start with a standard list. stop_0 &lt;- stopwords::stopwords(language = &#39;en&#39;,source=&#39;stopwords-iso&#39;) # Some are potentially useful, so remove them from the stop list. stop_restart &lt;- c( &quot;appreciate&quot;, &quot;&quot; ) stop_1 &lt;- stop_0[!stop_0 %in% stop_restart] # Add your own custom words hotel_2 &lt;- hotel_1 %&gt;% # remove punctuation mutate(review_text = str_remove_all(review_text, &quot;[:punct:]&quot;)) %&gt;% # create unigrams unnest_tokens(&quot;word&quot;, review_text) %&gt;% # correct misspellings left_join( fuzzyjoin::misspellings %&gt;% distinct(misspelling, .keep_all = TRUE), by = join_by(word == misspelling) ) %&gt;% mutate(word = coalesce(correct, word)) %&gt;% select(-correct) %&gt;% # lemmatize words mutate(word = textstem::lemmatize_words(word, dictionary = lexicon::hash_lemmas)) %&gt;% # remove stop words anti_join(stop_words, by = &quot;word&quot;) %&gt;% # reconstruct the text nest(token_list = word) %&gt;% mutate(review_text = map_chr(token_list, ~ unlist(.) %&gt;% paste(collapse = &quot; &quot;))) %&gt;% select(-token_list) # tokens_0 %&gt;% # count(review_id) %&gt;% # mutate(n_bin = cut(n, breaks = c(0, seq(50, 500, 50), Inf))) %&gt;% # summarize(.by = n_bin, n = n()) %&gt;% # ggplot(aes(x = n_bin, y = n)) + # geom_col() At this point, you might decide to throw out smaller reviews because they are unlikely to identify multiple topics (Gils 2020). I’ll References "],["topicmodeling.html", "Chapter 2 Topic Modeling", " Chapter 2 Topic Modeling These notes are primarily compiled from the vignette for the STM package. Topic models are unsupervised ML models that identify topics as clusters of words with an associated probability distribution, and a probability distribution of topics within each document. There are two commonly used models: LDA and STM. LDA is the simpler model and is implemented in the popular topicmodels package. STM incorporates document metadata into the model. It is implemented in the STM package. These notes also discuss CTM, also in topicmodels, that is somewhere between LDA and STM. I discuss CTM because it is a bridge from LDA to STM. All three topic models are generative models of word counts. That means they assume there is some process that generates text which is a mixture of topics composed of words which occur with varying probabilities. Think of the observed text document as the product of an algorithm that selected each word in two stages: 1) it sampled a topic from a probability distribution, then 2) it sampled a word from the topic’s word probability distribution. The object in topic modeling is to tune the hyperparameters that define those probability distributions. In a way, topic models do the opposite of what you might expect. They are not estimating the probability that each document is one of those topics. They assume all topics contribute to each document and instead estimate their relative contributions. More concisely, the models treat documents as a mixture of topics, and the topics as a mixture of words where each word has a probability of belonging to each topic. The sum of topic proportions in document is one; the sum of word probabilities in a topic is one. This leads to two frameworks for thinking about topics. A topic’s prevalance in a document measures the proportion of the document generated by it. The topic’s content is the probability distribution of words associated with it. What distinguishes the following following models is how they handle these frameworks. An STM model defines covariates associated with prevalence and content; CTM does not. I don’t think LDA does either. We’re kind of at the limit of my understanding here. "],["learning-by-example.html", "2.1 Learning by Example", " 2.1 Learning by Example I will work through the model concepts by example using the stm::gadarian data set. This data set has n = 351 comments about immigration in an experimental setting. The test group as specifically instructred to write about what made them anxious about immigration. There is also a variable pid_rep for political party. gadarian_dat &lt;- stm::gadarian %&gt;% rename(comment = open.ended.response) %&gt;% select(-MetaID) glimpse(gadarian_dat) ## Rows: 341 ## Columns: 3 ## $ treatment &lt;dbl&gt; 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, … ## $ pid_rep &lt;dbl&gt; 1.00000, 1.00000, 0.33300, 0.50000, 0.66667, 0.00000, 0.8330… ## $ comment &lt;chr&gt; &quot;problems caused by the influx of illegal immigrants who are… "],["lda.html", "2.2 LDA", " 2.2 LDA Latent Dirichlet allocation (LDA) is an instance of a general family of mixed membership models that decompose data into latent components. Latent refers to unidentified topics and Dirichlet refers to the type of distribution followed by the words in the the topics and by the topics in the documents. Algorithm LDA assumes each document is created by a generative process where topics are included according to probabilities and words are included in the topics according to probabilities. The LDA algorithm determines what those probabilities are. The algorithm is: For each document \\(d_i\\), randomly assign each word to one of the K topics. Note that each \\(w_j\\) may be assigned to a different topic in each documents. For each document, tabulate the number of words in each topic, a \\(d \\times K\\) matrix. For each word, tabulate the sum of occurrences across all documents, a \\(w \\times K\\) matrix. Resample a single instance of a word from the corpus and remove it from the analysis, decrementing the document’s topic count and the word’s topic count. Calculate the gamma matrix, \\(\\gamma\\), and the beta matrix, \\(\\beta\\). the gamma matrix is the probability distribution of topics for each document, \\[p(t_k|d_i) = \\frac{n_{ik} + \\alpha}{N_i + K \\alpha}\\] were \\(n_{ik}\\) is the number of words in document \\(i\\) for topic \\(k\\), \\(N_i\\) is the total number of words in \\(i\\), and \\(\\alpha\\) is a hyperparameter. For each \\(d_i\\), \\(\\sum_{k \\in K} \\gamma_{ik} = 1\\). the beta matrix is the probability distribution of words for each topic, \\[p(w_j|t_k) = \\frac{m_{j,k} + \\beta}{\\sum_{j \\in V}m_{j,k} + V\\beta}\\] where \\(m_{j,k}\\) is the corpus-wide frequency count of word \\(w_j\\) to topic \\(k\\), \\(V\\) is the number of distinct words in the corpus, and \\(\\beta\\) is a hyperparameter. For each \\(t_k\\), \\(\\sum_{j \\in V} \\beta_{kj} = 1\\). Perform Gibbs sampling. Calculate the joint probability distribution of words for each document and topic, \\(p(w_j|t_k,d_i) = p(t_k|d_i)p(w_j|t_k)\\). Assign each word, \\(w_j\\), to the topic with the maximum joint probability. Repeat steps 3-6 for all of the words in all of the documents. Repeat steps 3-7 for a pre-determined number of iterations. LDA thus has 3 hyperparameters: document-topic density factor, \\(\\alpha\\), topic-word density factor, \\(\\beta\\), and topic count, \\(K\\). \\(\\alpha\\) controls the number of topics expected per document (large \\(\\alpha\\) = more topics). \\(\\beta\\) controls the distribution of words per topic (large \\(\\beta\\) = more words). Ideally, you want a few topics per document and a few words per topics, so, \\(\\alpha\\) and \\(\\beta\\) are typically set below one. \\(K\\) is set using a combination of domain knowledge, coherence, and exclusivity. Evaluation Held-out Likelihood (discussion of hold-out probability) (Wallach et al., 2009). Semantic Coherence Exclusivity Generally, the greater the number of topics in a model, the lower the quality of the smallest topics. One way around this is simply hiding the low-quality topics. The coherence measure (Mimno et al. 2011) evaluates topics. References "],["ctm.html", "2.3 CTM", " 2.3 CTM The Correlated Topic Model (CTM) (Blei and Lafferty 2007) builds on the LDA model (chapter 2.2). References "],["stm.html", "2.4 STM", " 2.4 STM STM incorporates arbitrary document metadata into the topic model. Without the inclusion of covariates, STM reduces to a logistic-normal topic model, often called the Correlated Topic Model (CTM) (chapter 2.3). The goal of STM is to discover topics and estimate their relationship to the metadata. Data Preparation The stm package represents a text corpus as an object with three components: a sparse matrix of counts by document and vocabulary word vector index, the vocabulary word vector, and document metadata. stm::textProcessor() is essentially a wrapper around the tm package. It: * converts words to lowercase, * removes stop words (including custom stop words!), numbers, and punctuation, and * stems words. After processing, stm::prepDocuments() removes infrequently appearing words, and removes any documents that contain no words after processing and removing words. gadarian_processed &lt;- textProcessor(gadarian_dat$comment, metadata = gadarian_dat) ## Building corpus... ## Converting to Lower Case... ## Removing punctuation... ## Removing stopwords... ## Removing numbers... ## Stemming... ## Creating Output... plotRemoved(gadarian_processed$documents, lower.thresh = seq(10, 200, by = 10)) Prepare Evaluate Interpret Visualize "],["data-formats.html", "2.5 Data Formats", " 2.5 Data Formats There are five common text mining packages, each with their own format requirements. Whichever package you work in, there is a decent chance you will want to use a function from one of the others, so you need some fluency in them all. tm works with Corpus objects (raw text with document and corpus metadata). Many tm algorithms work with a document-term matrix (DTM), a sparse matrix with one row per document, one column per term, and values equaling the word count or tf-idf. quanteda also works with Corpus objects, but has its own implementation. Many quanteda algorithms work with a document-feature matrix (DFM), again similar to tm’s DTM. tidytext works with tibbles. Many tidytext algorithms work with tibbles with one row per token (usually a word, but possibly a large item of text), a frequency count column, and possibly other metadata columns. qdap works with text fields in a data frame, so it does not require any particular data structure. sentimentr is similar to qdap. Let’s take the sawyer_raw data frame and pre-process it for all three packages. tm Turn the character vector sawyer_raw$text into a text source with VectorSource(), then turn the text source into a corpus with vCorpus(). Clean the corpus with a series of utility functions. One particularly important function, removeWords(), removes stop words, plus any custom stop words. I would normally add “tom” because it is so ubiquitous throughout the text. However, in this case I won’t because stopwords includes valence shifting words like “very” which are used in polarity scoring. I can remove them later for other exercises. # (sawyer_tm &lt;- VCorpus(VectorSource(sawyer$text)) %&gt;% # tm_map(content_transformer(replace_abbreviation)) %&gt;% # tm_map(removePunctuation) %&gt;% # tm_map(removeNumbers) %&gt;% # tm_map(content_transformer(tolower)) %&gt;% # tm_map(removeWords, c(stopwords(&quot;en&quot;), &quot;tom&quot;)) %&gt;% # tm_map(stripWhitespace)) Each document in the sawyer_tm VCorpus is a line of text. Use DocumentTermMaterix() to convert the vCorpus into tm’s bag-of-words format, DTM. # (sawyer_tm_dtm &lt;- DocumentTermMatrix(sawyer_tm)) This is a very sparse (nearly 100% sparse) matrix documents as rows and distinct words as columns. # group_by(chapter) %&gt;% # mutate(text = paste(text, collapse = &quot; &quot;)) %&gt;% # slice_head(n = 1) %&gt;% # select(chapter, text) # # sawyer_sent &lt;- sawyer %&gt;% # sentSplit(&quot;text&quot;) # # skimr::skim(sawyer) quanteda dafdafd tidytext dafdafd "],["sentimentanalysis.html", "Chapter 3 Sentiment Analysis", " Chapter 3 Sentiment Analysis Sentiment analysis is the extraction of the emotional intent of text. You can classify the polarity (positive | negative) or sentiment (angry | sad | happy | …) at the document, sentence, or feature level. To practice with real-world data, I will use the Customer reviews of London-based hotels data set hosted on data.world. hotel_raw contains reviews of the top 10 most- and least-expensive hotels in London. hotel_raw_1 &lt;- read_csv(&quot;https://query.data.world/s/2zsbemxf66vevuuc47jqe24n4zwl54&quot;) %&gt;% mutate(`Date Of Review` = lubridate::mdy(`Date Of Review`), `Property Name` = trimws(str_remove(`Property Name`, &quot;Hotel&quot;)), `Property Name` = trimws(str_remove(`Property Name`, &quot;The&quot;)), `Property Name` = case_when( str_detect(`Property Name`, &quot;^45 Park Lane&quot;) ~ &quot;45 Park Lane&quot;, str_detect(`Property Name`, &quot;^Apex&quot;) ~ &quot;Apex&quot;, str_detect(`Property Name`, &quot;^Bulgari&quot;) ~ &quot;Bulgari&quot;, str_detect(`Property Name`, &quot;^Corinthia&quot;) ~ &quot;Corinthia&quot;, str_detect(`Property Name`, &quot;^London Guest House&quot;) ~ &quot;Guest House&quot;, str_detect(`Property Name`, &quot;^Xenia&quot;) ~ &quot;Xenia&quot;, str_detect(`Property Name`, &quot;^Mandarin&quot;) ~ &quot;Mandarin&quot;, str_detect(`Property Name`, &quot;^Mondrian&quot;) ~ &quot;Mondrian&quot;, str_detect(`Property Name`, &quot;^Wellesley&quot;) ~ &quot;Wellesley&quot;, TRUE ~ `Property Name`), `Property Name` = factor(`Property Name`), review_id = row_number() ) %&gt;% janitor::clean_names(case = &quot;snake&quot;) %&gt;% select(review_id, everything()) skimr::skim(hotel_raw_1) ## Warning: There was 1 warning in `dplyr::summarize()`. ## ℹ In argument: `dplyr::across(tidyselect::any_of(variable_names), mangled_skimmers$funs)`. ## ℹ In group 0: . ## Caused by warning: ## ! There were 7010 warnings in `dplyr::summarize()`. ## The first warning was: ## ℹ In argument: `dplyr::across(tidyselect::any_of(variable_names), mangled_skimmers$funs)`. ## Caused by warning in `grepl()`: ## ! unable to translate &#39;Ottima qualit&lt;e0&gt; prezzo&#39; to a wide string ## ℹ Run `dplyr::last_dplyr_warnings()` to see the 7009 remaining warnings. Table 3.1: Data summary Name hotel_raw_1 Number of rows 27330 Number of columns 7 _______________________ Column type frequency: character 3 Date 1 factor 1 numeric 2 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace review_title 0 1.00 1 508 0 22323 0 review_text 0 1.00 16 32759 0 27329 0 location_of_the_reviewer 3953 0.86 1 178 0 6624 0 Variable type: Date skim_variable n_missing complete_rate min max median n_unique date_of_review 1 1 2002-04-01 2018-10-18 2015-07-22 3870 Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts property_name 0 1 FALSE 20 Sav: 5417, Mon: 4330, Rem: 3028, Cor: 2820 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist review_id 0 1 13665.50 7889.64 1 6833.25 13665.5 20497.75 27330 ▇▇▇▇▇ review_rating 0 1 4.49 0.89 1 4.00 5.0 5.00 5 ▁▁▁▂▇ There are 27,330 reviews of 20 hotels posted between 2002-04-01 and 2018-10-18. This raw text needs cleaned. One issue is tags like &lt;e9&gt; and unicode characters like &lt;U+0440&gt;. One way to get rid of unicode characters is to convert them to ascii tags with iconv() and then remove the tags with str_remove(). For example, iconv() will change &lt;U+0093&gt; to ascii string “&lt;93&gt;” that you can remove with regex \"\\\\&lt;[:alnum]+\\\\&gt;]\" (more help with regex on RStudio’s cheat sheets). hotel_raw_2 &lt;- hotel_raw_1 %&gt;% mutate( review_text = iconv(review_text, from = &quot;&quot;, to = &quot;ASCII&quot;, sub = &quot;byte&quot;), review_text = str_remove_all(review_text, &quot;\\\\&lt;[[:alnum:]]+\\\\&gt;&quot;) ) I also want to remove reviews written in a foreign language. One blunt force way to handle this is to look for words common in other languages that do not also occur in English. hotel &lt;- hotel_raw_2 %&gt;% filter(!str_detect(review_text, &quot;( das )|( der )|( und )|( en )&quot;)) %&gt;% # German filter(!str_detect(review_text, &quot;( et )|( de )|( le )|( les )&quot;)) %&gt;% # French filter(!str_detect(review_text, &quot;( di )|( e )|( la )&quot;)) %&gt;% # Italian filter(!str_detect(review_text, &quot;( un )|( y )&quot;)) %&gt;% # Spanish select(review_id, everything()) That got rid of 3,557 rows. My cleansed data set hotel has 23,773 rows. "],["subjectivity-lexicons.html", "3.1 Subjectivity Lexicons", " 3.1 Subjectivity Lexicons There are three common sentiment lexicons. You commonly use Bing for polarity scoring, and AFINN for identifying emotions. Bing classifies words as positive or negative. sentiment_bing &lt;- get_sentiments(&quot;bing&quot;) sentiment_bing %&gt;% count(sentiment) %&gt;% adorn_totals() %&gt;% flextable() %&gt;% autofit() %&gt;% colformat_int(j = 2) %&gt;% set_caption(&quot;Bing Lexicon&quot;) .cl-d4b59896{}.cl-d4aeb274{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d4aeb288{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d4b14570{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d4b14571{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d4b1552e{width:0.941in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d4b1552f{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d4b15538{width:0.941in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d4b15539{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d4b1553a{width:0.941in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d4b1553b{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d4b15542{width:0.941in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d4b15543{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 3.2: Bing Lexicon sentimentnnegative4,781positive2,005Total6,786 The AFINN lexicon associates words with a manually rated valence integer between -5 (negative) and +5 (positive) by Finn Arup Nielsen. sentiment_afinn &lt;- get_sentiments(&quot;afinn&quot;) sentiment_afinn %&gt;% count(value) %&gt;% adorn_totals() %&gt;% flextable() %&gt;% autofit() %&gt;% colformat_int(j = 2) %&gt;% set_caption(&quot;AFINN Lexicon&quot;) .cl-d507dbb0{}.cl-d5018a26{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d5018a30{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d503ec62{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d503ec6c{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d503fa2c{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d503fa36{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d503fa37{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d503fa38{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d503fa40{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d503fa41{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d503fa4a{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d503fa4b{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d503fa4c{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d503fa54{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d503fa55{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d503fa56{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d503fa5e{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d503fa5f{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 3.3: AFINN Lexicon valuen-516-443-3264-2966-13090112082448317244555Total2,477 The NRC lexicon associates words with eight emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) corresponding to the second level of Plutchik’s Wheel of Emotions and two sentiments (negative and positive). NRC was created by manual annotation on a crowdsourcing platform. Read more here. sentiment_nrc &lt;- get_sentiments(&quot;nrc&quot;) sentiment_nrc %&gt;% count(sentiment) %&gt;% adorn_totals() %&gt;% flextable() %&gt;% autofit() %&gt;% colformat_int(j = 2) %&gt;% set_caption(&quot;NRC Lexicon&quot;) .cl-d51f7c70{}.cl-d519032c{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d5190336{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d51b8dd6{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d51b8de0{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d51b9ca4{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d51b9cae{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d51b9caf{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d51b9cb0{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d51b9cb8{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d51b9cb9{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d51b9cba{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d51b9cc2{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d51b9cc3{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d51b9ccc{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d51b9ccd{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d51b9cce{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 3.4: NRC Lexicon sentimentnanger1,245anticipation837disgust1,056fear1,474joy687negative3,316positive2,308sadness1,187surprise532trust1,230Total13,872 "],["polarity-scoring.html", "3.2 Polarity Scoring", " 3.2 Polarity Scoring You can use three packages to measure text polarity. The simplest method is using tidytext. You transform your text into one-row per word with unnest_tokens(), join to one of the sentiment lexicons, and add up the positives and negatives. The polarity score is the net of positive - negative. qdap is more sophisticated. It takes into account valence shifters, surrounding words that change the intensity of a sentiment (e.g., “very”) or switch its direction (e.g., “not”). 3.2.1 tidytext The tidy way to score polarity is tagging individual words as “positive” and “negative” using the bing lexicon, then defining polarity as difference in counts. In principle, you ought to be able to use the positive|negative subset of NRC, or create weighted counts using AFINN. The qdap and sentimentr packages also correct for text length. A wordy review using twice as many positives as negatives shouldn’t score twice as positive. Those packages divide by \\(\\sqrt{n}\\) to reduce the impact of word count (longer reviews are still more emotional, just less so). Another “improvement” I employ below is capturing the words that registered as positive an negative. This is useful for explaining how the polarity score was calculated. I also attached the results back to the original data frame with an outer join since not all reviews would necessarily have any sentiment words (but they did). This leaves me with the original data, plus all my polarity metadata. hotel_tidy &lt;- hotel %&gt;% select(review_id, review_text) %&gt;% unnest_tokens(output = &quot;word&quot;, input = review_text) hotel_wordcount &lt;- hotel_tidy %&gt;% count(review_id) hotel_tidy_polarity_1 &lt;- hotel_tidy %&gt;% inner_join(get_sentiments(&quot;bing&quot;), by = &quot;word&quot;) %&gt;% group_by(review_id, sentiment) %&gt;% summarize(.groups = &quot;drop&quot;, n = n(), words = list(word)) %&gt;% pivot_wider(names_from = sentiment, values_from = c(n, words), values_fill = list(n = 0)) ## Warning in inner_join(., get_sentiments(&quot;bing&quot;), by = &quot;word&quot;): Detected an unexpected many-to-many relationship between `x` and `y`. ## ℹ Row 748865 of `x` matches multiple rows in `y`. ## ℹ Row 5608 of `y` matches multiple rows in `x`. ## ℹ If a many-to-many relationship is expected, set `relationship = &quot;many-to-many&quot;` to silence this ## warning. hotel_tidy_polarity_bing &lt;- hotel %&gt;% left_join(hotel_wordcount, by = &quot;review_id&quot;) %&gt;% left_join(hotel_tidy_polarity_1, by = &quot;review_id&quot;) %&gt;% mutate(polarity_score = (n_positive - n_negative) / sqrt(n), polarity_desc = if_else(polarity_score &gt;= 0, &quot;Positive&quot;, &quot;Negative&quot;)) Out of curiosity, I’ll try this with AFINN too. hotel_tidy_polarity_2 &lt;- hotel_tidy %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% group_by(review_id) %&gt;% summarize(.groups = &quot;drop&quot;, sentiment = sum(value), words = list(word)) hotel_tidy_polarity_afinn &lt;- hotel %&gt;% left_join(hotel_wordcount, by = &quot;review_id&quot;) %&gt;% left_join(hotel_tidy_polarity_2, by = &quot;review_id&quot;) %&gt;% mutate(polarity_score = sentiment / sqrt(n), polarity_desc = if_else(polarity_score &gt;= 0, &quot;Positive&quot;, &quot;Negative&quot;)) How should we present these results? One way is to treat polarity as the numeric measure and group by the factor variable property_name. Here are my Bing results. hotel_tidy_polarity_bing %&gt;% group_by(property_name) %&gt;% mutate(median_polarity = median(polarity_score, na.rm = TRUE)) %&gt;% ungroup() %&gt;% arrange(median_polarity) %&gt;% ggplot(aes(x = fct_inorder(property_name), y = polarity_score)) + geom_jitter(width = 0.2, alpha = 0.3, color = &quot;#5DA5DA&quot;, size = 1) + geom_boxplot(alpha = 0) + geom_hline(yintercept = 0, linetype = &quot;longdash&quot;, color = &quot;#60BD68&quot;, size = 1.25) + coord_flip() + theme_minimal() + labs(title = &quot;Few Reviews for a Reason?&quot;, subtitle = &quot;Hotels with few reviews tend to have negative reviews.&quot;, x = NULL, y = &quot;Sentiment Polarity Score&quot;, caption = &quot;Bing Polarity = (n_pos - n_neg) / sqrt(n_words)&quot;) And here is is with AFINN. hotel_tidy_polarity_afinn %&gt;% group_by(property_name) %&gt;% mutate(median_polarity = median(polarity_score, na.rm = TRUE)) %&gt;% ungroup() %&gt;% arrange(median_polarity) %&gt;% ggplot(aes(x = fct_inorder(property_name), y = polarity_score)) + geom_jitter(width = 0.2, alpha = 0.3, color = &quot;#5DA5DA&quot;, size = 1) + geom_boxplot(alpha = 0) + geom_hline(yintercept = 0, linetype = &quot;longdash&quot;, color = &quot;#60BD68&quot;, size = 1.25) + coord_flip() + theme_minimal() + labs(title = &quot;Few Reviews for a Reason?&quot;, subtitle = &quot;Hotels with few reviews tend to have negative reviews.&quot;, x = NULL, y = &quot;Sentiment Polarity Score&quot;, caption = &quot;AFINN Polarity = sentiment / sqrt(n_words)&quot;) Not too different. 2 of the top 3 are the same. The bottom 4 are the same group, just shuffled. Savoy fared better in AFINN than in Bing. The data set includes a numeric rating review_rating (1-5). Does the polarity score reveal anything that the numeric rating doesn’t already tell you? hotel_tidy_polarity_bing %&gt;% filter(!is.na(polarity_score)) %&gt;% ggplot(aes(x = as_factor(review_rating), y = polarity_score)) + geom_jitter(width = 0.2, alpha = 0.3, color = &quot;#5DA5DA&quot;, size = 1) + geom_boxplot(alpha = 0) + theme_minimal() + labs(title = &quot;Few Reviews for a Reason?&quot;, subtitle = &quot;Hotels with few reviews tend to have negative reviews.&quot;, x = NULL, y = &quot;Sentiment Polarity Score&quot;, caption = &quot;Polarity = (n_pos - n_neg) / sqrt(n_words)&quot;) Most reviews are positive - even for 2-rated hotels. While sentiment does increase with review rating, there are plenty of reviews with a rating of 5 and a polarity score &lt;0. Let’s dig into that a little. Here is a 1-rated review with a decent polarity score. hotel_tidy_polarity_bing %&gt;% filter(review_rating == 1) %&gt;% slice_max(polarity_score) %&gt;% select(polarity_score, review_text) %&gt;% flextable() %&gt;% autofit() .cl-da0a4bd4{}.cl-da0488ca{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-da0488d4{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-da06d058{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-da06d062{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-da06df44{width:1.235in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-da06df45{width:32.807in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-da06df4e{width:1.235in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-da06df4f{width:32.807in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}polarity_scorereview_text0.8728716Stayed here many times both for business and pleasure, alone, with my wife and even with extended family and children. It is in fact impossible to seperate the business and pleasure stays because everytime was a breathtakingly delectable pleasure. Meeting friends and colleagues whether in the lobby one of the resturants always left an impressive memorable impression. Alas, I haven't been able to visit again for sometime - either fully booked or are not able to guarantee convenient parking for my personal chauffeur and car. Well, that is amusing - I the reviewer treating the rating as a kind of ranking (first place!). How about a 5-rating with negative sentiment? hotel_tidy_polarity_bing %&gt;% filter(review_rating == 5) %&gt;% slice_min(polarity_score) %&gt;% select(polarity_score, review_text) %&gt;% flextable() %&gt;% autofit() .cl-da1f7f5e{}.cl-da1992b0{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-da1992ba{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-da1bedbc{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-da1bedd0{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-da1bfc3a{width:1.235in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-da1bfc44{width:22.26in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-da1bfc45{width:1.235in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-da1bfc4e{width:22.26in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}polarity_scorereview_text-0.70014Some design faults in the bathroom - no stool, misplaced grab handles and vanity mirror. | Very disappointing experience in Savoy Grill. Good quality ingredients but poorly presented and tasteless. Numerous mistakes in service including charging for expensive drinks which we did not have. Service charge revoked and booking for following night cancelled. | Again, I think they got it backwards. The polarity scoring can reveal why some hotels rated worse by looking at the words used. (this plot di not come out well). hotel_tidy_polarity_bing %&gt;% mutate(review_rating = as.factor(review_rating)) %&gt;% filter(!is.na(polarity_desc)) %&gt;% select(!c(words_positive, words_negative)) %&gt;% # no lists allowed unnest_tokens(&quot;word&quot;, review_text) %&gt;% anti_join(stop_words, by = &quot;word&quot;) %&gt;% filter(!str_detect(word, &quot;[0-9]&quot;)) %&gt;% filter(!word %in% c(&quot;hotel&quot;, &quot;stay&quot;, &quot;night&quot;, &quot;london&quot;)) %&gt;% count(review_rating, polarity_desc, word) %&gt;% group_by(review_rating, polarity_desc) %&gt;% slice_max(order_by = n, n = 8, with_ties = FALSE) %&gt;% mutate(n = if_else(polarity_desc == &quot;Negative&quot;, -n, n), word = paste0(if_else(polarity_desc == &quot;Negative&quot;, &quot;-&quot;, &quot;+&quot;), word)) %&gt;% ungroup() %&gt;% arrange(review_rating, n) %&gt;% ggplot(aes(x = fct_inorder(word), y = n, fill = polarity_desc, color = polarity_desc)) + geom_col(alpha = 0.6) + facet_wrap(~as.factor(review_rating), scales = &quot;free&quot;) + scale_fill_few() + scale_color_few() + coord_flip() + theme_minimal() + theme(legend.position = &quot;top&quot;) + labs(title = &quot;Top Words by review type&quot;, y = &quot;Word Count&quot;, x = NULL, fill = NULL, color = NULL) Word clouds are a nice way to get an overview of the data. hotel_tidy_polarity_bing %&gt;% unnest_tokens(output = &quot;word&quot;, input = review_text) %&gt;% anti_join(stop_words, by = &quot;word&quot;) %&gt;% filter(!str_detect(word, &quot;[0-9]&quot;) &amp; !is.na(polarity_desc)) %&gt;% filter(!word %in% c(&quot;hotel&quot;, &quot;stay&quot;, &quot;night&quot;, &quot;london&quot;)) %&gt;% count(word, polarity_desc, wt = n) %&gt;% pivot_wider(names_from = polarity_desc, values_from = n, values_fill = 0) %&gt;% data.table::data.table() %&gt;% as.matrix(rownames = &quot;word&quot;) %&gt;% wordcloud::comparison.cloud(max.words = 20, title.size = 1.5) ## Warning in wordcloud::comparison.cloud(., max.words = 20, title.size = 1.5): ## breakfast could not be fit on page. It will not be plotted. ## Warning in wordcloud::comparison.cloud(., max.words = 20, title.size = 1.5): ## amazing could not be fit on page. It will not be plotted. ## Warning in wordcloud::comparison.cloud(., max.words = 20, title.size = 1.5): ## wonderful could not be fit on page. It will not be plotted. ## Warning in wordcloud::comparison.cloud(., max.words = 20, title.size = 1.5): ## restaurant could not be fit on page. It will not be plotted. 3.2.2 sentimentr sentimentr calculates polarity at the sentence level. It improves on tidytext in that it takes into account the context in which the sentiment words occur, that is, the valence shifters. A negator flips the sign of a polarized word (e.g., “I do not like it.”). See lexicon::hash_valence_shifters[y==1]. An amplifier (intensifier) increases the impact (e.g., “I really like it.”). See lexicon::hash_valence_shifters[y==2]. A de-amplifier (downtoner) reduces the impact (e.g., “I hardly like it.”). See lexicon::hash_valence_shifters[y==3]. An adversative conjunction overrules the previous clause containing a polarized word (e.g., “I like it but it’s not worth it.”). See lexicon::hash_valence_shifters[y==4]. sentimentr uses a lexicon package combined from the syuzhet and lexicon packages. Positive words are scored +1 and negative words are scored -1. sentimentr identifies clusters of words within sentences of the text. The 4 words before and 2 words after are candidate valence shifters. Polarized words are weighted by the valence shifter weights: negators = -1; amplifiers and de-amplifiers = 1.8; adversative conjunctions decrease the value of the prior cluster and increase the value of the following cluster. Neutral words hold no value, but do affect the word count. hotel_sentr_polarity_1 &lt;- hotel %&gt;% mutate(sentences = get_sentences(review_text)) %$% sentiment_by(sentences, review_id) hotel_sentr_polarity &lt;- hotel %&gt;% left_join(hotel_sentr_polarity_1, by = &quot;review_id&quot;) %&gt;% mutate(polarity_desc = if_else(ave_sentiment &gt;= 0, &quot;Positive&quot;, &quot;Negative&quot;)) Here is the chart treating polarity as the numeric measure and group by the factor variable property_name. hotel_sentr_polarity %&gt;% group_by(property_name) %&gt;% mutate(median_polarity = median(ave_sentiment, na.rm = TRUE)) %&gt;% ungroup() %&gt;% arrange(median_polarity) %&gt;% ggplot(aes(x = fct_inorder(property_name), y = ave_sentiment)) + geom_jitter(width = 0.2, alpha = 0.3, color = &quot;#5DA5DA&quot;, size = 1) + geom_boxplot(alpha = 0) + geom_hline(yintercept = 0, linetype = &quot;longdash&quot;, color = &quot;#60BD68&quot;, size = 1.25) + coord_flip() + theme_minimal() + labs(title = &quot;Few Reviews for a Reason?&quot;, subtitle = &quot;Hotels with few reviews tend to have negative reviews.&quot;, x = NULL, y = &quot;Sentiment Polarity Score&quot;, caption = &quot;Sentimentr Polarity&quot;) Here, Xenia came out on top. The data set includes a numeric rating review_rating (1-5). Does the polarity score reveal anything that the numeric rating doesn’t already tell you? hotel_sentr_polarity %&gt;% filter(!is.na(ave_sentiment)) %&gt;% ggplot(aes(x = as_factor(review_rating), y = ave_sentiment)) + geom_jitter(width = 0.2, alpha = 0.3, color = &quot;#5DA5DA&quot;, size = 1) + geom_boxplot(alpha = 0) + theme_minimal() + labs(title = &quot;Few Reviews for a Reason?&quot;, subtitle = &quot;Hotels with few reviews tend to have negative reviews.&quot;, x = NULL, y = &quot;Sentiment Polarity Score&quot;, caption = &quot;Polarity = (n_pos - n_neg) / sqrt(n_words)&quot;) One nice feature of sentimentr is that it creates a highlighted text based on sentiment score (green = positive, red = negative). hotel_sentr_polarity %&gt;% highlight() 3.2.3 qdap qdap::polarity(text.var, grouping.var = NULL) calculates the polarity score for each character string text.var, grouping by optional character vector grouping.var. polarity uses the sentiment dictionary to tag polarized words. It considers a context cluster of words around polarized words as valence shifters (neutral, negator, amplifier, or de-amplifier). Neutral words hold no value but do affect word count. polarity applies the dictionary weights to each polarized word and then further weights by the number and position of the valence shifters. Last, it sums the context cluster and divides by the square root of the word count, yielding an unbounded polarity score. # hotel_sentr_polarity &lt;- hotel %&gt;% # mutate(sentences = get_sentences(review_text)) %&gt;% # sentiment_by(sentences, list(review_id)) # hotel_qdap_polarity &lt;- hotel %$% # polarity(review_text, review_id) # # scores(hotel_qdap_polarity) The counts() function returns one row for each line of text. It includes a list of the positive and negative words that contribute to the polarity score. Line 57 has a polarity score of zero because it has a pair of positive and negative words. # sawyer[57,] # counts(sawyer_tm_polarity)[57,] # counts(sawyer_tm_polarity)[57, c(&quot;pos.words&quot;, &quot;neg.words&quot;)] %&gt;% unlist() Oh, but wait - Twain doesn’t use mighty as a positive adjective, but rather, as an amplifier adverb. Mighty appears sawyer %&gt;% filter(str_detect(text, \"mighty\")) %&gt;% nrow times in Tom Sawyer. We should remove it from the polarity.frame and add it to the amplifiers. # custom_frame &lt;- sentiment_frame( # positives = qdapDictionaries::positive.words[qdapDictionaries::positive.words != &quot;mighty&quot;], # negatives = qdapDictionaries::negative.words # ) # # sawyer_tm_polarity_2 &lt;- sawyer %&gt;% # mutate(text = str_remove_all(text, &quot;\\\\_&quot;)) %$% # polarity( # text, chapter, # polarity.frame = custom_frame, # amplifiers = sort(c(qdapDictionaries::amplification.words, &quot;mighty&quot;)) # ) # # counts(sawyer_tm_polarity_2)[57,] Something is still wrong here. It removed mighty as a positive word, but did not apply it as amplifier. It seems to be confused by the presence of the comma in “tomorrow, to punish”. I’ll drop the matter for now, but perhaps how we parse the data into rows makes a difference. It also advises that you run SentSplit() on the data first, but the function never stopped running, so I abandoned it. Here is a plot of the polarity results. # plot(sawyer_tm_polarity_2) Chapter 22 had the lowest polarity score and chapter 34 the highest. # sawyer_tm_polarity_2 %&gt;% # scores() %&gt;% # mutate(chapter = as.integer(chapter)) %&gt;% # ggplot(aes(x = chapter, y = ave.polarity)) + # geom_point() + # geom_segment(aes(x = chapter, xend = chapter, y = 0, yend = ave.polarity)) + # geom_smooth() + # geom_hline(yintercept = 0, color = &quot;red&quot;) + # theme_minimal() + # labs(title = &quot;Adventures of Tom Sawyer Chronological Polarity&quot;) Create to strings, one with the positive chapters, and one from the negative chapters. # sawyer_poloarity_pos &lt;- sawyer_tm_polarity_2$all %&gt;% # filter(polarity &gt; 0) %&gt;% # pull(text.var) %&gt;% # paste(collapse = &quot; &quot;) # # sawyer_poloarity_neg &lt;- sawyer_tm_polarity_2$all %&gt;% # filter(polarity &lt; 0) %&gt;% # pull(text.var) %&gt;% # paste(collapse = &quot; &quot;) # # sawyer_polarity_tdm &lt;- c(sawyer_poloarity_pos, sawyer_poloarity_neg) %&gt;% # VectorSource() %&gt;% # VCorpus() %&gt;% # TermDocumentMatrix(control = list(weighting = weightTfIdf, # removePunctuation = TRUE, # stopwords = stopwords(kind = &quot;en&quot;))) Often authors will use more words when they are more passionate. Lengthy reviews may inflate overall sentiment since the reviews will inherently contain more positive or negative language as the review lengthens. "],["emotion-classification.html", "3.3 Emotion Classification", " 3.3 Emotion Classification Bar plots are usually a clearer alternative, but radar charts do look pretty. https://en.wikipedia.org/wiki/Robert_Plutchik#/media/File:Plutchik-wheel.svg # dat &lt;- sawyer_tidy %&gt;% # inner_join(get_sentiments(&quot;nrc&quot;), by = &quot;word&quot;) %&gt;% # filter(!sentiment %in% c(&quot;positive&quot;, &quot;negative&quot;)) %&gt;% # mutate(sentiment = case_when(sentiment == &quot;joy&quot; ~ 1, # sentiment == &quot;trust&quot; ~ 2, # sentiment == &quot;fear&quot; ~ 3, # sentiment == &quot;surprise&quot; ~ 4, # sentiment == &quot;sadness&quot; ~ 5, # sentiment == &quot;disgust&quot; ~ 6, # sentiment == &quot;anger&quot; ~ 7, # sentiment == &quot;anticipation&quot; ~ 8, # TRUE ~ 9), # sentiment = factor(sentiment, levels = c(1:9), # labels = c(&quot;joy&quot;, &quot;trust&quot;, &quot;fear&quot;, &quot;surprise&quot;, # &quot;sadness&quot;, &quot;disgust&quot;, &quot;anger&quot;, # &quot;anticipation&quot;, &quot;other&quot;))) %&gt;% # count(sentiment) # # dat %&gt;% # radarchart::chartJSRadar() # library(gutenbergr) # sawyer_raw &lt;- gutenberg_works(title == &quot;The Adventures of Tom Sawyer&quot;) %&gt;% # gutenberg_download() # hotel %&gt;% filter(is.na(date_of_review)) # min(hotel$date_of_review, na.rm = TRUE) # skimr::skim(sawyer_raw) sawyer_raw is a tibble with 8,832 rows, with one row per line of text and 0-78 characters per line. This is a corpus with a single document and no metadata (although you could get multiple books at once, and attach the title and author as metadata). Most text requires some cleaning. I will want to remove the title lines, and add add some metadata, including the chapter number and line number. # sawyer &lt;- sawyer_raw %&gt;% # tail(-455) %&gt;% # chapter 1 starts on line 456 # mutate( # is_chap = str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE)), # chapter = cumsum(is_chap) # ) %&gt;% # filter(text != &quot;&quot; &amp; !str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE))) %&gt;% # mutate(line = row_number()) %&gt;% # select(line, chapter, text) # head(sawyer) A subjectivity lexicon is a predefined list of words associated with emotional context such as positive/negative. qdap::polarity() uses the lexicon::hash_sentiment_huliu lexicon with sentiment values in (+1, 0, -1.05, -1, -2). It is similar to tidytext::sentiments() AFINN lexicon (-5 to 5). tidytext::sentiments() also includes the NRC lexicon (classifying among 8 emotions) and the Bing lexicon (classifying as positive or negative). Subjectivity lexicons are typically short (a few thousand words), but work because of Zipf’s law. According to this law, the nth-ranked item in a frequency table has a frequency count equal to 1/n of the top-ranked item. So infrequently used words are used very infrequently. "],["comparison-cloud.html", "3.4 Comparison Cloud", " 3.4 Comparison Cloud # sawyer_tidy %&gt;% # anti_join(stop_words, by = &quot;word&quot;) %&gt;% # filter(!str_detect(word, &quot;[0-9]&quot;)) %&gt;% # inner_join(get_sentiments(&quot;nrc&quot;), by = &quot;word&quot;) %&gt;% # filter(sentiment %in% c(&quot;positive&quot;, &quot;negative&quot;)) %&gt;% # count(chapter, sentiment) %&gt;% # group_by(chapter) %&gt;% # mutate(pct = n / sum(n)) %&gt;% # ggplot(aes(x = chapter, y = pct, fill = sentiment, color = sentiment)) + # geom_area(alpha = 0.6) + # scale_x_continuous(breaks = 1:35, minor_breaks = NULL) + # scale_fill_few() + # scale_color_few() + # geom_hline(yintercept = 0.5, linetype = 2) + # theme_minimal() + # theme(legend.position = &quot;top&quot;) + # labs(title = &quot;Sentiment Proportion by Chapter&quot;, x = NULL, y = NULL, fill = NULL, color = NULL) # sawyer_tidy %&gt;% # anti_join(stop_words, by = &quot;word&quot;) %&gt;% # filter(!str_detect(word, &quot;[0-9]&quot;)) %&gt;% # inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% # ggplot(aes(x = value)) + # geom_density(fill = ggthemes::few_pal()(1), alpha = 0.6) + # theme_minimal() + # labs(title = &quot;AFINN Score Density&quot;) # sawyer_tidy %&gt;% # inner_join(get_sentiments(&quot;bing&quot;), by = &quot;word&quot;) %&gt;% # count(chapter, line, sentiment) %&gt;% # pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% # mutate(polarity = positive - negative) %&gt;% # ggplot(aes(x = as.factor(chapter), y = polarity)) + # geom_boxplot() + # geom_jitter(aes(color = as.factor(chapter)), alpha = 0.6, size = .5, show.legend = FALSE) + # theme_minimal() + # labs(title = &quot;Chapter Polarity&quot;) # sawyer_tidy %&gt;% # inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% # count(chapter, value) %&gt;% # ggplot(aes(area = n, fill = value)) + # treemapify::geom_treemap() One more cleaning step. Since these reviews are recorded in web sites, they are likely rife with spelling errors. I’ll use the misspellings dataset from the fuzzyjoin package to remove them. # data(&quot;misspellings&quot;, package = &quot;fuzzyjoin&quot;) # # # Some misspelling have multiple correct possibilities - choose one # misspellings_winner &lt;- misspellings %&gt;% # group_by(misspelling) %&gt;% # slice(n = 1) # # hotel &lt;- hotel_raw_3 %&gt;% # unnest_tokens(&quot;word&quot;, review_text) %&gt;% # left_join(misspellings, by = c(&quot;word&quot; = &quot;misspelling&quot;)) %&gt;% # word = coalesce(correct, word) # hotel_raw_2 %&gt;% filter(review_id %in% c(28)) %&gt;% # select(review_text) %&gt;% # flextable::flextable() "],["statistical-test.html", "3.5 Statistical Test", " 3.5 Statistical Test Since this is a comparison of a numeric outcome to a multinominal predictor (property_name) or ordinal predictor(review_rating), you could perform an ANOVA test. my_aov &lt;- aov(polarity_score ~ as.factor(review_rating), data = hotel_tidy_polarity_bing) my_anova &lt;- anova(my_aov) broom::tidy(my_anova) ## # A tibble: 2 × 6 ## term df sumsq meansq statistic p.value ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 as.factor(review_rating) 4 908. 227. 1875. 0 ## 2 Residuals 23271 2818. 0.121 NA NA #plot(TukeyHSD(my_aov)) my_lm &lt;- lm(polarity_score ~ as.factor(review_rating), data = hotel_tidy_polarity_bing) summary(my_lm) ## ## Call: ## lm(formula = polarity_score ~ as.factor(review_rating), data = hotel_tidy_polarity_bing) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.50252 -0.23263 -0.00321 0.22338 2.88057 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.14021 0.01574 -8.91 &lt;2e-16 *** ## as.factor(review_rating)2 0.22704 0.02143 10.59 &lt;2e-16 *** ## as.factor(review_rating)3 0.47721 0.01838 25.97 &lt;2e-16 *** ## as.factor(review_rating)4 0.80924 0.01653 48.96 &lt;2e-16 *** ## as.factor(review_rating)5 0.94258 0.01597 59.01 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.348 on 23271 degrees of freedom ## (497 observations deleted due to missingness) ## Multiple R-squared: 0.2438, Adjusted R-squared: 0.2436 ## F-statistic: 1875 on 4 and 23271 DF, p-value: &lt; 2.2e-16 tbl_regression(my_lm) #hroqfhsqkj table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #hroqfhsqkj thead, #hroqfhsqkj tbody, #hroqfhsqkj tfoot, #hroqfhsqkj tr, #hroqfhsqkj td, #hroqfhsqkj th { border-style: none; } #hroqfhsqkj p { margin: 0; padding: 0; } #hroqfhsqkj .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #hroqfhsqkj .gt_caption { padding-top: 4px; padding-bottom: 4px; } #hroqfhsqkj .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #hroqfhsqkj .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #hroqfhsqkj .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #hroqfhsqkj .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #hroqfhsqkj .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #hroqfhsqkj .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #hroqfhsqkj .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #hroqfhsqkj .gt_column_spanner_outer:first-child { padding-left: 0; } #hroqfhsqkj .gt_column_spanner_outer:last-child { padding-right: 0; } #hroqfhsqkj .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #hroqfhsqkj .gt_spanner_row { border-bottom-style: hidden; } #hroqfhsqkj .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #hroqfhsqkj .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #hroqfhsqkj .gt_from_md > :first-child { margin-top: 0; } #hroqfhsqkj .gt_from_md > :last-child { margin-bottom: 0; } #hroqfhsqkj .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #hroqfhsqkj .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #hroqfhsqkj .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #hroqfhsqkj .gt_row_group_first td { border-top-width: 2px; } #hroqfhsqkj .gt_row_group_first th { border-top-width: 2px; } #hroqfhsqkj .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #hroqfhsqkj .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #hroqfhsqkj .gt_first_summary_row.thick { border-top-width: 2px; } #hroqfhsqkj .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #hroqfhsqkj .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #hroqfhsqkj .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #hroqfhsqkj .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #hroqfhsqkj .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #hroqfhsqkj .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #hroqfhsqkj .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #hroqfhsqkj .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #hroqfhsqkj .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #hroqfhsqkj .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #hroqfhsqkj .gt_left { text-align: left; } #hroqfhsqkj .gt_center { text-align: center; } #hroqfhsqkj .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #hroqfhsqkj .gt_font_normal { font-weight: normal; } #hroqfhsqkj .gt_font_bold { font-weight: bold; } #hroqfhsqkj .gt_font_italic { font-style: italic; } #hroqfhsqkj .gt_super { font-size: 65%; } #hroqfhsqkj .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #hroqfhsqkj .gt_asterisk { font-size: 100%; vertical-align: 0; } #hroqfhsqkj .gt_indent_1 { text-indent: 5px; } #hroqfhsqkj .gt_indent_2 { text-indent: 10px; } #hroqfhsqkj .gt_indent_3 { text-indent: 15px; } #hroqfhsqkj .gt_indent_4 { text-indent: 20px; } #hroqfhsqkj .gt_indent_5 { text-indent: 25px; } Characteristic Beta 95% CI1 p-value as.factor(review_rating)     1 — —     2 0.23 0.19, 0.27     3 0.48 0.44, 0.51     4 0.81 0.78, 0.84     5 0.94 0.91, 0.97 1 CI = Confidence Interval "],["final-words.html", "Chapter 4 Final Words", " Chapter 4 Final Words We have finished a nice book. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
