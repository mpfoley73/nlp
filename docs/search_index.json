[["data-preparation.html", "Chapter 1 Data Preparation", " Chapter 1 Data Preparation This section covers how to prepare a corpus for text analysis. I’ll work with the customer reviews of London-based hotels data set hosted on data.world. hotel_raw contains 27K reviews of the top 10 most- and least-expensive hotels in London. The csv file is located online here. I saved it to my \\inputs directory. library(tidyverse) library(tidytext) library(scales) library(glue) hotel_0 &lt;- read_csv(&quot;input/london_hotel_reviews.csv&quot;) %&gt;% mutate( `Date Of Review` = lubridate::mdy(`Date Of Review`), `Property Name` = str_trim(str_remove(`Property Name`, &quot;Hotel&quot;)), `Property Name` = str_trim(str_remove(`Property Name`, &quot;The&quot;)), `Property Name` = case_when( str_detect(`Property Name`, &quot;^45 Park Lane&quot;) ~ &quot;45 Park Lane&quot;, str_detect(`Property Name`, &quot;^Apex&quot;) ~ &quot;Apex&quot;, str_detect(`Property Name`, &quot;^Bulgari&quot;) ~ &quot;Bulgari&quot;, str_detect(`Property Name`, &quot;^Corinthia&quot;) ~ &quot;Corinthia&quot;, str_detect(`Property Name`, &quot;^London Guest House&quot;) ~ &quot;Guest House&quot;, str_detect(`Property Name`, &quot;^Xenia&quot;) ~ &quot;Xenia&quot;, str_detect(`Property Name`, &quot;^Mandarin&quot;) ~ &quot;Mandarin&quot;, str_detect(`Property Name`, &quot;^Mondrian&quot;) ~ &quot;Mondrian&quot;, str_detect(`Property Name`, &quot;^Wellesley&quot;) ~ &quot;Wellesley&quot;, TRUE ~ `Property Name` ), `Property Name` = factor(`Property Name`), review_id = row_number() ) %&gt;% janitor::clean_names(case = &quot;snake&quot;) %&gt;% rename(review_dt = date_of_review, reviewer_loc = location_of_the_reviewer) %&gt;% select(review_id, everything()) hotel_0 contains 27,330 reviews of 20 hotels posted between 2002-04-01 and 2018-10-18. The raw data needs cleaned. One issue is tags like &lt;e9&gt; and unicode characters like &lt;U+0440&gt;. One way to get rid of unicode characters is to convert them to ASCII tags with iconv() and then remove the ASCII tags with str_remove(). E.g., iconv() converts &lt;U+0093&gt; to &lt;93&gt; which you can remove with regex \"\\\\&lt;[:alnum]+\\\\&gt;]\".1 hotel_1 &lt;- hotel_0 %&gt;% mutate( review_text = iconv(review_text, from = &quot;&quot;, to = &quot;ASCII&quot;, sub = &quot;byte&quot;), review_text = str_remove_all(review_text, &quot;\\\\&lt;[[:alnum:]]+\\\\&gt;&quot;) ) %&gt;% # Exclude reviews written in a foreign language. One heuristic to handle this # is to look for words common in other languages that do not also occur in English. filter(!str_detect(review_text, &quot;( das )|( der )|( und )|( en )&quot;)) %&gt;% # German filter(!str_detect(review_text, &quot;( et )|( de )|( le )|( les )&quot;)) %&gt;% # French filter(!str_detect(review_text, &quot;( di )|( e )|( la )&quot;)) %&gt;% # Italian filter(!str_detect(review_text, &quot;( un )|( y )&quot;)) # Spanish hotel_1 %&gt;% count(property_name, review_rating) %&gt;% mutate(review_rating = factor(review_rating)) %&gt;% ggplot(aes(y = fct_rev(property_name), x = n, fill = fct_rev(review_rating))) + geom_col(color = &quot;gray80&quot;) + scale_fill_brewer(type = &quot;div&quot;, direction = -1) + labs( y = NULL, fill = NULL, title = glue(&quot;{comma(nrow(hotel_1),1)} Reviews of 20 Hotels&quot;) ) That removes 3,557 rows. Tokenize the reviews. Even if you want bigrams, it is often helpful to tokenize into unigrams first to clean and regularize. # Get list of misspellings and their correction. Unfortunately, there are multiple # possible right spellings! (sigh) just choose one. spell_check &lt;- fuzzyjoin::misspellings %&gt;% distinct(misspelling, .keep_all = TRUE) # Create a list of stop words. Start with a standard list. stop_0 &lt;- stopwords::stopwords(language = &#39;en&#39;,source=&#39;stopwords-iso&#39;) # Some are potentially useful, so remove them from the stop list. stop_restart &lt;- c( &quot;appreciate&quot;, &quot;&quot; ) stop_1 &lt;- stop_0[!stop_0 %in% stop_restart] # Add your own custom words hotel_2 &lt;- hotel_1 %&gt;% # remove punctuation mutate(review_text = str_remove_all(review_text, &quot;[:punct:]&quot;)) %&gt;% # create unigrams unnest_tokens(&quot;word&quot;, review_text) %&gt;% # correct misspellings left_join( fuzzyjoin::misspellings %&gt;% distinct(misspelling, .keep_all = TRUE), by = join_by(word == misspelling) ) %&gt;% mutate(word = coalesce(correct, word)) %&gt;% select(-correct) %&gt;% # lemmatize words mutate(word = textstem::lemmatize_words(word, dictionary = lexicon::hash_lemmas)) %&gt;% # remove stop words anti_join(stop_words, by = &quot;word&quot;) %&gt;% # reconstruct the text nest(token_list = word) %&gt;% mutate(review_text = map_chr(token_list, ~ unlist(.) %&gt;% paste(collapse = &quot; &quot;))) %&gt;% select(-token_list) # tokens_0 %&gt;% # count(review_id) %&gt;% # mutate(n_bin = cut(n, breaks = c(0, seq(50, 500, 50), Inf))) %&gt;% # summarize(.by = n_bin, n = n()) %&gt;% # ggplot(aes(x = n_bin, y = n)) + # geom_col() At this point, you might decide to throw out smaller reviews because they are unlikely to identify multiple topics (Gils 2020). I’ll References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
