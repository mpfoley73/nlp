[["index.html", "Natural Language Processing in R Intro", " Natural Language Processing in R Michael Foley 2023-11-30 Intro These notes consolidate several resources I’ve encountered while working on text mining projects: Text Mining with R (Silge and Robinson 2017) Introduction to Natural Language Processing in R (DataCamp) Topic Modeling in R (DataCamp) Introduction to Text Analysis in R” (DataCamp) String Manipulation in R with stringr (DataCamp) Text Mining with Bag-of-Words in R (DataCamp) Sentiment Analysis in R (DataCamp) Tidy Sentiment Analysis in R (DataCamp) Julia Silge’s The game is afoot! Topic modeling of Sherlock Holmes stories Julia Silge’s Training, evaluating, and interpreting topic models Toward understanding 17th century English culture: A structural topic model of Francis Bacon’s ideas References "],["data-prep.html", "Chapter 1 Data Preparation", " Chapter 1 Data Preparation This section covers how to prepare a corpus for text analysis. I’ll work with the customer reviews of London-based hotels data set hosted on data.world. hotel_raw contains 27K reviews of the ten most- and ten least-expensive hotels in London. The csv file is located online here. I saved it to my \\inputs directory. library(tidyverse) library(tidytext) library(scales) library(glue) hotel_0 &lt;- read_csv( &quot;input/london_hotel_reviews.csv&quot;, col_types = &quot;cicccc&quot;, col_names = c(&quot;property&quot;, &quot;rating&quot;, &quot;title&quot;, &quot;review&quot;, &quot;reviewer_loc&quot;, &quot;review_dt&quot;), skip = 1 ) %&gt;% mutate(review_id = row_number()) %&gt;% select(review_id, everything()) glimpse(hotel_0) ## Rows: 27,330 ## Columns: 7 ## $ review_id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17… ## $ property &lt;chr&gt; &quot;Apex London Wall Hotel&quot;, &quot;Corinthia Hotel London&quot;, &quot;The … ## $ rating &lt;int&gt; 5, 5, 5, 4, 5, 1, 5, 5, 5, 5, 5, 4, 2, 4, 5, 5, 5, 5, 5, … ## $ title &lt;chr&gt; &quot;Ottima qualità prezzo&quot;, &quot;By far, my best hotel in the wo… ## $ review &lt;chr&gt; &quot;Siamo stati a Londra per un week end ed abbiamo alloggia… ## $ reviewer_loc &lt;chr&gt; &quot;Casale Monferrato, Italy&quot;, &quot;Savannah, Georgia&quot;, &quot;London&quot;… ## $ review_dt &lt;chr&gt; &quot;10/20/2012&quot;, &quot;3/23/2016&quot;, &quot;7/30/2013&quot;, &quot;6/2/2012&quot;, &quot;11/2… "],["scrub.html", "1.1 Scrub", " 1.1 Scrub The data needs to be cleaned. I’ll follow some of the techniques used by Nagelkerke (2020a). One issue is tags like &lt;e9&gt; and unicode characters like &lt;U+0440&gt;. One way to get rid of unicode characters is to convert them to ASCII tags with iconv() and then remove the ASCII tags with str_remove(). E.g., iconv() converts &lt;U+0093&gt; to &lt;93&gt; which you can remove with regex \"\\\\&lt;[:alnum]+\\\\&gt;]\".1 There are also some reviews in other languages that I’ll just drop. And some hotel names are pretty long, so I’ll abbreviate them. hotel_1 &lt;- hotel_0 %&gt;% mutate( # Create ASCII bytes review = iconv(review, from = &quot;&quot;, to = &quot;ASCII&quot;, sub = &quot;byte&quot;), # Remove &lt;..&gt; review = str_remove_all(review, &quot;\\\\&lt;[[:alnum:]]+\\\\&gt;&quot;), # Remove &lt;U+....&gt; review = str_remove_all(review, &quot;\\\\&lt;U\\\\+[[:alnum:]]{4}\\\\&gt;&quot;), # Lots of pipes? review = str_remove_all(review, &quot;(\\\\|)&quot;), review = str_squish(review), # Shorten some of the hotel names. property = str_remove_all( property, &quot;( - .*)|(, .*)|( Hotel)|( London)|(The )|( at .*)|( Hyde .*)|( Knights.*)&quot; ), property = factor(property, ordered = TRUE), # Reducing number of hotels for modeling simplicity. property = fct_lump_prop(property, prop = .05), # Bin common locations, reviewer_loc = factor(case_when( str_detect(reviewer_loc, &quot;(London)|(United Kingdom)|(UK)&quot;) ~ &quot;United Kingdom&quot;, str_detect(reviewer_loc, &quot;(New York)|(California)&quot;) ~ &quot;United States&quot;, TRUE ~ &quot;Other&quot; )), # Low ratings are so rare, lump the bottom two. rating = fct_collapse(as.character(rating), `1-2` = c(&quot;1&quot;, &quot;2&quot;)), # Interesting metadata chr_cnt = str_length(review) ) %&gt;% # Exclude reviews written in a foreign language. One heuristic to handle this # is to look for words common in other languages that do not also occur in English. filter( !str_detect(review, &quot;( das )|( der )|( und )|( en )&quot;), # German !str_detect(review, &quot;( et )|( de )|( le )|( les )&quot;), # French !str_detect(review, &quot;( di )|( e )|( la )&quot;), # Italian !str_detect(review, &quot;( un )|( y )&quot;), # Spanish str_length(review) &gt; 0 ) That might be enough. Let’s explore the data. # 90% of reviews rate the property a 4 or 5. hotel_1 %&gt;% janitor::tabyl(rating) ## rating n percent ## 1-2 1085 0.04570923 ## 3 1378 0.05805283 ## 4 4852 0.20440662 ## 5 16422 0.69183132 # Some reviews are as small as 1 character, but they can get quite large. hotel_1 %&gt;% select(chr_cnt) %&gt;% summary() ## chr_cnt ## Min. : 1.0 ## 1st Qu.: 306.0 ## Median : 512.0 ## Mean : 710.6 ## 3rd Qu.: 866.0 ## Max. :30498.0 # A few reviews hotel_1 %&gt;% sample_n(3, seed = 12345) %&gt;% select(chr_cnt, review) %&gt;% flextable::flextable() %&gt;% flextable::valign(j = 1, valign = &quot;top&quot;) %&gt;% flextable::autofit() .cl-801bfe28{}.cl-80151950{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-80151964{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-8017e1ee{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-8017e1f8{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-8017e1f9{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-8017f404{width:0.787in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8017f405{width:87.872in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8017f40e{width:0.787in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8017f418{width:87.872in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8017f419{width:0.787in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8017f422{width:87.872in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8017f423{width:0.787in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8017f424{width:87.872in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}chr_cntreview421Very stylish hotel with lovely rooms, great decor! Hotel is fitted out beautifully. Service was ok - not a differentiator (it often isn't in London hotels). Location great though and was perfect for taking in the iconic views of the Thames. This side of the river has so much to offer! Bar was v popular ( couldn't get a seat) seemed to cater for non guests more than guests. Really pleasant stay if a little over priced.338We stayed in a twin room for three nights and had a great stay. Front desk gentleman was very helpful with our bags and very nice with all questions. Room was a little small but worked fine. Bathroom is small but the shower was great. Breakfast is good and the room guy is very friendly and nice. The location is great for getting around.1,457I stayed at the Corinthia for Valentine weekend in February. It is a true 5 stars hotel with minor things that could be changed for a perfect experience. The inside is marvelous, high ceiling, spotless tea area, excellent staff. One of the best hotel breakfast I ever had, in 2 beautiful massive rooms and with some original Asian suggestions for the more adventurous! Although the hotel is big, it always fell intimate. Our room was great with a warm atmosphere u didn't expect. I adored the heated bathroom floor. The bathroom curtain and blinds were broken though, and needed serious repair which fell strange for this level of hotel. Corinthia is very well located. Outside are your typical grand and classic London buildings, close to the river. But of course we went for the famous spa. It does get busy so be careful with the moment u pick to go (can't imagine what it's like on a Saturday). In an outstanding dark setting, incredible sauna, massive jacuzzi pool, perfect hammam, lovely sleeping pods. But its low point is its visible popularity. Some parts could get an update, a couple of broken elements, lots of towels everywhere. Could do with more staff inside to tell people to keep quiet also! But it is very impressive. We also tried the room service which was pricey and a bit slow but good. So I would definitely recommend the Corinthia for a romantic getaway. They could do with a little refurbishment but it definitely is a 5 star hotel! Nagelkerke (2020a) recommends removing punctuation to focus on the entire text rather than the sentences within. Nagelkerke also suggests removing very short (&lt;= 3 chars) for anything other than sentiment analysis. I’m going to keep punctuation and short reviews for now. Some of those extremely short reviews are gibberish, but tokenizing will filter out some of that. References "],["tokenize.html", "1.2 Tokenize", " 1.2 Tokenize Tokenize the reviews. Even if you want bigrams, it is often helpful to tokenize into unigrams first to clean and regularize. token_0 &lt;- hotel_1 %&gt;% select(review_id, review) %&gt;% unnest_tokens(&quot;word&quot;, review) %&gt;% mutate(.by = review_id, n = n()) %&gt;% # Short reviews are mostly gibberish. Require at least 10 words. filter(n &gt;= 10) # Attach word counts back to main data frame. Inner join side effect is removal # of some bogus reviews. hotel_2 &lt;- token_0 %&gt;% count(review_id, name = &quot;word_cnt&quot;) %&gt;% inner_join(hotel_1, by = &quot;review_id&quot;) %&gt;% relocate(word_cnt, .after = chr_cnt) hotel_2 %&gt;% select(chr_cnt, word_cnt) %&gt;% summary() ## chr_cnt word_cnt ## Min. : 51.0 Min. : 10.0 ## 1st Qu.: 314.0 1st Qu.: 56.0 ## Median : 520.0 Median : 93.0 ## Mean : 721.1 Mean : 130.9 ## 3rd Qu.: 875.0 3rd Qu.: 159.0 ## Max. :30498.0 Max. :5712.0 Nagelkerke (2020a) recommends discarding reviews with few (&lt;=50) tokens on the grounds that short reviews will not add much in terms of different topics within reviews.2. That’s about 25% of this sample. I set a lower limit of 10 tokens. References "],["spell-check.html", "1.3 Spell-check", " 1.3 Spell-check Run a spell-check to regularize the data. Its possible to land on the wrong correction, but there is probably more to gain than lose. Only a very small fraction of the tokens were misspellings. # There are multiple possible right spellings, so just choose one. spell_check &lt;- fuzzyjoin::misspellings %&gt;% distinct(misspelling, .keep_all = TRUE) token_1 &lt;- token_0 %&gt;% left_join(spell_check, by = join_by(word == misspelling)) %&gt;% mutate(word = coalesce(correct, word)) %&gt;% select(-correct) # Only .09% of words were misspelled. mean(token_0$word != token_1$word) ## [1] 0.0009311988 # Examples. tibble(before = token_0$word, after = token_1$word) %&gt;% filter(before != after) %&gt;% head() ## # A tibble: 6 × 2 ## before after ## &lt;chr&gt; &lt;chr&gt; ## 1 resturant restaurant ## 2 excellance excellence ## 3 reccomend recommend ## 4 reccommend recommend ## 5 accomodating accommodating ## 6 ther there "],["remove-stop-words.html", "1.4 Remove Stop Words", " 1.4 Remove Stop Words Stop words usually add no value, but you should pay attention to what you are dropping. Be ready to add pertinent words back and perhaps drop others. # Start with a standard list. stop &lt;- tidytext::stop_words %&gt;% # Don&#39;t remove potentially useful words. filter(!word %in% c(&quot;appreciate&quot;, &quot;room&quot;)) %&gt;% # But also exclude these words. bind_rows(tibble(word = c(&quot;hotel&quot;))) token_2 &lt;- token_1 %&gt;% anti_join(stop, by = &quot;word&quot;) %&gt;% # also exclude words that are, or contain, numbers. filter(!str_detect(word, &quot;[0-9]&quot;)) # Most frequently removed words token_1 %&gt;% anti_join(token_2, by = join_by(review_id, word)) %&gt;% count(word, sort = TRUE) ## # A tibble: 2,490 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 the 196439 ## 2 and 117631 ## 3 a 89194 ## 4 to 78025 ## 5 was 64152 ## 6 in 54398 ## 7 we 47724 ## 8 of 44773 ## 9 i 43742 ## 10 for 38260 ## # ℹ 2,480 more rows "],["lemmatize.html", "1.5 Lemmatize", " 1.5 Lemmatize Stemming and lemmatizing are ways to convert word variations like “staying”, “stayed”, and “stay” into a generic form: “stay”. Stemming tends to chop off endings to create a root word, but the stem is often not a word itself. E.g., “staying” becomes “stai”. Lemmatize gives you the more natural “stay”. token &lt;- token_2 %&gt;% mutate(word = textstem::lemmatize_words(word)) # Examples. tibble(before = token_2$word, after = token$word) %&gt;% filter(before != after) %&gt;% count(before, after, sort = TRUE) ## # A tibble: 8,054 × 3 ## before after n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 stayed stay 8785 ## 2 hotels hotel 4169 ## 3 amazing amaze 4017 ## 4 booked book 3041 ## 5 nights night 2948 ## 6 arrived arrive 2517 ## 7 bit bite 2473 ## 8 restaurants restaurant 2398 ## 9 staying stay 2313 ## 10 minutes minute 2180 ## # ℹ 8,044 more rows # A typical review, before and after bind_cols( review_id = 2:4, before = hotel_2 %&gt;% filter(review_id %in% 2:4) %&gt;% pull(review), after = token %&gt;% filter(review_id %in% 2:4) %&gt;% summarize(.by = review_id, x = paste(word, collapse = &quot; &quot;)) %&gt;% pull(x) ) %&gt;% flextable::flextable() %&gt;% flextable::valign(valign = &quot;top&quot;) %&gt;% flextable::autofit() .cl-83bbbc6c{}.cl-83b1d076{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-83b1d0b2{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-83b75190{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-83b7519a{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-83b751a4{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-83b751a5{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-83b76180{width:0.918in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-83b7618a{width:87.136in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-83b76194{width:42.66in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-83b7619e{width:0.918in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-83b7619f{width:87.136in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-83b761a8{width:42.66in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-83b761b2{width:0.918in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-83b761b3{width:87.136in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-83b761bc{width:42.66in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-83b761bd{width:0.918in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-83b761be{width:87.136in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-83b761bf{width:42.66in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}review_idbeforeafter2I had a pleasure of staying in this hotel for 7 nights recently. This hotel was perfect in every way. Communication with the hotel before staying was prompt, and very efficient. Checking in was a breeze. You go through the spectacular lobby with modern glass chandeliers and take the elevator to your room. My room, they gave me an upgrade to junior suite, was spectacular. We had a walk-in closet of the size where you could have put a small bed in there; it served us nicely for the seven day stay. The decor was very refined, and oh the bathroom! Carrera marble floor was heated throughout, rain shower was to die for! Location, as it turned out, was as good as it can be. We were 5 minutes walk to Trafalgar Square, but it was very quiet. Right outside was Embankment tube stop. We would walk to theater area and to numerous restaurants, and many major sites, such as London Eye or Westminster Abbey were within walking distance. We had buffet breakfast or room service every morning. It was pricy, but my rate included it. Couple of nights, we had glass of wine sitting in front of fire place in the lobby. I used the spa, which is included in the room rate, almost every night. After a windchill day of sightseeing, the steam sauna and jacuzzi would soften my weary muscles. I have stayed in many 5 star hotels around the world, but this hotel tops it. I would return here in a heartbeat next time I am in London.pleasure stay night recently perfect communication stay prompt efficient check breeze spectacular lobby modern glass chandelier elevator room room upgrade junior suite spectacular walk closet size bed serve nicely day stay decor refine bathroom carrera marble floor heat rain shower die location minute walk trafalgar square quiet embankment tube stop walk theater numerous restaurant major site london eye westminster abbey walk distance buffet breakfast room service morning pricy rate include couple night glass wine sit front fire lobby spa include room rate night windchill day sightsee steam sauna jacuzzi soften weary muscle stay star hotel world top return heartbeat time london3A very lovely first visit to this iconic hotel bar! Wonderful service, without being intrusive at all! Very delicious cocktails and just generally all round, a very indulgent experience. Well worth visiting only for that 'once in a lifetime' experience, though do make sure you are feeling 'flush' it doesn't come cheap!lovely visit iconic bar wonderful service intrusive delicious cocktail round indulgent experience worth visit lifetime experience feel flush cheap43 of us stayed at the Rhodes Hotel for 4 nights, its a great location for taking the Paddington Express from Heathrow. We like the location clost to the partk and in walking distance of most locations. The room and bath were small compared to American Hotels but very clean. We enjoyed the free WIFI. The owners and the staff were very friendly and helpful with taxi's and resturant recomendations. We would stay there again.stay rhodes night location take paddington express heathrow location clost partk walk distance location room bath compare american hotel clean enjoy free wifi owner staff friendly helpful taxi's restaurant recomendations stay At this point, you might decide to throw out smaller reviews because they are unlikely to identify multiple topics (Gils 2020). References "],["bigrams.html", "1.6 Bigrams", " 1.6 Bigrams Bigrams should not contain stop words. However, they should be adjacent words, so tokenize into bigrams, split into words an then filter out rows where one or both words is stop word. bigram_0 &lt;- # start with the tokenized data _prior_ to removing stop words. token_2 %&gt;% summarize(.by = review_id, reconstructed = paste(word, collapse = &quot; &quot;)) %&gt;% unnest_tokens(&quot;bigram&quot;, reconstructed, token = &quot;ngrams&quot;, n = 2) # Remove bigrams where one or both words are stop words. bigram &lt;- bigram_0 %&gt;% separate(bigram, into = c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% anti_join(stop, by = join_by(word1 == word)) %&gt;% anti_join(stop, by = join_by(word2 == word)) %&gt;% filter(!str_detect(word1, &quot;[0-9]&quot;), !str_detect(word2, &quot;[0-9]&quot;)) %&gt;% mutate(bigram = paste(word1, word2)) %&gt;% select(review_id, bigram) # Example bind_cols( hotel_2 %&gt;% filter(review_id == 3) %&gt;% select(review), bigram %&gt;% filter(review_id == 3) %&gt;% summarize(bigrams = paste(bigram, collapse = &quot;\\n&quot;)) ) %&gt;% flextable::flextable() %&gt;% flextable::autofit() %&gt;% flextable::width(j = 1, width = 4.5, unit = &quot;in&quot;) %&gt;% flextable::width(j = 2, width = 1.5, unit = &quot;in&quot;) %&gt;% flextable::valign(valign = &quot;top&quot;) .cl-8bc8c8d2{}.cl-8bc1c474{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-8bc1c47e{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-8bc4aa90{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-8bc4aa9a{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-8bc4be04{width:4.5in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8bc4be0e{width:1.5in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8bc4be18{width:4.5in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8bc4be22{width:1.5in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}reviewbigramsA very lovely first visit to this iconic hotel bar! Wonderful service, without being intrusive at all! Very delicious cocktails and just generally all round, a very indulgent experience. Well worth visiting only for that 'once in a lifetime' experience, though do make sure you are feeling 'flush' it doesn't come cheap!lovely visitvisit iconiciconic barbar wonderfulwonderful serviceservice intrusiveintrusive deliciousdelicious cocktailscocktails roundround indulgentindulgent experienceexperience worthworth visitingvisiting lifetimelifetime experienceexperience feelingfeeling flushflush cheap "],["save.html", "1.7 Save", " 1.7 Save Combine the tokens and bigrams into cleaner reviews. Save the cleaned data for other analyses like topic modeling and sentiment analysis. review_words &lt;- token %&gt;% summarize(.by = review_id, review_words = paste(word, collapse = &quot; &quot;)) review_bigrams &lt;- bigram %&gt;% summarize(.by = review_id, review_bigrams = paste(bigram, collapse = &quot; &quot;)) hotel_prep &lt;- hotel_2 %&gt;% inner_join(review_words, by = join_by(review_id)) %&gt;% inner_join(review_bigrams, by = join_by(review_id)) save(hotel_prep, token, bigram, file = &quot;input/hotel_prepped.Rdata&quot;) "],["topicmodeling.html", "Chapter 2 Topic Modeling", " Chapter 2 Topic Modeling Topic models are unsupervised ML models that identify topics as clusters of words with an associated probability distribution, and a probability distribution of topics within each document. There are two commonly used models: LDA and STM. LDA, implemented in the topicmodels package, is the simpler model. STM, implemented in the stm package, incorporates document metadata into the model. Both topic models are generative models of word counts. That means they assume there is some process that generates text which is a mixture of topics composed of words which occur with varying probabilities. Think of the observed text document as the product of an algorithm that selected each word in two stages: 1) it sampled a topic from a probability distribution, then 2) it sampled a word from the topic’s word probability distribution. The object in topic modeling is to tune the hyperparameters that define those probability distributions. In a way, topic models do the opposite of what you might expect. They are not estimating the probability that each document is one of those topics. They assume all topics contribute to each document and instead estimate their relative contributions. More concisely, the models treat documents as a mixture of topics, and the topics as a mixture of words, and both the topics and topic words have probability distributions. The sum of topic proportions in document is one; the sum of word probabilities in a topic is one. This leads to two frameworks for thinking about topics. A topic’s prevalance measures the proportion of the document generated by it. The topic’s content is the probability distribution of words associated with it. LDA and STM differ only in how they handle these frameworks. STM defines covariates associated with prevalence and content while sTM does not.3 This section continues with the hotel data from Chapter 1 and follows the ideas from Nagelkerke (2020b) and the stm package vignette package. library(tidyverse) library(tidymodels) library(topicmodels) library(tidytext) library(stm) # library(tm) # generic text mining utilities. Great for DTMs. library(scales) library(glue) library(httr2) library(jsonlite) load(&quot;input/hotel_prepped.Rdata&quot;) glimpse(token) ## Rows: 1,112,307 ## Columns: 3 ## $ review_id &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … ## $ word &lt;chr&gt; &quot;pleasure&quot;, &quot;stay&quot;, &quot;night&quot;, &quot;recently&quot;, &quot;perfect&quot;, &quot;communi… ## $ n &lt;int&gt; 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, … References "],["lda.html", "2.1 LDA", " 2.1 LDA Latent Dirichlet allocation (LDA) is an instance of a general family of mixed membership models that decompose data into latent components. Latent refers to unidentified topics and Dirichlet refers to the type of distribution followed by the words in the the topics and by the topics in the documents. Algorithm LDA assumes each document is created by a generative process where topics are included according to probabilities and words are included in the topics according to probabilities. The LDA algorithm determines what those probabilities are. The algorithm is: For each document \\(d_i\\), randomly assign each word \\(w_j\\) to one of the \\(K\\) topics. Note that \\(w_j\\) may be assigned to a different topic in different documents. For each document, tabulate the number of words in each topic, a \\(d \\times K\\) matrix. For each word, tabulate the sum of occurrences across all documents, a \\(w \\times K\\) matrix. Resample a single instance of a word from the corpus and remove it from the analysis, decrementing the document’s topic count and the word’s topic count. Calculate the gamma matrix, \\(\\gamma\\), and the beta matrix, \\(\\beta\\). the gamma matrix, aka the topical prevalence, is the probability distribution of topics for each document, \\[p(t_k|d_i) = \\frac{n_{ik} + \\alpha}{N_i + K \\alpha}\\] were \\(n_{ik}\\) is the number of words in document \\(i\\) for topic \\(k\\), \\(N_i\\) is the total number of words in \\(i\\), and \\(\\alpha\\) is a hyperparameter. For each \\(d_i\\), \\(\\sum_{k \\in K} \\gamma_{ik} = 1\\). the beta matrix, aka the topical content, is the probability distribution of words for each topic, \\[p(w_j|t_k) = \\frac{m_{j,k} + \\beta}{\\sum_{j \\in V}m_{j,k} + V\\beta}\\] where \\(m_{j,k}\\) is the corpus-wide frequency count of word \\(w_j\\) to topic \\(k\\), \\(V\\) is the number of distinct words in the corpus, and \\(\\beta\\) is a hyperparameter. For each \\(t_k\\), \\(\\sum_{j \\in V} \\beta_{kj} = 1\\). Perform Gibbs sampling. Calculate the joint probability distribution of words for each document and topic, \\(p(w_j|t_k,d_i) = p(t_k|d_i)p(w_j|t_k)\\). Assign each word, \\(w_j\\), to the topic with the maximum joint probability. Repeat steps 3-6 for all of the words in all of the documents. Repeat steps 3-7 for a pre-determined number of iterations. LDA thus has 3 hyperparameters: the document-topic density factor, \\(\\alpha\\), the topic-word density factor, \\(\\beta\\), and the topic count, \\(K\\). \\(\\alpha\\) controls the number of topics expected per document (large \\(\\alpha\\) = more topics). \\(\\beta\\) controls the distribution of words per topic (large \\(\\beta\\) = more words). Ideally, you want a few topics per document and a few words per topic, so \\(\\alpha\\) and \\(\\beta\\) are typically set below one. \\(K\\) is set using a combination of domain knowledge, coherence, and exclusivity. Notice that LDA is a “bag of words” method. It does not consider the order of the tokens in the text, so where tokens are located what other tokens are nearby do not factor into the output. Data Preparation In addition to the cleaned text produced in Chapter 1, there are a few more data preparation tasks for LDA. Create a bag of words from the union of the word and bigram reviews to get all terms. Keep only the decent sized reviews (&gt;= 25 words). lda_dat &lt;- hotel_prep %&gt;% mutate(combined = paste(review_words, review_bigrams)) %&gt;% select(review_id, combined) %&gt;% unnest_tokens(output = &quot;word&quot;, input = combined) %&gt;% mutate(.by = review_id, n = n()) %&gt;% filter(n &gt;= 25) %&gt;% select(-n) lda_dat %&gt;% glimpse() ## Rows: 3,288,371 ## Columns: 2 ## $ review_id &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … ## $ word &lt;chr&gt; &quot;pleasure&quot;, &quot;stay&quot;, &quot;night&quot;, &quot;recently&quot;, &quot;perfect&quot;, &quot;communi… The next step is optional. If this is a predictive model, create a train/test split. You might even weight the splitting by rating (if that is the outcome variable) to ensure proportional coverage. hotel_split &lt;- rsample::initial_split(hotel_prep, prop = 3/4, strata = review_id) lda_train_0 &lt;- lda_dat %&gt;% inner_join(training(hotel_split) %&gt;% select(review_id, rating), by = join_by(review_id)) lda_test &lt;- lda_dat %&gt;% inner_join(testing(hotel_split) %&gt;% select(review_id, rating), by = join_by(review_id)) glimpse(lda_train_0) ## Rows: 2,460,378 ## Columns: 3 ## $ review_id &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ word &lt;chr&gt; &quot;lovely&quot;, &quot;visit&quot;, &quot;iconic&quot;, &quot;bar&quot;, &quot;wonderful&quot;, &quot;service&quot;, … ## $ rating &lt;fct&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, … glimpse(lda_test) ## Rows: 827,993 ## Columns: 3 ## $ review_id &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … ## $ word &lt;chr&gt; &quot;pleasure&quot;, &quot;stay&quot;, &quot;night&quot;, &quot;recently&quot;, &quot;perfect&quot;, &quot;communi… ## $ rating &lt;fct&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, … Low frequency tokens impact the topic model analysis. 49% of the data is composed of tokens that appear &lt;=3 times. Just use the high frequency tokens, the ones occurring at least 4 times in the training data. This is (hopefully) enough to learn topics from tokens that occur together in reviews. Nagelkerke (2020b) explains that TF-IDF, which combines the within-document token frequency and document frequency, is not always the best way to whittle down the token set. When documents are small, which is common in online reviews, the within document frequency is low and the IDF part is over-weighted. lda_train_0 %&gt;% count(word, name = &quot;token_n&quot;) %&gt;% mutate(token_n = if_else(token_n &gt; 20, 20, token_n)) %&gt;% count(token_n) %&gt;% mutate(pct = n / sum(n), cum_pct = cumsum(pct)) %&gt;% ggplot(aes(x = token_n)) + geom_col(aes(y = pct)) + geom_line(aes(y = cum_pct)) + scale_y_continuous(labels = percent_format(1)) + labs(x = &quot;Token Frequency&quot;, y = &quot;Pct of Tokens&quot;, title = &quot;Token Frequency&quot;) lda_train &lt;- lda_train_0 %&gt;% mutate(.by = word, corpus_token_n = n()) %&gt;% mutate(.by = c(review_id, word), review_token_n = n()) %&gt;% filter(corpus_token_n &gt;= 4) bind_rows( `high freq words` = lda_train %&gt;% summarize(total_words = n(), distinct_words = n_distinct(word)), `low freq words` = anti_join(lda_train_0, lda_train, by = join_by(review_id, word)) %&gt;% summarize(total_words = n(), distinct_words = n_distinct(word)), .id = &quot;partition&quot; ) %&gt;% mutate(total_pct = total_words / sum(total_words) * 100, distinct_pct = distinct_words / sum(distinct_words) * 100) %&gt;% select(partition, total_words, total_pct, distinct_words, distinct_pct) ## # A tibble: 2 × 5 ## partition total_words total_pct distinct_words distinct_pct ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 high freq words 2420232 98.4 15589 51.2 ## 2 low freq words 40146 1.63 14882 48.8 Now we can create a document term matrix (DTM). Again from Nagelkerke (2020b), you might expect the document specific term frequency (review_token_n), however I use the overall token frequency corpus_token_n to give more emphasis to terms that are more frequent in general. Reviews are short compared to books or articles, so the probability of a token occurring repeatedly in a review is low. lda_dtm &lt;- cast_dtm(lda_train, document = review_id, term = word, value = corpus_token_n) dim(lda_dtm) ## [1] 17452 15589 # Average token frequency. sum(lda_dtm) / sum(lda_dtm != 0) ## [1] 5020.583 Fit There are several parameters you might tweak for the model fit. The biggest surprise is that you set the number of topics, k. This article explains the harmonic mean method for optimization, but Nagelkerke (2020b) suggests sticking with the art vs science method and pick your own k. The model fit took about 3 minutes to run. I ran it once then saved the result. lda_fit &lt;- LDA(lda_dtm, k = 3) saveRDS(lda_fit, file = &quot;input/lda_fit.RDS&quot;) lda_fit &lt;- readRDS(file = &quot;input/lda_fit.RDS&quot;) The fitted object contains two matrices. The phi matrix is the distribution of tokens (cols) over topics (rows). The theta matrix is the distribution of documents (rows) over topics (cols). The row sum is 1 for each matrix (sum of topic probabilities, some of document probabilities). In each case, the values are probabilities that sum to 1 for each topic. lda_phi &lt;- posterior(lda_fit) %&gt;% pluck(&quot;terms&quot;) %&gt;% as.matrix() dim(lda_phi) ## [1] 3 15694 sum(lda_phi[1, ]) ## [1] 1 lda_theta &lt;- posterior(lda_fit) %&gt;% pluck(&quot;topics&quot;) %&gt;% as.matrix() dim(lda_theta) ## [1] 17458 3 sum(lda_theta[1, ]) ## [1] 1 The tidytext::tidy() function calculates the beta matrix then pivots-longer into a [topic, term, beta] data frame. lda_tidy &lt;- tidy(lda_fit) lda_top_tokens &lt;- lda_tidy %&gt;% mutate(topic = factor(paste(&quot;Topic&quot;, topic))) %&gt;% group_by(topic) %&gt;% slice_max(order_by = beta, n = 10) %&gt;% ungroup() lda_top_tokens %&gt;% ggplot(aes(x = beta, y = reorder_within(term, by = beta, within = topic))) + geom_col() + scale_y_reordered() + facet_wrap(facets = vars(topic), scales = &quot;free_y&quot;) + labs(y = NULL, title = &quot;LDA Top 10 Terms&quot;) There is a downside to this evaluation. Popular words like “stay” appear at or near the top in all three topics. You might want to look at relative popularity instead: the popularity within the topic divided by overall popularity. That’s problematic too because words that only appear in few reviews will pop to the top. What you want is a combination of both absolute term probability and relative term probability. LDAvis::serVis() can help you do that. Unfortunately, the plot from LDAvis::serVis() is interactive and does not render in the RMarkdown notebook html, so below is just a screenshot of the code chunk output. The left side shows the topic sizes (documents) and topic distances. The right side shows the most important tokens. doc_length &lt;- lda_train %&gt;% count(review_id) %&gt;% pull(n) # vocabulary: unique tokens vocab &lt;- colnames(lda_phi) # overall token frequency term_frequency &lt;- lda_train %&gt;% count(word) %&gt;% arrange(match(word, vocab)) %&gt;% pull(n) # create JSON containing all needed elements json &lt;- LDAvis::createJSON(lda_phi, lda_theta, doc_length, vocab, term_frequency) LDAvis::serVis(json) Iterate through the model by tweaking k, excluding words that dominate and suppress more interesting subdomains, and/or changing the minimal token frequency to focus on more/less dominant tokens. You can also change the document sampling strategy to promote interesting domains, like we did when we over sampled the low hotel ratings. Topic Labeling with ChatGPT To OpenAI’s ChatGPT API service requires an API token. I created one at https://platform.openai.com/api-keys and saved it to .Renviron. See usethis::edit_r_environ(). # Create a function to send each list of topic words to Open AI as a separate request. get_topic_from_openai &lt;- function(prompt) { my_resp &lt;- request(&quot;https://api.openai.com/v1/chat/completions&quot;) %&gt;% req_headers(Authorization = paste(&quot;Bearer&quot;, Sys.getenv(&quot;OPENAI_API_KEY&quot;))) %&gt;% req_body_json(list( model = &quot;gpt-3.5-turbo&quot;, # Temperature [0,2] controls creativity, predictable -&gt; variable. temperature = 1, messages = list( # System prompt sets repeated context. It is prefixed to prompts. list( role = &quot;system&quot;, content = paste(&quot;You are a topic modeling assistant. You accept lists &quot;, &quot;of words in a topic and summarizes them into a salient &quot;, &quot;topic label of five words or less. How would you &quot;, &quot;summarize the following list? The list is in descending &quot;, &quot;order of importance, so the first term in the list is most &quot;, &quot;strongly tied to the topic. Return just the topic label and &quot;, &quot;nothing else.&quot;) ), list( role = &quot;user&quot;, content = prompt ) ) )) %&gt;% req_perform() %&gt;% resp_body_json() %&gt;% pluck(&quot;choices&quot;, 1, &quot;message&quot;, &quot;content&quot;) } lda_topics &lt;- lda_top_tokens %&gt;% nest(data = term, .by = topic) %&gt;% mutate( token_str = map(data, ~paste(.$term, collapse = &quot;, &quot;)), topic_lbl = map_chr(token_str, get_topic_from_openai), topic_lbl = str_remove_all(topic_lbl, &#39;\\\\&quot;&#39;), topic_lbl = snakecase::to_any_case(topic_lbl, &quot;title&quot;) ) %&gt;% select(-data) # Save to file system to avoid regenerating. saveRDS(lda_topics, file = &quot;input/lda_topics.RDS&quot;) lda_topics &lt;- readRDS(file = &quot;input/lda_topics.RDS&quot;) Let’s see the topic summary with the newly generated labels. lda_top_tokens %&gt;% inner_join(lda_topics, by = join_by(topic)) %&gt;% mutate(topic_lbl = str_wrap(topic_lbl, 25)) %&gt;% ggplot(aes(x = beta, y = reorder_within(term, by = beta, within = topic_lbl))) + geom_col() + scale_y_reordered() + facet_wrap(facets = vars(topic_lbl), scales = &quot;free_y&quot;) + labs(y = NULL, title = &quot;LDA Top 10 Terms&quot;) TODO I still need to learn more about Held-out Likelihood (Wallach et al., 2009). Semantic Coherence Exclusivity. Generally, the greater the number of topics in a model, the lower the quality of the smallest topics. One way around this is simply hiding the low-quality topics. The coherence measure evaluates topics. References "],["stm.html", "2.2 STM", " 2.2 STM STM incorporates arbitrary document metadata into the topic model. The goal of STM is to discover topics and estimate their relationship to the metadata. Topical prevalence. If what topics are discussed depends on the metadata features, control for them in the prevalence (the gamma matrix). E.g., negative hotel reviews might focus on different topics than positive reviews. Topical content. If how a topic is discussed depends on metadata features, control for them in the content (the beta matrix). E.g., visitors from the US may discuss hotels differently than visitors from the UK. Algorithm STM is similar to LDA in that it assumes each document is created by a generative process where topics are included according to probabilities (topical prevalence) and words are included in the topics (topical content) according to probabilities. STM adds the possibility of including topical prevalence covariates, and topical content covariates. Data Preparation Chapter 1 prepped the data by correcting misspellings, lemmatizing words, and removing stop words The stm package represents a text corpus as an object with three components: a sparse matrix of counts by document and vocabulary word vector index, the vocabulary word vector, and document metadata. I used STM for my Battle of the Bands project. stm::textProcessor() is essentially a wrapper around the tm package. It produces a list object with three main components: vocab, a named vocabulary vector, one element per distinct word. documents, a list of matrices, one per document. Each matrix has 2 rows of integers. The first row is indices from the vocabulary vector; the second is their associated word counts. This is a concise representation of a document term matrix. The processing step sometimes removes a few documents if they are empty after removing stopwords, numbers, est. meta, a metadata data frame, one row per document containing the feature cols. This step took about 3 minutes to run, so I ran it once then saved the result. stm_processed &lt;- stm::textProcessor( documents = hotel_prep$review_words, metadata = hotel_prep %&gt;% select(rating, reviewer_loc, review), lowercase = FALSE, removestopwords = FALSE, removenumbers = FALSE, removepunctuation = FALSE, stem = FALSE ) saveRDS(stm_processed, file = &quot;input/stm_processed.RDS&quot;) stm_processed &lt;- readRDS(file = &quot;input/stm_processed.RDS&quot;) After processing, prepare the corpus by removing infrequently used words. stm::prepDocuments() removes infrequently appearing words, and removes any documents that contain no words after processing and removing words. 1% (about 230) is a conservative threshold. The plot below shows that removing even a few words will remove some documents, but you can still retain most document plotRemoved(stm_processed$documents, lower.thresh = seq(100, 4000, by = 100)) stm_prepared &lt;- stm::prepDocuments( stm_processed$documents, stm_processed$vocab, stm_processed$meta, lower.thresh = length(stm_processed$documents) * .01 ) ## Removing 26136 of 26886 terms (239369 of 943763 tokens) due to frequency ## Removing 21 Documents with No Words ## Your corpus now has 23352 documents, 750 terms and 704394 tokens. Fit The stm package allows you to either specify the number of topics (K) to identify, or it can choose an optimal number by setting parameter K = 0. The resulting probability distribution of topic words (beta matrix) will be a K x rlength(stm_prepared$vocab)matrix. The probability distribution of topics (gamma matrix, theta in the stm package) will be a 23,352 x K matrix. I expect topics to correlate with the review rating, soratingis a prevalence covariate, and I expect word usage to correlate with the reviewer location, soreviewer_loc` is a topical content covariate. Running the model with K = 3 threw an error. Error: chol(): decomposition failed I set it to K = 4 and it worked. It took a couple minutes to run, so I ran it once then saved the result. set.seed(1234) stm_fit &lt;- stm::stm( stm_prepared$documents, stm_prepared$vocab, K = 4, prevalence = ~ rating, content = ~ reviewer_loc, data = stm_prepared$meta, init.type = &quot;Spectral&quot;, verbose = FALSE ) saveRDS(stm_fit, file = &quot;input/stm_fit.RDS&quot;) stm_fit &lt;- readRDS(file = &quot;input/stm_fit.RDS&quot;) Interpret The fit summary has three sections showing the tops words. The first section shows the prevalence model; the second shows the topical content model; and the third shows their interaction. summary(stm_fit) ## A topic model with 4 topics, 23352 documents and a 750 word dictionary. ## Topic Words: ## Topic 1: favorite, notch, corinthia, awesome, property, exceed, mandarin ## Topic 2: neighborhood, convenient, victoria, heathrow, ride, underground, rembrandt ## Topic 3: promenade, beaufort, celebration, celebrate, savoy, surrounding, american ## Topic 4: closet, clothe, move, desk, smell, light, tub ## ## Covariate Words: ## Group Other: renovation, smile, renovate, direct, miss, europe, directly ## Group United Kingdom: partner, whilst, saturday, sunday, party, fab, round ## Group United States: hotel&#39;s, spectacular, elegant, accommodate, perfection, tate, renovation ## ## Topic-Covariate Interactions: ## Topic 1, Group Other: bridge, theater, boutique, blackfriars, lane, modern, pool ## Topic 1, Group United Kingdom: wed, fault, trouble, anniversary, brilliant, penny, faultless ## Topic 1, Group United States: exquisite, club, square, trafalgar, city, gorgeous, tate ## ## Topic 2, Group Other: british, share, block, host, store, ridgemount, advice ## Topic 2, Group United Kingdom: apex, appoint, comfy, blackfriars, toiletry, spacious, club ## Topic 2, Group United States: theater, phone, wonderful, tour, bridge, express, block ## ## Topic 3, Group Other: wed, anniversary, fab, attend, partner, wow, round ## Topic 3, Group United Kingdom: daughter, cake, rush, pianist, ritz, birthday, pass ## Topic 3, Group United States: classic, housekeeping, amenity, kid, speak, miss, level ## ## Topic 4, Group Other: club, pillow, executive, cold, fall, type, fan ## Topic 4, Group United Kingdom: lift, downstairs, miss, rumpus, bottle, car, complimentary ## Topic 4, Group United States: stylish, feature, renovation, black, elevator, boutique, smoke ## If this was just a regular topic model, or a prevalence or content model, we’d see top words by 4 metrics: highest probability, FREX, lift, and score. Highest probability weights words by their overall frequency. FREX weights words by their overall frequency and how exclusive they are to the topic. Lift weights words by dividing by their frequency in other topics, therefore giving higher weight to words that appear less frequently in other topics. Score divides the log frequency of the word in the topic by the log frequency of the word in other topics. Let’s fit a new model just to show that. set.seed(1234) stm_fit_simple &lt;- stm::stm( stm_prepared$documents, stm_prepared$vocab, K = 4, # prevalence = ~ rating, # content = ~ reviewer_loc, data = stm_prepared$meta, init.type = &quot;Spectral&quot;, verbose = FALSE ) saveRDS(stm_fit_simple, file = &quot;input/stm_fit_simple.RDS&quot;) stm_fit_simple &lt;- readRDS(file = &quot;input/stm_fit_simple.RDS&quot;) stm::labelTopics(stm_fit_simple) ## Topic 1 Top Words: ## Highest Prob: stay, london, staff, service, excellent, restaurant, location ## FREX: spa, corinthia, concierge, luxury, mondrian, love, pool ## Lift: personal, oriental, mandarin, notch, corinthia, pleasure, property ## Score: personal, london, service, stay, spa, corinthia, location ## Topic 2 Top Words: ## Highest Prob: breakfast, walk, room, stay, location, clean, london ## FREX: tube, station, museum, hyde, street, bus, walk ## Lift: hyde, paddington, albert, ridgemount, bus, kensington, rhodes ## Score: hyde, tube, station, museum, bus, walk, clean ## Topic 3 Top Words: ## Highest Prob: savoy, bar, tea, lovely, staff, special, birthday ## FREX: afternoon, birthday, savoy, cocktail, cake, american, treat ## Lift: scone, pianist, cake, piano, beaufort, afternoon, sandwich ## Score: scone, savoy, afternoon, birthday, tea, cake, cocktail ## Topic 4 Top Words: ## Highest Prob: room, night, check, stay, bed, book, bathroom ## FREX: check, charge, bath, call, floor, pay, issue ## Lift: rumpus, robe, mirror, wake, smell, curtain, corridor ## Score: rumpus, room, check, shower, bed, bathroom, floor It is interesting that the top terms for UK did not include “restaurant” or “location”. The top terms for the US did not include “excellent” or “amaze”, but did include “love”. stm_tidy &lt;- tidy(stm_fit) stm_top_tokens &lt;- stm_tidy %&gt;% mutate(topic = factor(paste(&quot;Topic&quot;, topic))) %&gt;% group_by(topic, y.level) %&gt;% slice_max(order_by = beta, n = 10) %&gt;% ungroup() stm_top_tokens %&gt;% filter(topic == &quot;Topic 1&quot;) %&gt;% ggplot(aes(x = beta, y = reorder_within(term, by = beta, within = topic))) + geom_col() + scale_y_reordered() + facet_wrap(facets = vars(y.level), scales = &quot;free_x&quot;, ncol = 3) + labs(y = NULL, title = &quot;STM Top 10 Terms for Topic 1&quot;) As we did with the LDA model, we can assign topic labels with Open AI. stm_topics &lt;- stm_top_tokens %&gt;% nest(data = term, .by = topic) %&gt;% mutate( token_str = map(data, ~paste(.$term, collapse = &quot;, &quot;)), topic_lbl = map_chr(token_str, get_topic_from_openai), topic_lbl = str_remove_all(topic_lbl, &#39;\\\\&quot;&#39;), topic_lbl = snakecase::to_any_case(topic_lbl, &quot;title&quot;) ) %&gt;% select(-data) # Save to file system to avoid regenerating. saveRDS(stm_topics, file = &quot;input/stm_topics.RDS&quot;) stm_topics &lt;- readRDS(file = &quot;input/stm_topics.RDS&quot;) stm_topics ## # A tibble: 4 × 3 ## topic token_str topic_lbl ## &lt;fct&gt; &lt;list&gt; &lt;chr&gt; ## 1 Topic 1 &lt;chr [1]&gt; Luxury Stay in London ## 2 Topic 2 &lt;chr [1]&gt; Comfortable Stay Near London ## 3 Topic 3 &lt;chr [1]&gt; Afternoon Tea at the Savoy ## 4 Topic 4 &lt;chr [1]&gt; Hotel Stay Another way to evaluate the model is to print reviews that are most representative of the topic. Topic 1 stm::findThoughts( stm_fit, n = 3, texts = stm_prepared$meta$review, topics = 1, meta = stm_prepared$meta ) ## ## Topic 1: ## To those who have stayed here in the past and have said anything negative of this hotel and services are foolish beyond belief. This hotel receives a five star plus recognition in my book of first class travels. It is one on my top ten list of first class hotel experiences. In fact I would confidently say that this hotel and it&#39;s staff ranks as the top three luxury hotel experiences that I have had so far. First and foremost I must say that the Fine Hotels and Resorts Pkg. is well worth it at this hotel. They truly went above and beyond to recognize our patronage. Second, this hotel went above and beyond to recognize our anniversary. Third, this hotel boasts the most superior, thoughtful and careful staff service that I have ever experienced in a first class hotel. While the hotel itself has an old world charm to it, it never underestimates the luxury offerings for it&#39;s guests. There is absolutely nothing that I can find fault during our week stay at is hotel. I would highly recommend this hotel for anyone looking for luxury and service. If in London again, I would seriously consider staying here again. I must be honest when I say that two years ago I tried the Mandarin hotel chain for my very first time in NYC and was disappointed. Since then I have tried the Mandarin Las Vegas and now London and both times I have been very impressed. I am confident to say that Mandarin Oriental Hotels is now my new favorite first class hotel chain. If you like first class hotels and luxury this hotel is for you. Enjoy it as much that I did. Bravo to the entire management team at the Mandarin Hyde Park London for maintaining such an impeccable property and focus on customer satisfaction. Every staff member at this hotel must receive a huge appreciation and thanks on behalf of our entire family for giving us a ver memorable and stellar holiday stay. Thank you! ## This was our third trip, the Corinthia London is a perfect 5 star and could be rated higher, the service is impeccable which is sometimes difficult to find in London, the food is fantastic and all the hotel has to offer is top notch with an a beautiful Spa in a prime location, it actually has everything you could want, the rooms and suites are beautifully appointed and very comfortable, it is currently our favourite hotel in central London and we will back and recommend to all, it&#39;s also very family friendly as well as being an ideal romantic stay or a luxurious base to experience the city, world class! ## I stayed at the Corinthia for three nights during a business trip and REALLY liked this hotel. I&#39;ve stayed at a number of other luxury 5-star properties in London and this is by far my favorite. The rooms are beautiful and well appointed and the spa is just amazing. More importantly, the service is outstanding and it feels like everyone really cares and is genuinely concerned about making sure you are enjoying your visit. Highly recommend staying here. "],["sentimentanalysis.html", "Chapter 3 Sentiment Analysis", " Chapter 3 Sentiment Analysis Sentiment analysis is the extraction of the emotional intent of text. You can classify the polarity (positive | negative) or sentiment (angry | sad | happy | …) at the document, sentence, or feature level. This section continues with the hotel data from 1. load(&quot;input/hotel_prepped.Rdata&quot;) hotel_0 &lt;- hotel_prep glimpse(token) ## Rows: 1,112,307 ## Columns: 3 ## $ review_id &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … ## $ word &lt;chr&gt; &quot;pleasure&quot;, &quot;stay&quot;, &quot;night&quot;, &quot;recently&quot;, &quot;perfect&quot;, &quot;communi… ## $ n &lt;int&gt; 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, … "],["subjectivity-lexicons.html", "3.1 Subjectivity Lexicons", " 3.1 Subjectivity Lexicons A subjectivity lexicon is a predefined list of words associated with emotional context such as positive/negative. Subjectivity lexicons are typically short (a few thousand words), but work because of Zipf’s law which holds that the nth-ranked item in a frequency table has a frequency count equal to 1/n of the top-ranked item. So infrequently used words are used very infrequently. There are three common sentiment lexicons. Bing is common for polarity scoring, AFINN for emotion classification. NRC is a less common option for emotion classification. Bing classifies words as positive or negative. bing &lt;- tidytext::get_sentiments(&quot;bing&quot;) %&gt;% # remove dups filter(!word %in% c(&quot;envious&quot;, &quot;enviously&quot;, &quot;enviousness&quot;)) bing %&gt;% count(sentiment) %&gt;% adorn_totals() %&gt;% flextable::flextable() %&gt;% flextable::autofit() .cl-a05b2af6{}.cl-a0530e16{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-a0530e20{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-a056ea0e{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a056ea22{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a057154c{width:0.941in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a057156a{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a057156b{width:0.941in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a0571574{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a0571588{width:0.941in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a0571589{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a0571592{width:0.941in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a0571593{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}sentimentnnegative4,778positive2,002Total6,780 AFINN, by Finn Arup Nielsen, associates words with a manually rated valence integer between -5 (negative) and +5 (positive). afinn &lt;- tidytext::get_sentiments(&quot;afinn&quot;) afinn %&gt;% count(value) %&gt;% adorn_totals() %&gt;% flextable::flextable() %&gt;% flextable::autofit() .cl-a08c827c{}.cl-a08652b2{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-a08652b3{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-a088c2ea{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a088c2f4{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a088d1b8{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a088d1c2{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a088d1c3{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a088d1cc{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a088d1cd{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a088d1ce{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a088d1d6{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a088d1d7{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a088d1e0{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a088d1e1{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a088d1e2{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a088d1e3{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a088d1ea{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a088d1eb{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}valuen-516-443-3264-2966-13090112082448317244555Total2,477 NRC lexicon associates words with eight emotions corresponding to the second level of Plutchik’s Wheel of Emotions and two sentiments (negative and positive). NRC was created by manual annotation on a crowd sourcing platform (see this). nrc &lt;- tidytext::get_sentiments(&quot;nrc&quot;) nrc %&gt;% count(sentiment) %&gt;% adorn_totals() %&gt;% flextable::flextable() %&gt;% flextable::autofit() .cl-a0afad6a{}.cl-a0a989bc{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-a0a989c6{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-a0abfb0c{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a0abfb16{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a0ac09da{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a0ac09e4{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a0ac09ee{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a0ac09ef{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a0ac09f0{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a0ac09f8{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a0ac09f9{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a0ac09fa{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a0ac0a02{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a0ac0a03{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a0ac0a04{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a0ac0a05{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}sentimentnanger1,245anticipation837disgust1,056fear1,474joy687negative3,316positive2,308sadness1,187surprise532trust1,230Total13,872 "],["polarity-scoring.html", "3.2 Polarity Scoring", " 3.2 Polarity Scoring Two packages measure text polarity. The simpler one tidytext: unnest tokens, join to the Bing lexicon, and calculate the net of positive minus negative polarity counts. sentimentr is more sophisticated in that it takes into account valence shifters, surrounding words that change the intensity of a sentiment (e.g., “very”) or switch its direction (e.g., “not”).4 tidytext The tidytext way to score polarity is to tag words as “positive” and “negative” using the Bing lexicon, then calculate the difference in counts. The qdap and sentimentr packages correct for text length by dividing by \\(\\sqrt{n}\\). It is useful to capture the positive and negative words back in the main data frame for explaining how the polarity score was calculated. polarity_bing &lt;- token %&gt;% left_join(bing, by = &quot;word&quot;, relationship = &quot;many-to-one&quot;) %&gt;% summarize(.by = c(review_id, sentiment), n = n(), words = list(word)) %&gt;% pivot_wider(names_from = sentiment, values_from = c(n, words), values_fill = list(n = 0)) %&gt;% select(-c(n_NA, words_NA)) %&gt;% inner_join(hotel_0 %&gt;% select(review_id, word_cnt), by = &quot;review_id&quot;) %&gt;% mutate( polarity = (n_positive - n_negative) / sqrt(word_cnt), polarity_desc = if_else(polarity &gt;= 0, &quot;Positive&quot;, &quot;Negative&quot;) ) polarity_bing %&gt;% filter(review_id == 520) ## # A tibble: 1 × 8 ## review_id n_positive n_negative words_positive words_negative word_cnt ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;list&gt; &lt;list&gt; &lt;int&gt; ## 1 520 0 0 &lt;NULL&gt; &lt;NULL&gt; 47 ## # ℹ 2 more variables: polarity &lt;dbl&gt;, polarity_desc &lt;chr&gt; polarity_afinn &lt;- token %&gt;% inner_join(afinn, by = &quot;word&quot;, relationship = &quot;many-to-one&quot;) %&gt;% summarize(.by = review_id, sentiment = sum(value), words = list(word)) %&gt;% inner_join(hotel_0 %&gt;% select(review_id, word_cnt), by = &quot;review_id&quot;) %&gt;% mutate( polarity = sentiment / sqrt(word_cnt), polarity_desc = if_else(polarity &gt;= 0, &quot;Positive&quot;, &quot;Negative&quot;) ) # Attach to main data frame hotel_1 &lt;- hotel_0 %&gt;% left_join(polarity_bing %&gt;% select(review_id, polarity, words_positive, words_negative) %&gt;% rename_with(~paste0(&quot;bing_&quot;, .x)), by = join_by(review_id == bing_review_id)) %&gt;% left_join(polarity_afinn %&gt;% select(review_id, polarity) %&gt;% rename_with(~paste0(&quot;afinn_&quot;, .x)), by = join_by(review_id == afinn_review_id)) Let’s see how the polarity scores compare. hotel_1 %&gt;% pivot_longer(cols = c(bing_polarity, afinn_polarity), names_to = &quot;lexicon&quot;, values_to = &quot;polarity&quot;) %&gt;% filter(!is.na(polarity)) %&gt;% ggplot(aes(x = polarity, y = fct_rev(property))) + geom_boxplot() + facet_wrap(facets = vars(lexicon)) + labs(title = &quot;Review polarity&quot;, x = NULL, y = NULL, caption = glue::glue(&quot;Bing Polarity = (n_pos - n_neg) / sqrt(n_words)\\n&quot;, &quot;AFINN Polarity = sentiment / sqrt(n_words)&quot;)) The two lexicons are similar. The data set includes a rating (1-5). I’ll stick with Bing going forward for convenience. The polarity score should correlate with the rating. hotel_1 %&gt;% filter(!is.na(bing_polarity)) %&gt;% ggplot(aes(x = as_factor(rating), y = bing_polarity)) + geom_jitter(width = 0.2, alpha = 0.3, color = &quot;#5DA5DA&quot;, size = 1) + geom_boxplot(alpha = 0) + theme_minimal() + labs(title = &quot;Polarity is associated with overall Likert score&quot;, x = &quot;Overall Likert Rating&quot;, y = &quot;Polarity Score&quot;) Sentiment increases with Likert rating, but there are many reviews with a rating of 5 and a polarity score &lt;0. In some cases this is because the reviewer interpreted the scale incorrectly. You can use polarity scores to identify problematic reviews like these. hotel_1 %&gt;% mutate( problematic = case_when( (rating == &quot;1-2&quot; &amp; bing_polarity &gt; 0.5) ~ &quot;Too Low&quot;, (rating == &quot;5&quot; &amp; bing_polarity &lt; -.5) ~ &quot;Too High&quot;, TRUE ~ &quot;Other&quot; ) ) %&gt;% filter(problematic %in% c(&quot;Too High&quot;, &quot;Too Low&quot;)) %&gt;% group_by(problematic) %&gt;% slice_max(order_by = abs(bing_polarity), n = 1) %&gt;% select(problematic, rating, bing_polarity, review) %&gt;% flextable::flextable() %&gt;% flextable::autofit() %&gt;% flextable::valign(valign = &quot;top&quot;) .cl-a2bfbdde{}.cl-a2b5e5ca{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-a2b5e5d4{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-a2ba46d8{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a2ba46ec{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a2ba46ed{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a2ba46f6{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a2ba6140{width:1.072in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a2ba6141{width:0.671in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a2ba614a{width:1.157in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a2ba6154{width:399.399in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a2ba6155{width:1.072in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a2ba615e{width:0.671in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a2ba615f{width:1.157in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a2ba6168{width:399.399in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a2ba6169{width:1.072in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a2ba616a{width:0.671in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a2ba6172{width:1.157in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a2ba6173{width:399.399in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}problematicratingbing_polarityreviewToo High5-0.9801961Some design faults in the bathroom - no stool, misplaced grab handles and vanity mirror. Very disappointing experience in Savoy Grill. Good quality ingredients but poorly presented and tasteless. Numerous mistakes in service including charging for expensive drinks which we did not have. Service charge revoked and booking for following night cancelled.Too Low1-21.1152378Having stayed at several Morgans Hotels on both sides of the pond, I was really looking forward to checking in to the Mondrian London. I have to say I was bitterly disappointed and left questioning whether this hotel was actually a Morgans property. Normally the second you step into any Morgans property, you instantly know youre in a Morgans Hotel. They always have the wow factor &amp;amp; the dcor has such a quirky, distinctive style, and the service is always exemplary without being stuffy. The same cannot be said about Mondrian London- we could have been in any generic high-end hotel in the world, and some of the staff behaviour we witnessed was highly sub-standard. The room was nice, but nice is not a word I would normally use to describe a Morgans room- its usually amazing, iconic, incredible. It was stylish but had no soul &amp;amp; dcor wise we could have been in a room in any hotel in the world. However the view of the Thames &amp;amp; St Pauls was fantastic and was a redemptive feature &amp;amp; the large marble bathroom, which is synonymous with Morgans was amazing &amp;amp; without doubt the highlight of the room. The bed was enormous, but was so hard it was like sleeping on breeze blocks- possibly one of the worst nights sleep Ive ever had- had to get up in the night &amp;amp; go &amp;amp; sleep on the couch- not really what you expect from a 500 room. We had aperitifs in the Dandelyan bar- again the dcor is completely unremarkable- nothing special when compared to the likes of the groups other bars, but was comfortable enough. We had really high hopes for this bar, as it is apparently run by an award winning mixologist, so we couldnt wait to sample to cocktails- and wait we did- while we pored over the enormous, incomprehensible menu. It was more like a botanical reference book than a cocktail menu- gargantuan in size and too wordy with no comprehensible flow. People know what they like to drink- either in terms of the spirit base of their cocktail (vodka, rum, gin, whiskey), or the type of drink they prefer (martini, flute, long etc..), therefore it is a sensible idea to arrange menus in a way that people can easily access the information they are looking for, as most high end places do. We got so fed up of trying to look for something suitable that we gave up &amp;amp; ordered a glass of Champagne- which was served warm &amp;amp; flat!!!! Quite ironic that these supposed award winning bartenders, werent even able to get a glass of Champagne right! But the waitress- not sure of her name, but she was of an eastern heritage- was very welcoming and friendly with a constant smile, so made up for the poor drinks. We went through to dinner, and it was fantastic, the food was all executed perfectly &amp;amp; really tasty &amp;amp; well presented. Service was also very good, but we found the waiter to be quite formal and stiff which was quite surprising in a Morgans. He was very efficient and slick, but wasnt very engaging and definitely needed to smile more. The dcor of the restaurant was very elegant and had great views across the Thames. There was a clear managerial presence on the floor, which always instils confidence in you as a diner. The Irish manager checked on our table, and the other tables around us, a touch that was very much appreciated. Breakfast was also excellent there. I would wholeheartedly recommend the Sea Containers restaurant, as a stand-alone venue. We had drinks up at the Rumpus Room &amp;amp; were welcomed by a very colourful character who instantly put a smile on our faces &amp;amp; gave us a very warm welcome (not sure if he was the manager or host- gentleman with a beard &amp;amp; earrings). Again the bar was nice dcor wise, it had a classy feel to it &amp;amp; the views across the Thames were spectacular. This time we opted to sit at the bar, so we could speak to the bartenders about the drinks &amp;amp; avoid another negative experience like we had in Dandelyan. We were served by great bartender- cant recall his name- blue eyed chap who I believe told us he was from Hungary. He was friendly &amp;amp; chatty &amp;amp; made us feel welcome without being intrusive. He made us some bespoke cocktails based on our preferences, which were exquisite. Faith restored, we opted to order the next round from the menu- big mistake- they pre make their cocktails in batches &amp;amp; literally just pour it out of a bottle onto ice!!! You may expect this kind of cutting corners from a high-volume, low-end venue, but we certainly did not expect this from a supposed high-end cocktail bar in a luxury hotel!!!! When you are paying around 15 for a cocktail, plus service charge, you expect your drinks to be freshly made &amp;amp; for there to be a bit more service than simply opening a bottle! I couldnt believe what I was seeing! The bartender made us some more bespoke cocktails instead, which again were great- if you visit this bar ask for the bartenders to make you something based on your likes &amp;amp; avoid their menu. We moved to a table &amp;amp; had several different waitresses check on us &amp;amp; serve us, all of whom were friendly &amp;amp; attentive. The ambience was good in the bar &amp;amp; it is great for people watching, but one thing we did notice, which we thought was highly inappropriate was the lady with the short hair &amp;amp; American accent -not sure what her role was but she seemed to work there, as went behind the bar &amp;amp; was talking (well shouting across the room actually) to all the other staff- openly imbibing in the bar! In our experience of high end hotels, the staff dont normally openly drink to the point of belligerence in front of the guests during hours of service, so we were quite shocked by this &amp;amp; didnt think having a drunk member of staff on the floor was acceptable. So this &amp;amp; the pre made cocktails let this bar down &amp;amp; clouded our otherwise good experience. Overall I would say this hotel &amp;amp; its facilities are mediocre at best. They are ok, but when you are paying five star prices, you expect a five star experience. If you are a Morgans aficionado like me, then avoid this place and stay at Sanderson or St Martins Lane instead, where you will receive the full Morgans experience. But I would certainly recommend the Sea Containers restaurant, excellent food &amp;amp; good service in a beautiful location. And if you dont mind a drunken member of staff shouting across the room &amp;amp; having to sit at the bar to order with the bartenders, then I would also recommend Rumpus Room. The polarity words can help explain why some hotels rated poor or excellent. token %&gt;% inner_join(hotel_1 %&gt;% filter(rating %in% c(&quot;1-2&quot;, &quot;5&quot;)), by = join_by(review_id)) %&gt;% filter(!word %in% c(&quot;hotel&quot;, &quot;stay&quot;, &quot;night&quot;)) %&gt;% filter((rating == &quot;5&quot; &amp; bing_polarity &gt; 0) | (rating == &quot;1-2&quot; &amp; bing_polarity &lt; 0)) %&gt;% count(rating, word) %&gt;% mutate(.by = rating, pct = n / sum(n)) %&gt;% group_by(rating) %&gt;% slice_max(order_by = pct, n = 10) %&gt;% ggplot(aes(x = pct, y = reorder_within(word, by = pct, within = rating))) + geom_col() + scale_y_reordered() + scale_x_continuous(labels = percent_format(1)) + labs(y = NULL, x = NULL) + facet_wrap(facets = vars(rating), scales = &quot;free_y&quot;) Word clouds are a nice way to get an overview of the data. token %&gt;% inner_join(hotel_1 %&gt;% filter(rating %in% c(&quot;1-2&quot;, &quot;5&quot;)), by = join_by(review_id)) %&gt;% filter(!word %in% c(&quot;hotel&quot;, &quot;stay&quot;, &quot;night&quot;, &quot;london&quot;), !is.na(bing_polarity)) %&gt;% mutate(polarity_desc = if_else(bing_polarity &gt; 0, &quot;Positive&quot;, &quot;Negative&quot;)) %&gt;% count(word, polarity_desc, wt = word_cnt) %&gt;% pivot_wider(names_from = polarity_desc, values_from = n, values_fill = 0) %&gt;% data.table::data.table() %&gt;% as.matrix(rownames = &quot;word&quot;) %&gt;% wordcloud::comparison.cloud(max.words = 30, title.size = 1.5, scale = c(1, 3.5)) sentimentr sentimentr calculates polarity at the sentence level. It improves on tidytext in that it takes into account the context in which the sentiment words occur by incorporating valence shifters. A negator flips the direction of a polarizing word (e.g., “I do not like it.”). lexicon::hash_valence_shifters[y==1]. An amplifier intensifies the impact (e.g., “I really like it.”). lexicon::hash_valence_shifters[y==2]. A de-amplifier (downtoner) reduces the impact (e.g., “I hardly like it.”). lexicon::hash_valence_shifters[y==3]. An adversative conjunction overrules the previous clause containing a polarized word (e.g., “I like it but it’s not worth it.”). lexicon::hash_valence_shifters[y==4]. sentimentr uses a lexicon package combined from the syuzhet and lexicon packages. Positive words are scored +1 and negative words are scored -1. sentimentr identifies clusters of words within sentences of the text. The 4 words before and 2 words after are candidate valence shifters. Polarized words are weighted by the valence shifter weights: negators = -1; amplifiers and de-amplifiers = 1.8; adversative conjunctions decrease the value of the prior cluster and increase the value of the following cluster. Neutral words hold no value, but do affect the word count. hotel_sentimentr &lt;- sentimentr::get_sentences(hotel_1$review) %&gt;% sentimentr::sentiment() %&gt;% summarize(.by = element_id, sentimentr_polarity = mean(sentiment)) hotel_2 &lt;- hotel_1 %&gt;% mutate(element_id = row_number()) %&gt;% inner_join(hotel_sentimentr, by = join_by(element_id)) %&gt;% select(-element_id) Let’s see a few examples where sentimentr differed from tidytext. Looks like bing did a better job on the first one, but sentimentr was better on the next two. hotel_2 %&gt;% filter((bing_polarity &gt; 0.2 &amp; sentimentr_polarity &lt; -0.2) | (bing_polarity &lt; -0.2 &amp; sentimentr_polarity &gt; 0.2)) %&gt;% select(review, bing_polarity, sentimentr_polarity) %&gt;% head(3) %&gt;% flextable::flextable() %&gt;% flextable::autofit() .cl-d2d4b5f6{}.cl-d2cd72f0{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d2cd7304{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d2d01da2{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d2d01dac{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d2d02f86{width:56.161in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d2d02f90{width:1.157in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d2d02f91{width:1.574in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d2d02f9a{width:56.161in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d2d02fa4{width:1.157in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d2d02fa5{width:1.574in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d2d02fa6{width:56.161in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d2d02fae{width:1.157in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d2d02faf{width:1.574in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d2d02fb8{width:56.161in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d2d02fc2{width:1.157in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d2d02fc3{width:1.574in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}reviewbing_polaritysentimentr_polarityWe were locked out of our room twice in a 3 day stay due to the Savoy not being able to track our reservation - the excuses included that we were responsible until they finally admitted their system is fouled up-0.31622780.2161730For many years I had fantasised about staying at The Dorchester so when the opportunity arose I jumped at it. Needless to say, as one of the great British institutions, it's a lovely hotel however it is not fabulous. It has two lovely dining areas /restaurants albeit pricey. I was extremely disappointed by the rooms! The corridors were creaky and narrow and the rooms pokey. There's no doubt it was tasteful decorated, however I was noisy -if the internal door wasn't shut and space was tight with a capital T. The double bed was very compact for 2, there was hardly any space to walk around it! Very claustrophobic! On a positive note the bathroom was spacious with delicious toiletries and good high pressure hot showers - and it you are a lover of hotel slippers- this one was good quality. All in all - charming but not worth the price or reputation- that is in the standard rooms. A word of advise- don't take your car!0.4670994-0.2377933Wonderful. Far better than The Ritz (slow and stiff) and Harvey Nichols (noisy and showy). The service is unsurpassable. Extensive range of teas. Enough food to make dinner unnecessary. Relaxed and unhurried atmosphere. I'm already looking forward to coming back.-0.47434160.2145884 There is a third package called qdap, but the sentimentr Read Me explains sentimentr is an improved version that better balances accuracy and speed.↩︎ "],["statistical-test.html", "3.3 Statistical Test", " 3.3 Statistical Test You can fit an ordinal logistic regression model to predict the rating based on the review sentiment. Which performs better, tidytext or sentimentr? Start with an intercept-only model for a baseline and review of ordinal logistic regression. fit_intercept &lt;- ordinal::clm(rating ~ 1, data = hotel_2) summary(fit_intercept) ## formula: rating ~ 1 ## data: hotel_2 ## ## link threshold nobs logLik AIC niter max.grad cond.H ## logit flexible 23373 -20669.84 41345.68 6(0) 1.58e-11 1.0e+01 ## ## Threshold coefficients: ## Estimate Std. Error z value ## 1-2|3 -3.04000 0.03134 -97.01 ## 3|4 -2.15898 0.02147 -100.54 ## 4|5 -0.81248 0.01418 -57.31 The threshold coefficients in the summary table are the log-odds of the outcome variable having a level at or below vs above. Below, 10.3% of ratings were &lt;=3 and 89.7% were &gt;3 for a log-odds of log(.103/.897) = -2.1620836, corresponding to the 3|4 line in the regression summary. hotel_2 %&gt;% tabyl(rating) %&gt;% mutate(cum = cumsum(percent), `1-cum` = 1 - cum) ## rating n percent cum 1-cum ## 1-2 1067 0.04565096 0.04565096 0.9543490 ## 3 1352 0.05784452 0.10349549 0.8965045 ## 4 4765 0.20386771 0.30736320 0.6926368 ## 5 16189 0.69263680 1.00000000 0.0000000 Now fit the bing and sentimentr models. The bing model has the higher -2 * log-likelihood. fit_bing &lt;- ordinal::clm(rating ~ bing_polarity, data = hotel_2) fit_sentimentr &lt;- ordinal::clm(rating ~ sentimentr_polarity, data = hotel_2) anova(fit_bing, fit_sentimentr, fit_intercept) ## Likelihood ratio tests of cumulative link models: ## ## formula: link: threshold: ## fit_intercept rating ~ 1 logit flexible ## fit_bing rating ~ bing_polarity logit flexible ## fit_sentimentr rating ~ sentimentr_polarity logit flexible ## ## no.par AIC logLik LR.stat df Pr(&gt;Chisq) ## fit_intercept 3 41346 -20670 ## fit_bing 4 36300 -18146 5047.93 1 &lt; 2.2e-16 *** ## fit_sentimentr 4 37080 -18536 -780.67 0 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 How about predictive performance? They performed about the same. bing_conf &lt;- fit_bing %&gt;% augment(type = &quot;class&quot;) %&gt;% conf_mat(truth = rating, estimate = .fitted) sentimentr_conf &lt;- fit_sentimentr %&gt;% augment(type = &quot;class&quot;) %&gt;% conf_mat(truth = rating, estimate = .fitted) bind_rows( bing = as_tibble(bing_conf$table), sentimentr = as_tibble(sentimentr_conf$table), .id = &quot;lexicon&quot; ) %&gt;% pivot_wider(names_from = Truth, values_from = n) %&gt;% flextable::flextable() %&gt;% flextable::merge_v(j = 1) %&gt;% flextable::valign(j = 1, valign = &quot;top&quot;) .cl-d36c93d0{}.cl-d365f5e8{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d365f5fc{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d368cb10{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d368cb1a{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d368cb1b{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d368d9b6{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d368d9c0{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d368d9ca{width:0.75in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d368d9cb{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d368d9d4{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d368d9d5{width:0.75in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d368d9de{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d368d9df{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d368d9e8{width:0.75in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d368d9e9{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d368d9f2{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}lexiconPrediction1-2345bing1-221755271530000458244950867252688484,23015,502sentimentr1-215438201330000463345930732152808554,43815,855 bind_rows( bing = summary(bing_conf), sentimentr = summary(sentimentr_conf), .id = &quot;lexicon&quot; ) %&gt;% pivot_wider(names_from = lexicon, values_from = .estimate) ## # A tibble: 13 × 4 ## .metric .estimator bing sentimentr ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.694 0.698 ## 2 kap multiclass 0.156 0.136 ## 3 sens macro 0.317 0.297 ## 4 spec macro 0.790 0.786 ## 5 ppv macro NA NA ## 6 npv macro 0.858 0.881 ## 7 mcc multiclass 0.186 0.176 ## 8 j_index macro 0.107 0.0833 ## 9 bal_accuracy macro 0.553 0.542 ## 10 detection_prevalence macro 0.25 0.25 ## 11 precision macro 0.555 0.534 ## 12 recall macro 0.317 0.297 ## 13 f_meas macro 0.432 0.392 "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
