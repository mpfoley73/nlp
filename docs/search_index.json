[["topicmodeling.html", "Chapter 2 Topic Modeling", " Chapter 2 Topic Modeling Topic models are generative probabilistic models that identify topics as clusters of words with an associated probability distribution and a probability distribution of topics within each document. Topic models such as Latent Dirichlet Allocation (LDA) and Structural Topic Modeling (STM) treat documents within a corpora as “bags of words” and identifies groups of words that tend to co-occur. The groups are the topics, formally conceptualized as probability distributions over vocabulary. LDA and STM are generative models of word counts, meaning they model a process that generates text which is a mixture of topics composed of words both of which follow probability distributions. Think of documents as the product of an algorithm that selects each word in two stages: 1) sample a topic, then 2) sample a word from that topic. The task in topic modeling is to tune the hyperparameters that define the probability distributions. In a way, topic models do the opposite of what you might expect. They do not estimate the probability that document x is about topic y. Rather, they estimate the contribution of all Y topics to document x. This leads to two frameworks for thinking about topics. Prevalance estimates the proportions of the document generated by the topic. Content is the probability distribution of words within the topic. LDA and STM differ only in how they handle these frameworks. STM controls for covariates associated with prevalence and content while LDA does not. LDA is implemented in the topicmodels package and STM is implemented in the stm package. Whether you use LDA or STM, start with the bag-of-words that you created in Chapter 1. This chapter continues from there and follows the ideas from Nagelkerke (2020), Meaney (2022), and the stm package vignette. library(tidyverse) library(tidymodels) library(topicmodels) library(tidytext) library(stm) library(scales) library(glue) library(httr2) library(jsonlite) load(&quot;input/hotel_prepped.Rdata&quot;) glimpse(prepped_hotel) ## Rows: 1,448 ## Columns: 9 ## $ review_id &lt;int&gt; 14478, 24627, 25306, 10904, 21306, 605, 14923, 2264, 99… ## $ hotel &lt;ord&gt; Savoy, Ridgemount, Corinthia, Savoy, Other, Rembrandt, … ## $ rating &lt;fct&gt; 5, 5, 5, 5, 5, 1-2, 1-2, 1-2, 5, 4, 4, 1-2, 5, 5, 5, 5,… ## $ review &lt;chr&gt; &quot;Love Love Love The Savoy If you are looking for a luxe… ## $ reviewer_loc &lt;fct&gt; United Kingdom, Other, Other, Other, Other, United King… ## $ raw_chrcnt &lt;int&gt; 401, 406, 1297, 213, 1057, 395, 1764, 527, 1060, 395, 1… ## $ raw_wordcnt &lt;int&gt; 79, 68, 242, 33, 192, 84, 348, 96, 197, 74, 202, 236, 1… ## $ prepped_review &lt;chr&gt; &quot;love love love savoy luxe stuffy feel millionaire hang… ## $ prepped_wrdcnt &lt;int&gt; 23, 28, 90, 16, 66, 19, 106, 41, 79, 30, 58, 85, 44, 26… glimpse(token) ## Rows: 69,129 ## Columns: 2 ## $ review_id &lt;int&gt; 14478, 14478, 14478, 14478, 14478, 14478, 14478, 14478, 1447… ## $ word &lt;chr&gt; &quot;love&quot;, &quot;love&quot;, &quot;love&quot;, &quot;savoy&quot;, &quot;luxe&quot;, &quot;stuffy&quot;, &quot;feel&quot;, &quot;… glimpse(bigram) ## Rows: 21,354 ## Columns: 2 ## $ review_id &lt;int&gt; 14478, 14478, 14478, 14478, 14478, 14478, 24627, 24627, 2462… ## $ bigram &lt;chr&gt; &quot;love love&quot;, &quot;love love&quot;, &quot;classy decor&quot;, &quot;decor wonderful&quot;,… References "],["pre-processing.html", "2.1 Pre-processing", " 2.1 Pre-processing Chapter 1 cleaned the text. The next step is to create a document-term matrix (DTM). A DTM has one row per per document, one column per term, and frequencies in the cells. Even with stop words removed, the DTM contains mostly infrequently used terms that would not contribute to topics, so remove its sparse terms. The pre-processing steps are the same for LDA and STM with the exception of the final DTM object class. The LDA and STM model fitting functions in this chapter use different DTM objects. topicmodels::LDA() uses class DocumentTermMatrix from the tm package. stm::stm() uses class dfm from the quanteda package. DFM stands for document feature matrix. DTM and DFM are essentially the same thing. Keep only the decent sized reviews, ones with at least 25 words. If this is a predictive model, now is the time to create a train/test split. Consider weighting the split by the outcome variable of interest, rating in this case, to ensure proportional coverage. set.seed(12345) hotel_gte25 &lt;- prepped_hotel %&gt;% filter(prepped_wrdcnt &gt;= 25) nrow(hotel_gte25) ## [1] 973 # Parameter `strata` ensures proportional coverage of ratings. hotel_split &lt;- rsample::initial_split(hotel_gte25, prop = 3/4, strata = rating) hotel_train &lt;- training(hotel_split) nrow(hotel_train) ## [1] 729 hotel_test &lt;- testing(hotel_split) nrow(hotel_test) ## [1] 244 token_train &lt;- token %&gt;% semi_join(training(hotel_split), by = join_by(review_id)) token_test &lt;- token %&gt;% semi_join(testing(hotel_split), by = join_by(review_id)) Most words add little value to a topic model because they appear infrequently or too frequently. The most common metric for removing sparse terms is the term frequency-inverse document frequency (TF-IDF). TF(t,d) is term t’s usage proportion in document d. IDF(t) is the log of the inverse of the term t’s proportion of documents it appears. For example, “savoy” appears in n = 109 of the N = 729 training documents. Its IDF is log(N/n) = 1.90. “savoy” appears in review #16954 in 2 of 32 (6.25%) of the terms. The TF-IDF score is the product of the two numbers. Here is that prepped review. manage night savoy experience superb moment arrive greet experience departure staff fantastic cupcake tea coffee cocktail delicious room expect faultless comfortable breakfast wonderful experience list item breakfast world forward opportunity savoy night Nagelkerke (2020) suggests another route. You already removed the stop words, so the over-used words are out. The TF-IDF approach was developed for long documents. Smaller documents like online reviews have little TF variation (most words are used only once or twice in a review), and the IDF ends up dominating. Instead, just filter on each word’s corpus frequency. In the end, you need to experiment to find the right cutoff. Using TF-IDF, the elbow in the plot below is around .15. Using that threshold throws out about about 90% of the vocabulary. The corpus frequency plot has an elbow around 4 occurrences. That threshold throws out 80% of the vocabulary. hotel_word_stats &lt;- token_train %&gt;% count(review_id, word, name = &quot;doc_freq&quot;) %&gt;% bind_tf_idf(word, review_id, doc_freq) %&gt;% mutate(.by = word, corp_freq = sum(doc_freq)) %&gt;% mutate(corp_pct = corp_freq / sum(doc_freq)) hotel_word_stats %&gt;% mutate(tf_idf_bin = cut(tf_idf, breaks = 50)) %&gt;% summarize(.by = tf_idf_bin, vocab = n_distinct(word)) %&gt;% arrange(tf_idf_bin) %&gt;% mutate(pct = vocab / sum(vocab), cumpct = cumsum(pct)) %&gt;% ggplot(aes(x = tf_idf_bin)) + geom_col(aes(y = pct)) + geom_line(aes(y = cumpct, group = 1)) + geom_vline(xintercept = 13, linetype = 2) + labs(y = &quot;vocabulary&quot;, title = &quot;TF-IDF Method&quot;) + scale_y_continuous(breaks = seq(0, 1, .1), labels = percent_format(1)) + theme(axis.text.x = element_text(angle = 90, vjust = .5)) hotel_word_stats %&gt;% mutate( corp_freq_bin = if_else(corp_freq &gt; 19, &quot;20+&quot;, as.character(corp_freq)), corp_freq_bin = factor(corp_freq_bin, levels = c(as.character(1:19), &quot;20+&quot;)) ) %&gt;% # mutate(corp_pct_bin = cut(corp_pct, breaks = 100)) %&gt;% summarize(.by = corp_freq_bin, vocab = n_distinct(word)) %&gt;% arrange(corp_freq_bin) %&gt;% mutate(pct = vocab / sum(vocab), cumpct = cumsum(pct)) %&gt;% ggplot(aes(x = corp_freq_bin, y = cumpct)) + geom_col(aes(y = pct)) + geom_line(aes(y = cumpct, group = 1)) + geom_vline(xintercept = 4, linetype = 2) + labs(y = &quot;vocabulary&quot;, title = &quot;Corpus Frequency Method&quot;) + scale_y_continuous(breaks = seq(0, 1, .1), labels = percent_format(1)) + theme(axis.text.x = element_text(angle = 90, vjust = .5)) Compare the resulting DTMs. The TF-IDF method keeps half the documents and 1,500 terms. The corpus frequency method retains all documents while limiting the vocabulary to 1,200 terms. Corpus frequency does seem superior. (dtm_tfidf &lt;- hotel_word_stats %&gt;% filter(tf_idf &gt; .15) %&gt;% cast_dtm(document = review_id, term = word, value = doc_freq)) ## &lt;&lt;DocumentTermMatrix (documents: 426, terms: 1560)&gt;&gt; ## Non-/sparse entries: 1754/662806 ## Sparsity : 100% ## Maximal term length: 26 ## Weighting : term frequency (tf) (dtm_corpfreq &lt;- hotel_word_stats %&gt;% filter(corp_freq &gt; 5) %&gt;% cast_dtm(document = review_id, term = word, value = doc_freq)) ## &lt;&lt;DocumentTermMatrix (documents: 729, terms: 1211)&gt;&gt; ## Non-/sparse entries: 31208/851611 ## Sparsity : 96% ## Maximal term length: 14 ## Weighting : term frequency (tf) The pre-processing step sure pares down the corpus. The high frequency terms comprise only 20% of the vocabulary, but are still 83% of the total word usage. bind_rows( `high freq words` = hotel_word_stats %&gt;% filter(corp_freq &gt; 5) %&gt;% summarize(distinct_words = n_distinct(word), total_words = sum(doc_freq)), `low freq words` = hotel_word_stats %&gt;% filter(corp_freq &lt;= 5) %&gt;% summarize(distinct_words = n_distinct(word), total_words = sum(doc_freq)), .id = &quot;partition&quot; ) %&gt;% mutate(total_pct = total_words / sum(total_words) * 100, distinct_pct = distinct_words / sum(distinct_words) * 100) %&gt;% select(partition, distinct_words, distinct_pct, total_words, total_pct) %&gt;% janitor::adorn_totals() ## partition distinct_words distinct_pct total_words total_pct ## high freq words 1211 20.41126 38093 83.15252 ## low freq words 4722 79.58874 7718 16.84748 ## Total 5933 100.00000 45811 100.00000 References "],["lda.html", "2.2 LDA", " 2.2 LDA Latent Dirichlet allocation (LDA) is one of a family of mixed membership models that decompose data into latent components. Latent means unidentified topics and Dirichlet is the distribution followed by the words in topics and by topics in documents. LDA presumes each document is created by a generative process in which topics are selected from a probability distribution and then words from that topic are selected from another distribution. LDA optimizes the distributions by performing a random search through the parameter space to find the model with the largest log-likelihood. There are multiple search algorithms, but the preferred one appears to be Gibbs sampling, a type of Monte Carlo Markov Chain (MCMC) algorithm. The algorithm is: For each document \\(d = 1 \\ldots D\\), randomly assign each word \\(w = 1 \\ldots W\\) to one of \\(k = 1\\ldots K\\) topics. Tabulate the number of words in each document and topic, a \\(D \\times K\\) matrix, and tabulate the number of occurrences of each word in each document, a \\(W \\times D\\) matrix. Resample to remove a single instance of a word from the corpus, decrementing the document’s topic count and the word’s topic count. Calculate the gamma matrix and the beta matrix. the gamma matrix (aka theta) is the topical prevalence, the probability distribution of topics for each document, \\[p(k|d) = \\frac{n_{dk} + \\alpha}{N_d + K \\alpha}\\] were \\(n_{dk}\\) is the number of words in document \\(d\\) for topic \\(k\\), \\(N_d\\) is the total number of words in \\(d\\), and \\(\\alpha\\) is a hyperparameter. For each \\(d\\), \\(\\sum_{k \\in K} \\gamma_{dk} = 1\\). the beta matrix (aka phi), is the topical content, the probability distribution of words for each topic, \\[p(w|k) = \\frac{m_{w,k} + \\beta}{\\sum_{w \\in W}m_{d,k} + W\\beta}\\] where \\(m_{w,k}\\) is the corpus-wide frequency count of word \\(w\\) to topic \\(k\\), \\(W\\) is the number of distinct words in the corpus, and \\(\\beta\\) is a hyperparameter. For each \\(k\\), \\(\\sum_{w \\in W} \\beta_{kw} = 1\\). Calculate the joint probability distribution of words for each document and topic, \\(p(w|k,d) = p(k|d)p(w|k)\\). Assign each word, \\(w\\), to the topic with the maximum joint probability. Repeat steps 3-6 for all of the words in all of the documents. Repeat steps 3-7 for a pre-determined number of iterations. LDA thus has 3 hyperparameters: the document-topic density factor, \\(\\alpha\\), the topic-word density factor, \\(\\beta\\), and the topic count, \\(K\\). \\(\\alpha\\) controls the number of topics expected per document (large \\(\\alpha\\) = more topics). \\(\\beta\\) controls the distribution of words per topic (large \\(\\beta\\) = more words). Ideally, you want a few topics per document and a few words per topic, so \\(\\alpha\\) and \\(\\beta\\) are typically set below one. \\(K\\) is set using a combination of domain knowledge, coherence, and exclusivity. Notice that LDA is a “bag of words” method. It does not consider the order of the tokens in the text, so where tokens are located what other tokens are nearby do not factor into the output. Fit Fit the LDA model with topicmodels::LDA(). It uses the tm DocumentTermMatrix data class, the same as what you just created in the pre-processing step. hotel_dtm &lt;- dtm_corpfreq There are several parameters you might tweak for the model fit. The biggest surprise is that you set the number of topics, k. In general, you only want as many topics as are clearly distinct and that you can easily communicate to others. Nagelkerke (2020) suggests sticking with the art vs science approach and picking your own k. A common recommendation is the perplexity statistic. Perplexity is a measure of how well a probability model fits a new set of data. As the number of topics increase, the perplexity will generally decrease. That is not the case below. set.seed(12345) train_ind &lt;- sample(nrow(hotel_dtm), floor(0.75*nrow(hotel_dtm))) k_train &lt;- hotel_dtm[train_ind, ] k_test &lt;- hotel_dtm[-train_ind, ] k = c(seq(from = 2, to = 10, by = 2)) perp &lt;- k %&gt;% map(~ LDA(k_train, k = .x)) %&gt;% map(~ perplexity(.x, newdata = k_test, control = list(estimate.beta = FALSE))) %&gt;% as.numeric() data.frame(k = k, perplexity = perp) %&gt;% ggplot(aes(x = k, y = perplexity)) + geom_point() + geom_smooth(method = &quot;loess&quot;, se = FALSE, formula = &quot;y~x&quot;) + labs(title = &quot;Perplexity Plot for LDM model&quot;) I will stick with a simple k = 4 model, mostly for convenience since this is just an example. lda_fit &lt;- LDA(hotel_dtm, k = 4) The fitted object contains two matrices. The beta (aka “phi”) matrix is the distribution of tokens (cols) over topics (rows). The gamma (aka “theta”) matrix is the distribution of documents (rows) over topics (cols). The row sum is 1 for each matrix (sum of topic probabilities for beta, sum of document probabilities for gamma). lda_beta_mtrx &lt;- posterior(lda_fit) %&gt;% pluck(&quot;terms&quot;) %&gt;% as.matrix() # One row per topic, one col per word. dim(lda_beta_mtrx) ## [1] 4 1211 # Word probability distribution sums to 1 for each topic. sum(lda_beta_mtrx[1, ]) ## [1] 1 lda_gamma_mtrx &lt;- posterior(lda_fit) %&gt;% pluck(&quot;topics&quot;) %&gt;% as.matrix() # One row per document, one col per topic dim(lda_gamma_mtrx) ## [1] 729 4 # Topic probability distribution sums to 1 for each document. sum(lda_gamma_mtrx[1, ]) ## [1] 1 tidytext::tidy() pivots the beta matrix into a [topic, term, beta] data frame. lda_beta_tidy &lt;- tidy(lda_fit, matrix = &quot;beta&quot;) lda_top_tokens &lt;- lda_beta_tidy %&gt;% mutate(topic = factor(paste(&quot;Topic&quot;, topic))) %&gt;% group_by(topic) %&gt;% slice_max(order_by = beta, n = 10) %&gt;% ungroup() lda_top_tokens %&gt;% ggplot(aes(x = beta, y = reorder_within(term, by = beta, within = topic))) + geom_col() + scale_y_reordered() + facet_wrap(facets = vars(topic), scales = &quot;free_y&quot;) + labs(y = NULL, title = &quot;LDA Top 10 Terms&quot;) Word clouds tell you more or less the same thing. Here is the code below. I think everyone has pretty much moved past word clouds, so I’m not even wasting space on this. colors6 &lt;- RColorBrewer::brewer.pal(n = 4, name = &quot;Set2&quot;) x &lt;- map( c(1:4), ~ with(lda_beta_tidy %&gt;% filter(topic == .x), wordcloud::wordcloud( term, beta, max.words = 20, colors = colors6[.x] )) ) There is a downside to this evaluation. Popular words like “room” appear at or near the top in multiple topics. You might want to look at relative popularity instead: the popularity within the topic divided by overall popularity. That’s problematic too because words that only appear in few reviews will pop to the top. What you want is a combination of both absolute term probability and relative term probability. LDAvis::serVis() can help you do that. Unfortunately, the plot from LDAvis::serVis() is interactive and does not render in the RMarkdown notebook html, so below is just a screenshot of the code chunk output. The left side shows the topic sizes (documents) and topic distances. The right side shows the most important tokens. # word count for each document doc_length &lt;- hotel_word_stats %&gt;% filter(corp_freq &gt; 5) %&gt;% summarize(.by = review_id, n = sum(doc_freq)) %&gt;% pull(n) # vocabulary: unique tokens vocab &lt;- colnames(lda_beta_mtrx) # overall token frequency term_frequency &lt;- hotel_word_stats %&gt;% filter(corp_freq &gt; 5) %&gt;% summarize(.by = word, n = sum(doc_freq)) %&gt;% arrange(match(word, vocab)) %&gt;% pull(n) # create JSON containing all needed elements json &lt;- LDAvis::createJSON(lda_beta_mtrx, lda_gamma_mtrx, doc_length, vocab, term_frequency) LDAvis::serVis(json) The gamma matrix shows topic distributions. Use it to see if topics vary by a covariate. The topics did not vary by rating tidy(lda_fit, matrix = &quot;gamma&quot;) %&gt;% mutate(document = as.numeric(document), topic = factor(topic)) %&gt;% inner_join(prepped_hotel, by = join_by(document == review_id)) %&gt;% summarize(.by = c(rating, topic), gamma = mean(gamma)) %&gt;% ggplot(aes(x = gamma, y = fct_rev(rating), fill = topic)) + geom_col() + labs(y = NULL, title = &quot;LDA Topic Distribution&quot;) Iterate through the model by tweaking k, excluding words that suppress interesting subdomains, and/or changing the minimal token frequency to focus on more/less dominant tokens. You can also change the document sampling strategy to promote interesting domains, like we did when we over sampled the low hotel ratings. Topic Labeling with ChatGPT An API token is required for OpenAI’s ChatGPT API service. I created one at https://platform.openai.com/api-keys and saved it to .Renviron. See usethis::edit_r_environ(). # Create a function to send each list of topic words to Open AI as a separate request. get_topic_from_openai &lt;- function(prompt) { my_resp &lt;- request(&quot;https://api.openai.com/v1/chat/completions&quot;) %&gt;% req_headers(Authorization = paste(&quot;Bearer&quot;, Sys.getenv(&quot;OPENAI_API_KEY&quot;))) %&gt;% req_body_json(list( model = &quot;gpt-3.5-turbo&quot;, # Temperature [0,2] controls creativity, predictable -&gt; variable. temperature = 1, messages = list( # System prompt sets repeated context. It is prefixed to prompts. list( role = &quot;system&quot;, content = paste(&quot;You are a topic modeling assistant. You accept lists &quot;, &quot;of words in a topic and summarizes them into a salient &quot;, &quot;topic label of five words or less. How would you &quot;, &quot;summarize the following list? The list is in descending &quot;, &quot;order of importance, so the first term in the list is most &quot;, &quot;strongly tied to the topic. Return just the topic label and &quot;, &quot;nothing else.&quot;) ), list( role = &quot;user&quot;, content = prompt ) ) )) %&gt;% req_perform() %&gt;% resp_body_json() %&gt;% pluck(&quot;choices&quot;, 1, &quot;message&quot;, &quot;content&quot;) } lda_topics &lt;- lda_top_tokens %&gt;% nest(data = term, .by = topic) %&gt;% mutate( token_str = map(data, ~paste(.$term, collapse = &quot;, &quot;)), topic_lbl = map_chr(token_str, get_topic_from_openai), topic_lbl = str_remove_all(topic_lbl, &#39;\\\\&quot;&#39;), topic_lbl = snakecase::to_any_case(topic_lbl, &quot;title&quot;) ) %&gt;% select(-data) # Save to file system to avoid regenerating. saveRDS(lda_topics, file = &quot;input/lda_topics.RDS&quot;) lda_topics &lt;- readRDS(file = &quot;input/lda_topics.RDS&quot;) Let’s see the topic summary with the newly generated labels. lda_top_tokens %&gt;% inner_join(lda_topics, by = join_by(topic)) %&gt;% mutate(topic_lbl = str_wrap(topic_lbl, 25)) %&gt;% ggplot(aes(x = beta, y = reorder_within(term, by = beta, within = topic_lbl))) + geom_col() + scale_y_reordered() + facet_wrap(facets = vars(topic_lbl), scales = &quot;free_y&quot;) + labs(y = NULL, title = &quot;LDA Top 10 Terms&quot;) 2.2.1 Discussion LDA has some restrictive assumptions to be aware of. The distribution of topics is estimated independently of the documents. If the corpus was about vacation resorts, weather would an important topic in warm equatorial zones where sunny weather was the main selling point. Museums might be more important in historical European cities. The distribution of words in a topic is estimated independently of the documents. Two documents could be about the topic “violence during protests”, but use different language based on viewpoint “peaceful protesters and brutal police” vs “rioters and law and order”. It’s the same topic, but the content differs. TODO I still need to learn more about Held-out Likelihood (Wallach et al., 2009). Semantic Coherence. The coherence measure evaluates topics. Exclusivity. Generally, the greater the number of topics in a model, the lower the quality of the smallest topics. One way around this is hiding the low-quality topics. References "],["stm.html", "2.3 STM", " 2.3 STM STM introduces covariates which “structure” the prior distributions in the topic model Roberts et al. (2014). Topics can be correlated, each document has its own prior distribution over topics, and word use can vary by covariates. STM models incorporate two effects. Topical prevalence covariate effects. STM models prevalence as a function of the covariates. A survey respondent’s party affiliation may affect which topics they discuss in a question about their view on immigration. In our hotel case study, a local (UK) business traveler might care about different hotel qualities than a foreign tourist. Topical content covariate effects. STM models content as a function of the covariates. A survey respondent’s party affiliate may affect how they discuss topics in a question about public protests. A conservative might use words like “rioters” and “law and order” for a topic about violent demonstrations while a liberal might use words like “police brutality” and “oppression”. In the hotel example, negative reviews might focus on lapses in baseline expectations such as “sheets”, while positive reviewers focus on unexpected delights such as “origami” towels. STM is similar to LDA in that it assumes each document is created by a generative process where topics are included according to probabilities (topical prevalence) and words are included in the topics (topical content) according to probabilities. In LDA, topic prevalence and content came from Dirichlet distributions with hyperparameters set in advance, sometimes referred to as a and b. With STM, the topic prevalence and content come from document metadata. The covariates provide a way of “structuring” the prior distributions in the topic model, injecting valuable information into the inference procedure (tingly2014?). Fit Fit the STM model with stm::stm(). It uses the quanteda dfm data class.1 You can cast hotel_word_stats as a DFM with tidytext::cast_dfm(), but stm() returned errors with prevalence and topic covariates. Instead, use stm::textProcessor() and stm::prepDocuments(). stm::textProcessor() produces a list object containing a vocabulary vector, a list of mini-DTM matrices for each document, and a metadata data frame. textProcessor() can remove stop words and change case, etc., but we already did that, so set those parameters to FALSE. stm::prepDocuments() removes sparse terms from the matrix. # This produces errors in modeling phase, so don&#39;t use it. hotel_dfm_tidy &lt;- hotel_word_stats %&gt;% filter(corp_freq &gt; 5) %&gt;% cast_dfm(document = review_id, term = word, value = doc_freq) hotel_processed &lt;- stm::textProcessor( documents = training(hotel_split) %&gt;% pull(prepped_review), metadata = training(hotel_split) %&gt;% select(rating, reviewer_loc), lowercase = FALSE, removestopwords = FALSE, removenumbers = FALSE, removepunctuation = FALSE, stem = FALSE ) ## Building corpus... ## Creating Output... hotel_dfm &lt;- stm::prepDocuments( hotel_processed$documents, hotel_processed$vocab, hotel_processed$meta, lower.thresh = 5 ) ## Removing 4680 of 5814 terms (7476 of 37744 tokens) due to frequency ## Your corpus now has 729 documents, 1134 terms and 30268 tokens. What constitutes a “good” model? The topics should be cohesive in the sense that high-probability words tend to co-occur with documents. The topics should also be exclusive in the sense that the top words in topics are unlikely to be shared. To find the best model, generate a candidate set with varying tuning parameters, initialization, and pre-processing. Plot the coherence and exclusivity to identify the one with the best combination. Take time to validate the model by trying to predict covariate values from the text. Either specify the number of topics (K) to identify, or let stm() choose an optimal number by setting K = 0. The resulting probability distribution of topic words (beta matrix) will be a K x 5,814 matrix. The probability distribution of topics (gamma matrix, theta in the stm package) will be a 729 x K matrix. I expect topics to correlate with the review rating, so rating is a prevalence covariate, and I expect word usage to correlate with the reviewer location, so reviewer_loc is a topical content covariate. # This model fit operation took 15 minutes to run. Run once and save to disk. set.seed(1234) stm_fits &lt;- tibble(K = seq(from = 2, to = 10, by = 2)) %&gt;% mutate(fit = map( K, ~stm::stm( documents = hotel_dfm$documents, vocab = hotel_dfm$vocab, K = ., prevalence = ~ rating, content = ~ reviewer_loc, data = hotel_dfm$meta, verbose = FALSE ) )) stm_fit &lt;- stm::stm( documents = hotel_dfm$documents, vocab = hotel_dfm$vocab, K = 4, prevalence = ~ rating, content = ~ reviewer_loc, data = hotel_dfm$meta, verbose = FALSE ) saveRDS(stm_fit, file = &quot;input/stm_fit.RDS&quot;) stm_fit &lt;- readRDS(file = &quot;input/stm_fit.RDS&quot;) hotel_dfm %&gt;% summary() ## Length Class Mode ## documents 729 -none- list ## vocab 1134 -none- character ## meta 2 data.frame list ## words.removed 4680 -none- character ## docs.removed 0 -none- NULL ## tokens.removed 1 -none- numeric ## wordcounts 5814 -none- numeric Whereas LDA models are optimized using the perplexity statistic, STM offers several options. The most useful are the held-out likelihood and coherence. stm_heldout &lt;- stm::make.heldout(hotel_dfm$documents, vocab = hotel_dfm$vocab) stm::semanticCoherence(stm_fit, documents = hotel_dfm$documents) stm_fits %&gt;% mutate( semantic_coherence = map(fit, ~semanticCoherence(.x, documents = hotel_dfm$documents)) ) %&gt;% select(K, semantic_coherence) %&gt;% unnest(semantic_coherence) %&gt;% summarize(.by = K, semantic_coherence = mean(semantic_coherence)) %&gt;% ggplot(aes(x = K, y = semantic_coherence)) + geom_line() stm_fit2 &lt;- stm::stm( stm_heldout$documents, stm_heldout$vocab, K = 4, prevalence = ~ rating, content = ~ reviewer_loc, data = hotel_dfm$meta, init.type = &quot;Spectral&quot;, verbose = FALSE ) # stm_fit2 %&gt;% stm::exclusivity() # stm_fit2 %&gt;% stm::semanticCoherence(documents = stm_heldout$documents) Interpret The fit summary has three sections showing the tops words. The first section shows the prevalence model; the second shows the topical content model; and the third shows their interaction. summary(stm_fit) ## A topic model with 4 topics, 729 documents and a 1134 word dictionary. ## Topic Words: ## Topic 1: ham, bean, sausage, owner, chair, cereal, liverpool ## Topic 2: covent, trafalgar, piccadilly, victoria, square, oxford, rembrandt ## Topic 3: incredible, amaze, rumpus, knowledgeable, kaspers, overlook, cocktail ## Topic 4: seafood, savoy, medium, remove, simply, foyer, grill ## ## Covariate Words: ## Group Other: daily, market, vacation, internet, awesome, closet, hope ## Group United Kingdom: cook, ooze, downside, party, finish, weekend, wife&#39;s ## Group United States: handle, garden, london&#39;s, sightsee, plush, server, prompt ## ## Topic-Covariate Interactions: ## Topic 1, Group Other: rees, wifi, ensuite, ridgemount, window, gower, chris ## Topic 1, Group United Kingdom: gown, ben, escape, stain, secret, smell, corridor ## Topic 1, Group United States: warn, soap, driver, screen, pauls, overlook, flat ## ## Topic 2, Group Other: hop, ben, min, transportation, directly, heathrow, line ## Topic 2, Group United Kingdom: rate, contemporary, furnish, spotlessly, honest, dcor, central ## Topic 2, Group United States: market, paris, strong, rees, knowledgeable, unique, june ## ## Topic 3, Group Other: spa, pricey, sandwich, dorchester, cake, beautiful, champagne ## Topic 3, Group United Kingdom: brilliant, 15th, sister, deco, 14th, wed, minibar ## Topic 3, Group United States: freshly, massive, rate, outlet, appoint, traveller, butler ## ## Topic 4, Group Other: junior, server, call, follow, difference, fix, happen ## Topic 4, Group United Kingdom: sandwich, frankly, ambience, plush, surrounding, scone, excite ## Topic 4, Group United States: beautifully, deco, plan, venue, beaufort, seafood, grill ## If this was just a regular topic model, or a prevalence or content model, we’d see top words by 4 metrics: highest probability, FREX, lift, and score. Highest probability weights words by their overall frequency. FREX weights words by their overall frequency and how exclusive they are to the topic. Lift weights words by dividing by their frequency in other topics, therefore giving higher weight to words that appear less frequently in other topics. Score divides the log frequency of the word in the topic by the log frequency of the word in other topics. Let’s fit a new model just to show that. set.seed(1234) stm_fit_simple &lt;- stm::stm( hotel_dfm$documents, hotel_dfm$vocab, K = 4, # prevalence = ~ rating, # content = ~ reviewer_loc, data = hotel_dfm$meta, init.type = &quot;Spectral&quot;, verbose = FALSE ) saveRDS(stm_fit_simple, file = &quot;input/stm_fit_simple.RDS&quot;) stm_fit_simple &lt;- readRDS(file = &quot;input/stm_fit_simple.RDS&quot;) stm::labelTopics(stm_fit_simple) ## Topic 1 Top Words: ## Highest Prob: stay, london, staff, service, excellent, restaurant, location ## FREX: spa, corinthia, concierge, luxury, mondrian, love, pool ## Lift: personal, oriental, mandarin, notch, corinthia, pleasure, property ## Score: personal, london, service, stay, spa, corinthia, location ## Topic 2 Top Words: ## Highest Prob: breakfast, walk, room, stay, location, clean, london ## FREX: tube, station, museum, hyde, street, bus, walk ## Lift: hyde, paddington, albert, ridgemount, bus, kensington, rhodes ## Score: hyde, tube, station, museum, bus, walk, clean ## Topic 3 Top Words: ## Highest Prob: savoy, bar, tea, lovely, staff, special, birthday ## FREX: afternoon, birthday, savoy, cocktail, cake, american, treat ## Lift: scone, pianist, cake, piano, beaufort, afternoon, sandwich ## Score: scone, savoy, afternoon, birthday, tea, cake, cocktail ## Topic 4 Top Words: ## Highest Prob: room, night, check, stay, bed, book, bathroom ## FREX: check, charge, bath, call, floor, pay, issue ## Lift: rumpus, robe, mirror, wake, smell, curtain, corridor ## Score: rumpus, room, check, shower, bed, bathroom, floor It is interesting that the top terms for UK did not include “restaurant” or “location”. The top terms for the US did not include “excellent” or “amaze”, but did include “love”. stm_tidy &lt;- tidy(stm_fit) stm_top_tokens &lt;- stm_tidy %&gt;% mutate(topic = factor(paste(&quot;Topic&quot;, topic))) %&gt;% group_by(topic, y.level) %&gt;% slice_max(order_by = beta, n = 10) %&gt;% ungroup() stm_top_tokens %&gt;% filter(topic == &quot;Topic 1&quot;) %&gt;% ggplot(aes(x = beta, y = reorder_within(term, by = beta, within = topic))) + geom_col() + scale_y_reordered() + facet_wrap(facets = vars(y.level), scales = &quot;free_x&quot;, ncol = 3) + labs(y = NULL, title = &quot;STM Top 10 Terms for Topic 1&quot;) As we did with the LDA model, we can assign topic labels with Open AI. stm_topics &lt;- stm_top_tokens %&gt;% nest(data = term, .by = topic) %&gt;% mutate( token_str = map(data, ~paste(.$term, collapse = &quot;, &quot;)), topic_lbl = map_chr(token_str, get_topic_from_openai), topic_lbl = str_remove_all(topic_lbl, &#39;\\\\&quot;&#39;), topic_lbl = snakecase::to_any_case(topic_lbl, &quot;title&quot;) ) %&gt;% select(-data) # Save to file system to avoid regenerating. saveRDS(stm_topics, file = &quot;input/stm_topics.RDS&quot;) stm_topics &lt;- readRDS(file = &quot;input/stm_topics.RDS&quot;) stm_topics ## # A tibble: 4 × 3 ## topic token_str topic_lbl ## &lt;fct&gt; &lt;list&gt; &lt;chr&gt; ## 1 Topic 1 &lt;chr [1]&gt; Luxury Stay in London ## 2 Topic 2 &lt;chr [1]&gt; Comfortable Stay Near London ## 3 Topic 3 &lt;chr [1]&gt; Afternoon Tea at the Savoy ## 4 Topic 4 &lt;chr [1]&gt; Hotel Stay Another way to evaluate the model is to print reviews that are most representative of the topic. Topic 1 stm::findThoughts( stm_fit, n = 3, texts = hotel_dfm$meta$review, topics = 1, meta = hotel_dfm$meta ) ## Warning in stm::findThoughts(stm_fit, n = 3, texts = hotel_dfm$meta$review, : ## texts are of type &#39;factor.&#39; Converting to character vectors. Use ## &#39;as.character&#39; to avoid this warning in the future. ## ## Topic 1: ## United Kingdom ## Other ## United Kingdom References "],["prediction.html", "2.4 Prediction", " 2.4 Prediction "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
