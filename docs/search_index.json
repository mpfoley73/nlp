[["index.html", "Natural Language Processing in R Intro", " Natural Language Processing in R Michael Foley 2023-11-21 Intro These notes consolidate several resources I’ve encountered while working on text mining projects: Text Mining with R (Silge and Robinson 2017) Introduction to Natural Language Processing in R (DataCamp) Topic Modeling in R (DataCamp) Introduction to Text Analysis in R” (DataCamp) String Manipulation in R with stringr (DataCamp) Text Mining with Bag-of-Words in R (DataCamp) Sentiment Analysis in R (DataCamp) Tidy Sentiment Analysis in R (DataCamp) Julia Silge’s The game is afoot! Topic modeling of Sherlock Holmes stories Julia Silge’s Training, evaluating, and interpreting topic models Toward understanding 17th century English culture: A structural topic model of Francis Bacon’s ideas References "],["data-prep.html", "Chapter 1 Data Preparation", " Chapter 1 Data Preparation This section covers how to prepare a corpus for text analysis. I’ll work with the customer reviews of London-based hotels data set hosted on data.world. hotel_raw contains 27K reviews of the ten most- and ten least-expensive hotels in London. The csv file is located online here. I saved it to my \\inputs directory. library(tidyverse) library(tidytext) library(scales) library(glue) hotel_0 &lt;- read_csv( &quot;input/london_hotel_reviews.csv&quot;, col_types = &quot;cicccc&quot;, col_names = c(&quot;property&quot;, &quot;rating&quot;, &quot;title&quot;, &quot;review&quot;, &quot;reviewer_loc&quot;, &quot;review_dt&quot;), skip = 1 ) %&gt;% mutate(review_id = row_number()) %&gt;% select(review_id, everything()) glimpse(hotel_0) ## Rows: 27,330 ## Columns: 7 ## $ review_id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17… ## $ property &lt;chr&gt; &quot;Apex London Wall Hotel&quot;, &quot;Corinthia Hotel London&quot;, &quot;The … ## $ rating &lt;int&gt; 5, 5, 5, 4, 5, 1, 5, 5, 5, 5, 5, 4, 2, 4, 5, 5, 5, 5, 5, … ## $ title &lt;chr&gt; &quot;Ottima qualità prezzo&quot;, &quot;By far, my best hotel in the wo… ## $ review &lt;chr&gt; &quot;Siamo stati a Londra per un week end ed abbiamo alloggia… ## $ reviewer_loc &lt;chr&gt; &quot;Casale Monferrato, Italy&quot;, &quot;Savannah, Georgia&quot;, &quot;London&quot;… ## $ review_dt &lt;chr&gt; &quot;10/20/2012&quot;, &quot;3/23/2016&quot;, &quot;7/30/2013&quot;, &quot;6/2/2012&quot;, &quot;11/2… "],["scrub.html", "1.1 Scrub", " 1.1 Scrub The data needs to be cleaned. I’ll follow some of the techniques used by Nagelkerke (2020a). One issue is tags like &lt;e9&gt; and unicode characters like &lt;U+0440&gt;. One way to get rid of unicode characters is to convert them to ASCII tags with iconv() and then remove the ASCII tags with str_remove(). E.g., iconv() converts &lt;U+0093&gt; to &lt;93&gt; which you can remove with regex \"\\\\&lt;[:alnum]+\\\\&gt;]\".1 There are also some reviews in other languages that I’ll just drop. And some hotel names are pretty long, so I’ll abbreviate them. hotel_1 &lt;- hotel_0 %&gt;% mutate( # Create ASCII bytes review = iconv(review, from = &quot;&quot;, to = &quot;ASCII&quot;, sub = &quot;byte&quot;), # Remove &lt;..&gt; review = str_remove_all(review, &quot;\\\\&lt;[[:alnum:]]+\\\\&gt;&quot;), # Remove &lt;U+....&gt; review = str_remove_all(review, &quot;\\\\&lt;U\\\\+[[:alnum:]]{4}\\\\&gt;&quot;), # Lots of pipes? review = str_remove_all(review, &quot;(\\\\|)&quot;), review = str_squish(review), # Shorten some of the hotel names. property = factor(str_remove_all( property, &quot;( - .*)|(, .*)|( Hotel)|( London)|(The )|( at .*)|( Hyde .*)|( Knights.*)&quot; )), # Interesting metadata chr_cnt = str_length(review) ) %&gt;% # Exclude reviews written in a foreign language. One heuristic to handle this # is to look for words common in other languages that do not also occur in English. filter( !str_detect(review, &quot;( das )|( der )|( und )|( en )&quot;), # German !str_detect(review, &quot;( et )|( de )|( le )|( les )&quot;), # French !str_detect(review, &quot;( di )|( e )|( la )&quot;), # Italian !str_detect(review, &quot;( un )|( y )&quot;), # Spanish str_length(review) &gt; 0 ) That might be enough. Let’s explore the data. # 90% of reviews rate the property a 4 or 5. hotel_1 %&gt;% janitor::tabyl(rating) ## rating n percent ## 1 498 0.02097990 ## 2 587 0.02472933 ## 3 1378 0.05805283 ## 4 4852 0.20440662 ## 5 16422 0.69183132 # Some reviews are as small as 16 characters, but they can get quite large. hotel_1 %&gt;% select(chr_cnt) %&gt;% summary() ## chr_cnt ## Min. : 1.0 ## 1st Qu.: 306.0 ## Median : 512.0 ## Mean : 710.6 ## 3rd Qu.: 866.0 ## Max. :30498.0 # A few reviews hotel_1 %&gt;% sample_n(3, seed = 12345) %&gt;% select(chr_cnt, review) %&gt;% flextable::flextable() %&gt;% flextable::valign(j = 1, valign = &quot;top&quot;) %&gt;% flextable::autofit() .cl-59602f52{}.cl-59595d12{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-59595d26{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-595c148a{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-595c1494{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-595c1495{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-595c2614{width:0.787in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-595c2615{width:61.53in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-595c261e{width:0.787in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-595c261f{width:61.53in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-595c2620{width:0.787in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-595c2628{width:61.53in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-595c2629{width:0.787in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-595c262a{width:61.53in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}chr_cntreview995My Partner and I stayed in the Corinthia Hotel on the weekend of the 28th Dec 2013. The hotel looked amazing with all of the Christmas decorations in white. The service from the moment you stepped into the lobby was just what I would have expected from a hotel of this calibre and thats just what we recieved. We had a deluxe room which was perfect, after wondering around London all day I just wanted to soak in the bath and watch the TV in my own littel world. The bed was huge and very comfortable. We met with some friends for breakfast in the hotel and we also enjoyed high tea..both of which I would recommend. Its also in a perfect location, close to Trafalger square, Soho, etc, you can walk anywhere, its perfect. When we left for the next stage of our European trip to Stockholm, before we knew it we had checked and our bags already in the cab as we walked out of the hotel...I would never stay anywhere else when in London but in this hotel. We will see you later again this year ...975Stayed for one night as a special occasion. I thought I booked a room with a view however found that this was not recorded (Expedia) this was acknowledged by the check in staff but as they claimed the hotel was full there was nothing they could do, they did however offer a lovely drink as compensation that was very appreciated. Working locally I wanted this to be a perfect experience - the room we had was fine but painfully overstyled to the point of being small and uncomfortable. The bed was rather hard and the entrance to the bathroom had a bad step that is a massive trip hazard leaving me with a rather bad stubbed toe. Outside the room the upper floor is being renovated and so felt rather like a work site than a luxury hotel. The Gym while well provisioned was a soulless room with no windows and very busy. In all it was fine but not the best for the cost - I would return for a drink perhaps but not for the night as I think there is probably better available.667I am a wheelchair user and this was a disabled room. The bed was huge and was lifted off the floor(so I could get the hoist under it) by requested elephant feet about 6-8 inches tall. It was also very hot in London. So there was a sheet,a thin blanket and another sheet on top- perfect! Though air conditioner was in the room. The bathroom was ideal and the shower wheel-in. The staff were wonderful and could not have been more helpful. The only negative point was that outside the hotel there was no drop-kerb fora wheelchair so I had to go to the end of the road to get on the pavement. (This was only a few yards) A brilliant experience - one that we will repeat. Nagelkerke (2020a) recommends removing punctuation to focus on the entire text rather than the sentences within. Nagelkerke also suggests removing very short (&lt;= 3 chars) for anything other than sentiment analysis. I’m going to keep punctuation and short reviews for now. Some of those extremely short reviews are gibberish, but tokenizing will filter out some of that. References "],["tokenize.html", "1.2 Tokenize", " 1.2 Tokenize Tokenize the reviews. Even if you want bigrams, it is often helpful to tokenize into unigrams first to clean and regularize. token_0 &lt;- hotel_1 %&gt;% select(review_id, review) %&gt;% unnest_tokens(&quot;word&quot;, review) %&gt;% mutate(.by = review_id, n = n()) %&gt;% # Short reviews are mostly gibberish. Require at least 10 words. filter(n &gt;= 10) # Attach word counts back to main data frame. Inner joining will remove some # of the bogus reviews. hotel_2 &lt;- token_0 %&gt;% count(review_id, name = &quot;word_cnt&quot;) %&gt;% inner_join(hotel_1, by = &quot;review_id&quot;) %&gt;% relocate(word_cnt, .after = chr_cnt) hotel_2 %&gt;% select(chr_cnt, word_cnt) %&gt;% summary() ## chr_cnt word_cnt ## Min. : 51.0 Min. : 10.0 ## 1st Qu.: 314.0 1st Qu.: 56.0 ## Median : 520.0 Median : 93.0 ## Mean : 721.1 Mean : 130.9 ## 3rd Qu.: 875.0 3rd Qu.: 159.0 ## Max. :30498.0 Max. :5712.0 Nagelkerke (2020a) recommends discarding reviews with few (&lt;=50) tokens on the grounds that short reviews will not add much in terms of different topics within reviews.2. That’s about 25% of this sample. I set a lower limit of 10 tokens. References "],["spell-check.html", "1.3 Spell-check", " 1.3 Spell-check Run a spell-check to regularize the data. Its possible to land on the wrong correction, but there is probably more to gain than lose. Only a very small fraction of the tokens were misspellings. # There are multiple possible right spellings, so just choose one. spell_check &lt;- fuzzyjoin::misspellings %&gt;% distinct(misspelling, .keep_all = TRUE) token_1 &lt;- token_0 %&gt;% left_join(spell_check, by = join_by(word == misspelling)) %&gt;% mutate(word = coalesce(correct, word)) %&gt;% select(-correct) # Only .09% of words were misspelled. mean(token_0$word != token_1$word) ## [1] 0.0009312022 # Examples. tibble(before = token_0$word, after = token_1$word) %&gt;% filter(before != after) %&gt;% head() ## # A tibble: 6 × 2 ## before after ## &lt;chr&gt; &lt;chr&gt; ## 1 resturant restaurant ## 2 excellance excellence ## 3 reccomend recommend ## 4 reccommend recommend ## 5 accomodating accommodating ## 6 ther there "],["lemmatize.html", "1.4 Lemmatize", " 1.4 Lemmatize Stemming and lemmatizing are ways to convert word variations like “staying”, “stayed”, and “stay” into a generic form: “stay”. Stemming tends to chop off endings to create a root word, but the stem is often not a word itself. E.g., “staying” becomes “stai”. Lemmatize gives you the more natural “stay”. token_2 &lt;- token_1 %&gt;% mutate(word = textstem::lemmatize_words(word)) # Examples. tibble(before = token_1$word, after = token_2$word) %&gt;% filter(before != after) %&gt;% head() ## # A tibble: 6 × 2 ## before after ## &lt;chr&gt; &lt;chr&gt; ## 1 had have ## 2 staying stay ## 3 nights night ## 4 was be ## 5 staying stay ## 6 was be "],["remove-stop-words.html", "1.5 Remove Stop Words", " 1.5 Remove Stop Words Stop words usually add no value, but you should pay attention to what you are dropping. Be ready to add pertinent words back and perhaps drop others. # Start with a standard list. stop &lt;- tidytext::stop_words %&gt;% # Don&#39;t remove potentially useful words. filter(!word %in% c(&quot;appreciate&quot;, &quot;room&quot;)) %&gt;% # But also exclude these words. bind_rows(tibble(word = as.character(0:9))) token &lt;- token_2 %&gt;% anti_join(stop, by = &quot;word&quot;) # Most frequently removed words token_2 %&gt;% semi_join(stop, by = &quot;word&quot;) %&gt;% count(word, sort = TRUE) ## # A tibble: 523 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 the 196438 ## 2 be 162698 ## 3 and 117631 ## 4 a 98224 ## 5 to 78025 ## 6 in 54398 ## 7 we 47724 ## 8 of 44773 ## 9 i 43744 ## 10 have 40064 ## # ℹ 513 more rows # A typical review, before and after bind_cols( review_id = 2:4, before = hotel_2 %&gt;% filter(review_id %in% 2:4) %&gt;% pull(review), after = token %&gt;% filter(review_id %in% 2:4) %&gt;% summarize(.by = review_id, x = paste(word, collapse = &quot; &quot;)) %&gt;% pull(x) ) %&gt;% flextable::flextable() %&gt;% flextable::valign(valign = &quot;top&quot;) %&gt;% flextable::autofit() .cl-5d055272{}.cl-5cd96bf8{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-5cd96c02{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-5cff8d88{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-5cff8d92{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-5cff8d9c{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-5cff8d9d{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-5cffa796{width:0.918in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5cffa7a0{width:87.136in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5cffa7a1{width:44.019in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5cffa7aa{width:0.918in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5cffa7ab{width:87.136in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5cffa7ac{width:44.019in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5cffa7b4{width:0.918in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5cffa7b5{width:87.136in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5cffa7be{width:44.019in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5cffa7bf{width:0.918in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5cffa7c0{width:87.136in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-5cffa7c8{width:44.019in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}review_idbeforeafter2I had a pleasure of staying in this hotel for 7 nights recently. This hotel was perfect in every way. Communication with the hotel before staying was prompt, and very efficient. Checking in was a breeze. You go through the spectacular lobby with modern glass chandeliers and take the elevator to your room. My room, they gave me an upgrade to junior suite, was spectacular. We had a walk-in closet of the size where you could have put a small bed in there; it served us nicely for the seven day stay. The decor was very refined, and oh the bathroom! Carrera marble floor was heated throughout, rain shower was to die for! Location, as it turned out, was as good as it can be. We were 5 minutes walk to Trafalgar Square, but it was very quiet. Right outside was Embankment tube stop. We would walk to theater area and to numerous restaurants, and many major sites, such as London Eye or Westminster Abbey were within walking distance. We had buffet breakfast or room service every morning. It was pricy, but my rate included it. Couple of nights, we had glass of wine sitting in front of fire place in the lobby. I used the spa, which is included in the room rate, almost every night. After a windchill day of sightseeing, the steam sauna and jacuzzi would soften my weary muscles. I have stayed in many 5 star hotels around the world, but this hotel tops it. I would return here in a heartbeat next time I am in London.pleasure stay hotel night recently hotel perfect communication hotel stay prompt efficient check breeze spectacular lobby modern glass chandelier elevator room room upgrade junior suite spectacular walk closet size bed serve nicely day stay decor refine bathroom carrera marble floor heat rain shower die location minute walk trafalgar square quiet embankment tube stop walk theater numerous restaurant major site london eye westminster abbey walk distance buffet breakfast room service morning pricy rate include couple night glass wine sit front fire lobby spa include room rate night windchill day sightsee steam sauna jacuzzi soften weary muscle stay star hotel world hotel top return heartbeat time london3A very lovely first visit to this iconic hotel bar! Wonderful service, without being intrusive at all! Very delicious cocktails and just generally all round, a very indulgent experience. Well worth visiting only for that 'once in a lifetime' experience, though do make sure you are feeling 'flush' it doesn't come cheap!lovely visit iconic hotel bar wonderful service intrusive delicious cocktail round indulgent experience worth visit lifetime experience feel flush cheap43 of us stayed at the Rhodes Hotel for 4 nights, its a great location for taking the Paddington Express from Heathrow. We like the location clost to the partk and in walking distance of most locations. The room and bath were small compared to American Hotels but very clean. We enjoyed the free WIFI. The owners and the staff were very friendly and helpful with taxi's and resturant recomendations. We would stay there again.stay rhodes hotel night location paddington express heathrow location clost partk walk distance location room bath compare american hotel clean enjoy free wifi owner staff friendly helpful taxi's restaurant recomendations stay At this point, you might decide to throw out smaller reviews because they are unlikely to identify multiple topics (Gils 2020). References "],["bigrams.html", "1.6 Bigrams", " 1.6 Bigrams Bigrams should not contain stop words. However, they should be adjacent words, so tokenize into bigrams, split into words an then filter out rows where one or both words is stop word. bigram_0 &lt;- token_2 %&gt;% summarize(.by = review_id, reconstructed = paste(word, collapse = &quot; &quot;)) %&gt;% unnest_tokens(&quot;bigram&quot;, reconstructed, token = &quot;ngrams&quot;, n = 2) # Remove the bigrams where one or both words are stop words bigram &lt;- bigram_0 %&gt;% separate(bigram, into = c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% anti_join(stop, by = join_by(word1 == word)) %&gt;% anti_join(stop, by = join_by(word2 == word)) %&gt;% mutate(bigram = paste(word1, word2)) %&gt;% select(review_id, bigram) # Example bind_cols( hotel_2 %&gt;% filter(review_id == 3) %&gt;% select(review), bigram %&gt;% filter(review_id == 3) %&gt;% summarize(bigrams = paste(bigram, collapse = &quot;\\n&quot;)) ) %&gt;% flextable::flextable() %&gt;% flextable::autofit() %&gt;% flextable::width(j = 1, width = 4.5, unit = &quot;in&quot;) %&gt;% flextable::width(j = 2, width = 1.5, unit = &quot;in&quot;) %&gt;% flextable::valign(valign = &quot;top&quot;) .cl-6f0b9d5a{}.cl-6f04e618{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-6f04e622{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-6f07fd3a{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-6f07fd44{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-6f080b4a{width:4.5in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-6f080b4b{width:1.5in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-6f080b54{width:4.5in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-6f080b55{width:1.5in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}reviewbigramsA very lovely first visit to this iconic hotel bar! Wonderful service, without being intrusive at all! Very delicious cocktails and just generally all round, a very indulgent experience. Well worth visiting only for that 'once in a lifetime' experience, though do make sure you are feeling 'flush' it doesn't come cheap!iconic hotelhotel barbar wonderfulwonderful servicedelicious cocktailindulgent experienceworth visitlifetime experiencefeel flush "],["save.html", "1.7 Save", " 1.7 Save Save the cleaned data for other analyses like topic modeling and sentiment analysis. hotel_prep &lt;- hotel_2 save(hotel_prep, token, bigram, file = &quot;input/hotel_prepped.Rdata&quot;) "],["topicmodeling.html", "Chapter 2 Topic Modeling", " Chapter 2 Topic Modeling library(tidyverse) library(stm) library(topicmodels) library(scales) library(glue) Here I will continue working with the hotel data and follow the ideas from Nagelkerke (2020b) and the stm package vignette package. Topic models are unsupervised ML models that identify topics as clusters of words with an associated probability distribution, and a probability distribution of topics within each document. There are two commonly used models: LDA and STM. LDA is the simpler model and is implemented in the popular topicmodels package. STM incorporates document metadata into the model. It is implemented in the STM package. These notes also discuss CTM, also in topicmodels, that is somewhere between LDA and STM. I discuss CTM because it is a bridge from LDA to STM. All three topic models are generative models of word counts. That means they assume there is some process that generates text which is a mixture of topics composed of words which occur with varying probabilities. Think of the observed text document as the product of an algorithm that selected each word in two stages: 1) it sampled a topic from a probability distribution, then 2) it sampled a word from the topic’s word probability distribution. The object in topic modeling is to tune the hyperparameters that define those probability distributions. In a way, topic models do the opposite of what you might expect. They are not estimating the probability that each document is one of those topics. They assume all topics contribute to each document and instead estimate their relative contributions. More concisely, the models treat documents as a mixture of topics, and the topics as a mixture of words where each word has a probability of belonging to each topic. The sum of topic proportions in document is one; the sum of word probabilities in a topic is one. This leads to two frameworks for thinking about topics. A topic’s prevalance in a document measures the proportion of the document generated by it. The topic’s content is the probability distribution of words associated with it. What distinguishes the following following models is how they handle these frameworks. An STM model defines covariates associated with prevalence and content; CTM does not. I don’t think LDA does either. We’re kind of at the limit of my understanding here. References "],["learning-by-example.html", "2.1 Learning by Example", " 2.1 Learning by Example I will work through the model concepts by example using the stm::gadarian data set. This data set has n = 351 comments about immigration in an experimental setting. The test group as specifically instructred to write about what made them anxious about immigration. There is also a variable pid_rep for political party. load(&quot;input/hotel_prepped.Rdata&quot;) gadarian_dat &lt;- stm::gadarian %&gt;% rename(comment = open.ended.response) %&gt;% select(-MetaID) glimpse(gadarian_dat) ## Rows: 341 ## Columns: 3 ## $ treatment &lt;dbl&gt; 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, … ## $ pid_rep &lt;dbl&gt; 1.00000, 1.00000, 0.33300, 0.50000, 0.66667, 0.00000, 0.8330… ## $ comment &lt;chr&gt; &quot;problems caused by the influx of illegal immigrants who are… "],["lda.html", "2.2 LDA", " 2.2 LDA Latent Dirichlet allocation (LDA) is an instance of a general family of mixed membership models that decompose data into latent components. Latent refers to unidentified topics and Dirichlet refers to the type of distribution followed by the words in the the topics and by the topics in the documents. Algorithm LDA assumes each document is created by a generative process where topics are included according to probabilities and words are included in the topics according to probabilities. The LDA algorithm determines what those probabilities are. The algorithm is: For each document \\(d_i\\), randomly assign each word to one of the K topics. Note that each \\(w_j\\) may be assigned to a different topic in each documents. For each document, tabulate the number of words in each topic, a \\(d \\times K\\) matrix. For each word, tabulate the sum of occurrences across all documents, a \\(w \\times K\\) matrix. Resample a single instance of a word from the corpus and remove it from the analysis, decrementing the document’s topic count and the word’s topic count. Calculate the gamma matrix, \\(\\gamma\\), and the beta matrix, \\(\\beta\\). the gamma matrix is the probability distribution of topics for each document, \\[p(t_k|d_i) = \\frac{n_{ik} + \\alpha}{N_i + K \\alpha}\\] were \\(n_{ik}\\) is the number of words in document \\(i\\) for topic \\(k\\), \\(N_i\\) is the total number of words in \\(i\\), and \\(\\alpha\\) is a hyperparameter. For each \\(d_i\\), \\(\\sum_{k \\in K} \\gamma_{ik} = 1\\). the beta matrix is the probability distribution of words for each topic, \\[p(w_j|t_k) = \\frac{m_{j,k} + \\beta}{\\sum_{j \\in V}m_{j,k} + V\\beta}\\] where \\(m_{j,k}\\) is the corpus-wide frequency count of word \\(w_j\\) to topic \\(k\\), \\(V\\) is the number of distinct words in the corpus, and \\(\\beta\\) is a hyperparameter. For each \\(t_k\\), \\(\\sum_{j \\in V} \\beta_{kj} = 1\\). Perform Gibbs sampling. Calculate the joint probability distribution of words for each document and topic, \\(p(w_j|t_k,d_i) = p(t_k|d_i)p(w_j|t_k)\\). Assign each word, \\(w_j\\), to the topic with the maximum joint probability. Repeat steps 3-6 for all of the words in all of the documents. Repeat steps 3-7 for a pre-determined number of iterations. LDA thus has 3 hyperparameters: document-topic density factor, \\(\\alpha\\), topic-word density factor, \\(\\beta\\), and topic count, \\(K\\). \\(\\alpha\\) controls the number of topics expected per document (large \\(\\alpha\\) = more topics). \\(\\beta\\) controls the distribution of words per topic (large \\(\\beta\\) = more words). Ideally, you want a few topics per document and a few words per topics, so, \\(\\alpha\\) and \\(\\beta\\) are typically set below one. \\(K\\) is set using a combination of domain knowledge, coherence, and exclusivity. Evaluation Held-out Likelihood (discussion of hold-out probability) (Wallach et al., 2009). Semantic Coherence Exclusivity Generally, the greater the number of topics in a model, the lower the quality of the smallest topics. One way around this is simply hiding the low-quality topics. The coherence measure (Mimno et al. 2011) evaluates topics. References "],["ctm.html", "2.3 CTM", " 2.3 CTM The Correlated Topic Model (CTM) (Blei and Lafferty 2007) builds on the LDA model (chapter 2.2). References "],["stm.html", "2.4 STM", " 2.4 STM STM incorporates arbitrary document metadata into the topic model. Without the inclusion of covariates, STM reduces to a logistic-normal topic model, often called the Correlated Topic Model (CTM) (chapter 2.3). The goal of STM is to discover topics and estimate their relationship to the metadata. Data Preparation The stm package represents a text corpus as an object with three components: a sparse matrix of counts by document and vocabulary word vector index, the vocabulary word vector, and document metadata. stm::textProcessor() is essentially a wrapper around the tm package. It: * converts words to lowercase, * removes stop words (including custom stop words!), numbers, and punctuation, and * stems words. After processing, stm::prepDocuments() removes infrequently appearing words, and removes any documents that contain no words after processing and removing words. gadarian_processed &lt;- textProcessor(gadarian_dat$comment, metadata = gadarian_dat) ## Building corpus... ## Converting to Lower Case... ## Removing punctuation... ## Removing stopwords... ## Removing numbers... ## Stemming... ## Creating Output... plotRemoved(gadarian_processed$documents, lower.thresh = seq(10, 200, by = 10)) Prepare Evaluate Interpret Visualize "],["data-formats.html", "2.5 Data Formats", " 2.5 Data Formats There are five common text mining packages, each with their own format requirements. Whichever package you work in, there is a decent chance you will want to use a function from one of the others, so you need some fluency in them all. tm works with Corpus objects (raw text with document and corpus metadata). Many tm algorithms work with a document-term matrix (DTM), a sparse matrix with one row per document, one column per term, and values equaling the word count or tf-idf. quanteda also works with Corpus objects, but has its own implementation. Many quanteda algorithms work with a document-feature matrix (DFM), again similar to tm’s DTM. tidytext works with tibbles. Many tidytext algorithms work with tibbles with one row per token (usually a word, but possibly a large item of text), a frequency count column, and possibly other metadata columns. qdap works with text fields in a data frame, so it does not require any particular data structure. sentimentr is similar to qdap. Let’s take the sawyer_raw data frame and pre-process it for all three packages. tm Turn the character vector sawyer_raw$text into a text source with VectorSource(), then turn the text source into a corpus with vCorpus(). Clean the corpus with a series of utility functions. One particularly important function, removeWords(), removes stop words, plus any custom stop words. I would normally add “tom” because it is so ubiquitous throughout the text. However, in this case I won’t because stopwords includes valence shifting words like “very” which are used in polarity scoring. I can remove them later for other exercises. # (sawyer_tm &lt;- VCorpus(VectorSource(sawyer$text)) %&gt;% # tm_map(content_transformer(replace_abbreviation)) %&gt;% # tm_map(removePunctuation) %&gt;% # tm_map(removeNumbers) %&gt;% # tm_map(content_transformer(tolower)) %&gt;% # tm_map(removeWords, c(stopwords(&quot;en&quot;), &quot;tom&quot;)) %&gt;% # tm_map(stripWhitespace)) Each document in the sawyer_tm VCorpus is a line of text. Use DocumentTermMaterix() to convert the vCorpus into tm’s bag-of-words format, DTM. # (sawyer_tm_dtm &lt;- DocumentTermMatrix(sawyer_tm)) This is a very sparse (nearly 100% sparse) matrix documents as rows and distinct words as columns. # group_by(chapter) %&gt;% # mutate(text = paste(text, collapse = &quot; &quot;)) %&gt;% # slice_head(n = 1) %&gt;% # select(chapter, text) # # sawyer_sent &lt;- sawyer %&gt;% # sentSplit(&quot;text&quot;) # # skimr::skim(sawyer) quanteda dafdafd tidytext dafdafd "],["sentimentanalysis.html", "Chapter 3 Sentiment Analysis", " Chapter 3 Sentiment Analysis Sentiment analysis is the extraction of the emotional intent of text. You can classify the polarity (positive | negative) or sentiment (angry | sad | happy | …) at the document, sentence, or feature level. This section continues with the hotel data from 1. load(&quot;input/hotel_prepped.Rdata&quot;) hotel_0 &lt;- hotel_prep glimpse(token) ## Rows: 1,158,125 ## Columns: 3 ## $ review_id &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … ## $ word &lt;chr&gt; &quot;pleasure&quot;, &quot;stay&quot;, &quot;hotel&quot;, &quot;night&quot;, &quot;recently&quot;, &quot;hotel&quot;, &quot;… ## $ n &lt;int&gt; 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, … "],["subjectivity-lexicons.html", "3.1 Subjectivity Lexicons", " 3.1 Subjectivity Lexicons A subjectivity lexicon is a predefined list of words associated with emotional context such as positive/negative. Subjectivity lexicons are typically short (a few thousand words), but work because of Zipf’s law which holds that the nth-ranked item in a frequency table has a frequency count equal to 1/n of the top-ranked item. So infrequently used words are used very infrequently. There are three common sentiment lexicons. Bing is common for polarity scoring, AFINN for emotion classification. NRC is a less common option for emotion classification. Bing classifies words as positive or negative. bing &lt;- tidytext::get_sentiments(&quot;bing&quot;) %&gt;% # remove dups filter(!word %in% c(&quot;envious&quot;, &quot;enviously&quot;, &quot;enviousness&quot;)) bing %&gt;% count(sentiment) %&gt;% adorn_totals() %&gt;% flextable::flextable() %&gt;% flextable::autofit() .cl-726a895c{}.cl-72637dec{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-72637e00{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-7266cf4c{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-7266cf4d{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-7266dd5c{width:0.941in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7266dd66{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7266dd70{width:0.941in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7266dd71{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7266dd7a{width:0.941in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7266dd7b{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7266dd7c{width:0.941in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7266dd84{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}sentimentnnegative4,778positive2,002Total6,780 AFINN, by Finn Arup Nielsen, associates words with a manually rated valence integer between -5 (negative) and +5 (positive). afinn &lt;- tidytext::get_sentiments(&quot;afinn&quot;) afinn %&gt;% count(value) %&gt;% adorn_totals() %&gt;% flextable::flextable() %&gt;% flextable::autofit() .cl-728c2d82{}.cl-7285ebb6{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-7285ebc0{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-7288854c{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-72888560{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-72889406{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-72889410{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7288941a{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7288941b{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7288941c{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-72889424{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7288942e{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7288942f{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-72889438{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-72889439{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-72889442{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-72889443{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7288944c{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-7288944d{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}valuen-516-443-3264-2966-13090112082448317244555Total2,477 NRC lexicon associates words with eight emotions corresponding to the second level of Plutchik’s Wheel of Emotions and two sentiments (negative and positive). NRC was created by manual annotation on a crowd sourcing platform (see this). nrc &lt;- tidytext::get_sentiments(&quot;nrc&quot;) nrc %&gt;% count(sentiment) %&gt;% adorn_totals() %&gt;% flextable::flextable() %&gt;% flextable::autofit() .cl-72a2cbd2{}.cl-729c96ae{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-729c96b8{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-729efb2e{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-729efb38{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-729f09f2{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-729f09f3{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-729f09fc{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-729f09fd{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-729f0a06{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-729f0a07{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-729f0a08{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-729f0a10{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-729f0a11{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-729f0a1a{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-729f0a1b{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-729f0a24{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}sentimentnanger1,245anticipation837disgust1,056fear1,474joy687negative3,316positive2,308sadness1,187surprise532trust1,230Total13,872 "],["polarity-scoring.html", "3.2 Polarity Scoring", " 3.2 Polarity Scoring Two packages measure text polarity. The simpler one tidytext: unnest tokens, join to the Bing lexicon, and calculate the net of positive minus negative polarity counts. sentimentr is more sophisticated in that it takes into account valence shifters, surrounding words that change the intensity of a sentiment (e.g., “very”) or switch its direction (e.g., “not”).3 3.2.1 tidytext The tidytext way to score polarity is to tag words as “positive” and “negative” using the Bing lexicon, then calculate the difference in counts. The qdap and sentimentr packages correct for text length by dividing by \\(\\sqrt{n}\\). It is useful to capture the positive and negative words back in the main data frame for explaining how the polarity score was calculated. polarity_bing &lt;- token %&gt;% left_join(bing, by = &quot;word&quot;, relationship = &quot;many-to-one&quot;) %&gt;% summarize(.by = c(review_id, sentiment), n = n(), words = list(word)) %&gt;% pivot_wider(names_from = sentiment, values_from = c(n, words), values_fill = list(n = 0)) %&gt;% select(-c(n_NA, words_NA)) %&gt;% inner_join(hotel_0 %&gt;% select(review_id, word_cnt), by = &quot;review_id&quot;) %&gt;% mutate( polarity = (n_positive - n_negative) / sqrt(word_cnt), polarity_desc = if_else(polarity &gt;= 0, &quot;Positive&quot;, &quot;Negative&quot;) ) polarity_bing %&gt;% filter(review_id == 520) ## # A tibble: 1 × 8 ## review_id n_positive n_negative words_positive words_negative word_cnt ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;list&gt; &lt;list&gt; &lt;int&gt; ## 1 520 0 0 &lt;NULL&gt; &lt;NULL&gt; 47 ## # ℹ 2 more variables: polarity &lt;dbl&gt;, polarity_desc &lt;chr&gt; polarity_afinn &lt;- token %&gt;% inner_join(afinn, by = &quot;word&quot;, relationship = &quot;many-to-one&quot;) %&gt;% summarize(.by = review_id, sentiment = sum(value), words = list(word)) %&gt;% inner_join(hotel_0 %&gt;% select(review_id, word_cnt), by = &quot;review_id&quot;) %&gt;% mutate( polarity = sentiment / sqrt(word_cnt), polarity_desc = if_else(polarity &gt;= 0, &quot;Positive&quot;, &quot;Negative&quot;) ) # Attach to main data frame hotel_1 &lt;- hotel_0 %&gt;% left_join(polarity_bing %&gt;% select(review_id, polarity, words_positive, words_negative) %&gt;% rename_with(~paste0(&quot;bing_&quot;, .x)), by = join_by(review_id == bing_review_id)) %&gt;% left_join(polarity_afinn %&gt;% select(review_id, polarity) %&gt;% rename_with(~paste0(&quot;afinn_&quot;, .x)), by = join_by(review_id == afinn_review_id)) Let’s see how the polarity scores compare. hotel_1 %&gt;% pivot_longer(cols = c(bing_polarity, afinn_polarity), names_to = &quot;lexicon&quot;, values_to = &quot;polarity&quot;) %&gt;% filter(!is.na(polarity)) %&gt;% ggplot(aes(x = polarity, y = fct_rev(property))) + geom_boxplot() + facet_wrap(facets = vars(lexicon)) + labs(title = &quot;Review polarity&quot;, x = NULL, y = NULL, caption = glue::glue(&quot;Bing Polarity = (n_pos - n_neg) / sqrt(n_words)\\n&quot;, &quot;AFINN Polarity = sentiment / sqrt(n_words)&quot;)) The two lexicons are similar. The data set includes a numeric rating review_rating (1-5). I’ll stick with Bing going forward for convenience. The polarity score should correlate with the numeric rating. hotel_1 %&gt;% filter(!is.na(bing_polarity)) %&gt;% ggplot(aes(x = as_factor(rating), y = bing_polarity)) + geom_jitter(width = 0.2, alpha = 0.3, color = &quot;#5DA5DA&quot;, size = 1) + geom_boxplot(alpha = 0) + theme_minimal() + labs(title = &quot;Polarity is associated with overall Likert score&quot;, x = &quot;Overall Likert Rating&quot;, y = &quot;Polarity Score&quot;) Sentiment increases with Likert rating, but there are many reviews with a rating of 5 and a polarity score &lt;0. In some cases this is because the reviewer interpreted the scale incorrectly. You can use polarity scores to identify problematic reviews like these. hotel_1 %&gt;% mutate( problematic = case_when( (rating == 1 &amp; bing_polarity &gt; 0.5) ~ &quot;Too Low&quot;, (rating == 5 &amp; bing_polarity &lt; -.5) ~ &quot;Too High&quot;, TRUE ~ &quot;Other&quot; ) ) %&gt;% filter(problematic %in% c(&quot;Too High&quot;, &quot;Too Low&quot;)) %&gt;% group_by(problematic) %&gt;% slice_max(order_by = abs(bing_polarity), n = 1) %&gt;% select(problematic, rating, bing_polarity, review) %&gt;% flextable::flextable() %&gt;% flextable::autofit() %&gt;% flextable::valign(valign = &quot;top&quot;) .cl-74aa8118{}.cl-74a481aa{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-74a481b4{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-74a6e7a6{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-74a6e7b0{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-74a6e7ba{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-74a6e7c4{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-74a6f82c{width:1.072in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-74a6f836{width:0.671in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-74a6f837{width:1.157in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-74a6f840{width:32.807in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-74a6f841{width:1.072in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-74a6f84a{width:0.671in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-74a6f84b{width:1.157in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-74a6f854{width:32.807in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-74a6f855{width:1.072in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-74a6f85e{width:0.671in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-74a6f85f{width:1.157in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-74a6f868{width:32.807in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}problematicratingbing_polarityreviewToo High5-0.9801961Some design faults in the bathroom - no stool, misplaced grab handles and vanity mirror. Very disappointing experience in Savoy Grill. Good quality ingredients but poorly presented and tasteless. Numerous mistakes in service including charging for expensive drinks which we did not have. Service charge revoked and booking for following night cancelled.Too Low10.8728716Stayed here many times both for business and pleasure, alone, with my wife and even with extended family and children. It is in fact impossible to seperate the business and pleasure stays because everytime was a breathtakingly delectable pleasure. Meeting friends and colleagues whether in the lobby one of the resturants always left an impressive memorable impression. Alas, I haven't been able to visit again for sometime - either fully booked or are not able to guarantee convenient parking for my personal chauffeur and car. The polarity words can help explain why some hotels rated poor or excellent. token %&gt;% inner_join(hotel_1 %&gt;% filter(rating %in% c(1, 5)), by = join_by(review_id)) %&gt;% filter(!word %in% c(&quot;hotel&quot;, &quot;stay&quot;, &quot;night&quot;)) %&gt;% filter((rating == 5 &amp; bing_polarity &gt; 0) | (rating == 1 &amp; bing_polarity &lt; 0)) %&gt;% count(rating, word) %&gt;% mutate(.by = rating, pct = n / sum(n)) %&gt;% group_by(rating) %&gt;% slice_max(order_by = pct, n = 10) %&gt;% ggplot(aes(x = pct, y = reorder_within(word, by = pct, within = rating))) + geom_col() + scale_y_reordered() + scale_x_continuous(labels = percent_format(1)) + labs(y = NULL, x = NULL) + facet_wrap(facets = vars(rating), scales = &quot;free_y&quot;) Word clouds are a nice way to get an overview of the data. token %&gt;% inner_join(hotel_1 %&gt;% filter(rating %in% c(1, 5)), by = join_by(review_id)) %&gt;% filter(!word %in% c(&quot;hotel&quot;, &quot;stay&quot;, &quot;night&quot;, &quot;london&quot;), !is.na(bing_polarity)) %&gt;% mutate(polarity_desc = if_else(bing_polarity &gt; 0, &quot;Positive&quot;, &quot;Negative&quot;)) %&gt;% count(word, polarity_desc, wt = word_cnt) %&gt;% pivot_wider(names_from = polarity_desc, values_from = n, values_fill = 0) %&gt;% data.table::data.table() %&gt;% as.matrix(rownames = &quot;word&quot;) %&gt;% wordcloud::comparison.cloud(max.words = 30, title.size = 1.5, scale = c(1, 3.5)) 3.2.2 sentimentr sentimentr calculates polarity at the sentence level. It improves on tidytext in that it takes into account the context in which the sentiment words occur by incorporating valence shifters. A negator flips the direction of a polarizing word (e.g., “I do not like it.”). lexicon::hash_valence_shifters[y==1]. An amplifier intensifies the impact (e.g., “I really like it.”). lexicon::hash_valence_shifters[y==2]. A de-amplifier (downtoner) reduces the impact (e.g., “I hardly like it.”). lexicon::hash_valence_shifters[y==3]. An adversative conjunction overrules the previous clause containing a polarized word (e.g., “I like it but it’s not worth it.”). lexicon::hash_valence_shifters[y==4]. sentimentr uses a lexicon package combined from the syuzhet and lexicon packages. Positive words are scored +1 and negative words are scored -1. sentimentr identifies clusters of words within sentences of the text. The 4 words before and 2 words after are candidate valence shifters. Polarized words are weighted by the valence shifter weights: negators = -1; amplifiers and de-amplifiers = 1.8; adversative conjunctions decrease the value of the prior cluster and increase the value of the following cluster. Neutral words hold no value, but do affect the word count. hotel_sentimentr &lt;- sentimentr::get_sentences(hotel_1$review) %&gt;% sentimentr::sentiment() %&gt;% summarize(.by = element_id, sentimentr_polarity = mean(sentiment)) hotel_2 &lt;- hotel_1 %&gt;% mutate(element_id = row_number()) %&gt;% inner_join(hotel_sentimentr, by = join_by(element_id)) %&gt;% select(-element_id) Let’s see a few examples where sentimentr differed from tidytext. Looks like bing did a better job on the first one, but sentimentr was better on the next two. hotel_2 %&gt;% filter((bing_polarity &gt; 0.2 &amp; sentimentr_polarity &lt; -0.2) | (bing_polarity &lt; -0.2 &amp; sentimentr_polarity &gt; 0.2)) %&gt;% select(review, bing_polarity, sentimentr_polarity) %&gt;% head(3) %&gt;% flextable::flextable() %&gt;% flextable::autofit() .cl-9fdafe30{}.cl-9fd381b4{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-9fd381c8{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-9fd72e2c{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9fd72e36{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9fd73cf0{width:56.161in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9fd73cfa{width:1.157in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9fd73cfb{width:1.574in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9fd73d04{width:56.161in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9fd73d05{width:1.157in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9fd73d0e{width:1.574in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9fd73d0f{width:56.161in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9fd73d18{width:1.157in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9fd73d19{width:1.574in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9fd73d22{width:56.161in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9fd73d2c{width:1.157in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9fd73d2d{width:1.574in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}reviewbing_polaritysentimentr_polarityWe were locked out of our room twice in a 3 day stay due to the Savoy not being able to track our reservation - the excuses included that we were responsible until they finally admitted their system is fouled up-0.31622780.2161730For many years I had fantasised about staying at The Dorchester so when the opportunity arose I jumped at it. Needless to say, as one of the great British institutions, it's a lovely hotel however it is not fabulous. It has two lovely dining areas /restaurants albeit pricey. I was extremely disappointed by the rooms! The corridors were creaky and narrow and the rooms pokey. There's no doubt it was tasteful decorated, however I was noisy -if the internal door wasn't shut and space was tight with a capital T. The double bed was very compact for 2, there was hardly any space to walk around it! Very claustrophobic! On a positive note the bathroom was spacious with delicious toiletries and good high pressure hot showers - and it you are a lover of hotel slippers- this one was good quality. All in all - charming but not worth the price or reputation- that is in the standard rooms. A word of advise- don't take your car!0.4670994-0.2377933Wonderful. Far better than The Ritz (slow and stiff) and Harvey Nichols (noisy and showy). The service is unsurpassable. Extensive range of teas. Enough food to make dinner unnecessary. Relaxed and unhurried atmosphere. I'm already looking forward to coming back.-0.47434160.2145884 There is a third package called qdap, but the sentimentr Read Me explains sentimentr is an improved version that better balances accuracy and speed.↩︎ "],["statistical-test.html", "3.3 Statistical Test", " 3.3 Statistical Test You can fit an ordinal logistic regression model to predict the rating based on the review sentiment. Which performs better, tidytext or sentimentr? Start with an intercept-only model for a baseline and review of ordinal logistic regression. # Limit to cases where both methods were able to score the review. mdl_dat &lt;- hotel_2 %&gt;% mutate(rating_fct = factor(rating, ordered = TRUE)) fit_intercept &lt;- ordinal::clm(rating_fct ~ 1, data = mdl_dat) summary(fit_intercept) ## formula: rating_fct ~ 1 ## data: mdl_dat ## ## link threshold nobs logLik AIC niter max.grad cond.H ## logit flexible 23376 -21411.54 42831.08 6(0) 1.13e-11 2.3e+01 ## ## Threshold coefficients: ## Estimate Std. Error z value ## 1|2 -3.84388 0.04566 -84.19 ## 2|3 -3.03916 0.03132 -97.03 ## 3|4 -2.15866 0.02147 -100.55 ## 4|5 -0.81226 0.01417 -57.30 The threshold coefficients in the summary table are the log-odds of the outcome variable having a level at or below vs above. Below, 10.3% of ratings were &lt;=3 and 89.7% were &gt;=4 for a log-odds of log(.103/.897) = -2.1620836, corresponding to the 3|4 line in the regression summary. mdl_dat %&gt;% tabyl(rating_fct) %&gt;% mutate(cum = cumsum(percent), `1-cum` = 1 - cum) ## rating_fct n percent cum 1-cum ## 1 490 0.02096167 0.02096167 0.9790383 ## 2 578 0.02472621 0.04568789 0.9543121 ## 3 1352 0.05783710 0.10352498 0.8964750 ## 4 4766 0.20388433 0.30740931 0.6925907 ## 5 16190 0.69259069 1.00000000 0.0000000 Now fit the bing and sentimentr models. The bing model has the lowest log-likelihood. fit_bing &lt;- ordinal::clm(rating_fct ~ bing_polarity, data = mdl_dat) fit_sentimentr &lt;- ordinal::clm(rating_fct ~ sentimentr_polarity, data = mdl_dat) anova(fit_bing, fit_sentimentr, fit_intercept) ## Likelihood ratio tests of cumulative link models: ## ## formula: link: threshold: ## fit_intercept rating_fct ~ 1 logit flexible ## fit_bing rating_fct ~ bing_polarity logit flexible ## fit_sentimentr rating_fct ~ sentimentr_polarity logit flexible ## ## no.par AIC logLik LR.stat df Pr(&gt;Chisq) ## fit_intercept 4 42831 -21412 ## fit_bing 5 37728 -18859 5105.38 1 &lt; 2.2e-16 *** ## fit_sentimentr 5 38528 -19259 -800.44 0 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 How about predictive performance? They both peformed about the same. (bing_conf &lt;- fit_bing %&gt;% augment(type = &quot;class&quot;) %&gt;% conf_mat(truth = rating_fct, estimate = .fitted)) ## Truth ## Prediction 1 2 3 4 5 ## 1 72 21 12 7 2 ## 2 0 0 0 0 0 ## 3 34 29 19 6 4 ## 4 306 345 478 547 705 ## 5 78 183 843 4206 15479 summary(bing_conf) ## # A tibble: 13 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.689 ## 2 kap multiclass 0.144 ## 3 sens macro 0.246 ## 4 spec macro 0.831 ## 5 ppv macro NA ## 6 npv macro 0.885 ## 7 mcc multiclass 0.171 ## 8 j_index macro 0.0778 ## 9 bal_accuracy macro 0.539 ## 10 detection_prevalence macro 0.2 ## 11 precision macro 0.453 ## 12 recall macro 0.246 ## 13 f_meas macro 0.314 (sentimentr_conf &lt;- fit_sentimentr %&gt;% augment(type = &quot;class&quot;) %&gt;% conf_mat(truth = rating_fct, estimate = .fitted)) ## Truth ## Prediction 1 2 3 4 5 ## 1 34 18 14 2 6 ## 2 0 0 0 0 0 ## 3 20 9 7 6 1 ## 4 362 346 485 322 331 ## 5 74 205 846 4436 15852 summary(sentimentr_conf) ## # A tibble: 13 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.694 ## 2 kap multiclass 0.123 ## 3 sens macro 0.224 ## 4 spec macro 0.828 ## 5 ppv macro NA ## 6 npv macro 0.904 ## 7 mcc multiclass 0.159 ## 8 j_index macro 0.0524 ## 9 bal_accuracy macro 0.526 ## 10 detection_prevalence macro 0.2 ## 11 precision macro 0.384 ## 12 recall macro 0.224 ## 13 f_meas macro 0.268 "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
