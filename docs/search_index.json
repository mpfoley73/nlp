[["topicmodeling.html", "Chapter 2 Topic Modeling", " Chapter 2 Topic Modeling Topic models are unsupervised ML models that identify topics as clusters of words with an associated probability distribution, and a probability distribution of topics within each document. There are two commonly used models: LDA and STM. LDA, implemented in the topicmodels package, is the simpler model. STM, implemented in the stm package, incorporates document metadata into the model. Both topic models are generative models of word counts. That means they assume there is some process that generates text which is a mixture of topics composed of words which occur with varying probabilities. Think of the observed text document as the product of an algorithm that selected each word in two stages: 1) it sampled a topic from a probability distribution, then 2) it sampled a word from the topic’s word probability distribution. The object in topic modeling is to tune the hyperparameters that define those probability distributions. In a way, topic models do the opposite of what you might expect. They are not estimating the probability that each document is one of those topics. They assume all topics contribute to each document and instead estimate their relative contributions. More concisely, the models treat documents as a mixture of topics, and the topics as a mixture of words, and both the topics and topic words have probability distributions. The sum of topic proportions in document is one; the sum of word probabilities in a topic is one. This leads to two frameworks for thinking about topics. A topic’s prevalance measures the proportion of the document generated by it. The topic’s content is the probability distribution of words associated with it. LDA and STM differ only in how they handle these frameworks. STM defines covariates associated with prevalence and content while sTM does not.1 This section continues with the hotel data from Chapter 1 and follows the ideas from Nagelkerke (2020) and the stm package vignette package. library(tidyverse) library(tidymodels) library(topicmodels) library(tidytext) library(stm) # library(tm) # generic text mining utilities. Great for DTMs. # library(LDAvis) # visualizing LDA models library(scales) library(glue) load(&quot;input/hotel_prepped.Rdata&quot;) glimpse(token) ## Rows: 1,142,890 ## Columns: 3 ## $ review_id &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … ## $ word &lt;chr&gt; &quot;pleasure&quot;, &quot;stay&quot;, &quot;hotel&quot;, &quot;night&quot;, &quot;recently&quot;, &quot;hotel&quot;, &quot;… ## $ n &lt;int&gt; 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, … References "],["lda.html", "2.1 LDA", " 2.1 LDA Latent Dirichlet allocation (LDA) is an instance of a general family of mixed membership models that decompose data into latent components. Latent refers to unidentified topics and Dirichlet refers to the type of distribution followed by the words in the the topics and by the topics in the documents. Algorithm LDA assumes each document is created by a generative process where topics are included according to probabilities and words are included in the topics according to probabilities. The LDA algorithm determines what those probabilities are. The algorithm is: For each document \\(d_i\\), randomly assign each word \\(w_j\\) to one of the \\(K\\) topics. Note that \\(w_j\\) may be assigned to a different topic in different documents. For each document, tabulate the number of words in each topic, a \\(d \\times K\\) matrix. For each word, tabulate the sum of occurrences across all documents, a \\(w \\times K\\) matrix. Resample a single instance of a word from the corpus and remove it from the analysis, decrementing the document’s topic count and the word’s topic count. Calculate the gamma matrix, \\(\\gamma\\), and the beta matrix, \\(\\beta\\). the gamma matrix is the probability distribution of topics for each document, \\[p(t_k|d_i) = \\frac{n_{ik} + \\alpha}{N_i + K \\alpha}\\] were \\(n_{ik}\\) is the number of words in document \\(i\\) for topic \\(k\\), \\(N_i\\) is the total number of words in \\(i\\), and \\(\\alpha\\) is a hyperparameter. For each \\(d_i\\), \\(\\sum_{k \\in K} \\gamma_{ik} = 1\\). the beta matrix is the probability distribution of words for each topic, \\[p(w_j|t_k) = \\frac{m_{j,k} + \\beta}{\\sum_{j \\in V}m_{j,k} + V\\beta}\\] where \\(m_{j,k}\\) is the corpus-wide frequency count of word \\(w_j\\) to topic \\(k\\), \\(V\\) is the number of distinct words in the corpus, and \\(\\beta\\) is a hyperparameter. For each \\(t_k\\), \\(\\sum_{j \\in V} \\beta_{kj} = 1\\). Perform Gibbs sampling. Calculate the joint probability distribution of words for each document and topic, \\(p(w_j|t_k,d_i) = p(t_k|d_i)p(w_j|t_k)\\). Assign each word, \\(w_j\\), to the topic with the maximum joint probability. Repeat steps 3-6 for all of the words in all of the documents. Repeat steps 3-7 for a pre-determined number of iterations. LDA thus has 3 hyperparameters: the document-topic density factor, \\(\\alpha\\), the topic-word density factor, \\(\\beta\\), and the topic count, \\(K\\). \\(\\alpha\\) controls the number of topics expected per document (large \\(\\alpha\\) = more topics). \\(\\beta\\) controls the distribution of words per topic (large \\(\\beta\\) = more words). Ideally, you want a few topics per document and a few words per topic, so \\(\\alpha\\) and \\(\\beta\\) are typically set below one. \\(K\\) is set using a combination of domain knowledge, coherence, and exclusivity. Notice that LDA is a “bag of words” method. It does not consider the order of the tokens in the text, so where tokens are located what other tokens are nearby do not factor into the output. Data Preparation In addition to the cleaned text produced in Chapter 1, there are a few more data preparation tasks for LDA. Create a bag of words from the union of the word and bigram reviews to get all terms. Keep only the decent sized reviews (&gt;= 25 words). lda_dat &lt;- hotel_prep %&gt;% mutate(combined = paste(review_words, review_bigrams)) %&gt;% select(review_id, combined) %&gt;% unnest_tokens(output = &quot;word&quot;, input = combined) %&gt;% mutate(.by = review_id, n = n()) %&gt;% filter(n &gt;= 25) %&gt;% select(-n) lda_dat %&gt;% glimpse() ## Rows: 1,790,643 ## Columns: 2 ## $ review_id &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … ## $ word &lt;chr&gt; &quot;pleasure&quot;, &quot;stay&quot;, &quot;hotel&quot;, &quot;night&quot;, &quot;recently&quot;, &quot;hotel&quot;, &quot;… The next step is optional. If this is a predictive model, create a train/test split. You might even weight the splitting by rating (if that is the outcome variable) to ensure proportional coverage. hotel_split &lt;- rsample::initial_split(hotel_prep, prop = 3/4, strata = review_id) lda_train_0 &lt;- lda_dat %&gt;% inner_join(training(hotel_split) %&gt;% select(review_id, rating), by = join_by(review_id)) lda_test &lt;- lda_dat %&gt;% inner_join(testing(hotel_split) %&gt;% select(review_id, rating), by = join_by(review_id)) glimpse(lda_train_0) ## Rows: 1,342,642 ## Columns: 3 ## $ review_id &lt;int&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, … ## $ word &lt;chr&gt; &quot;stay&quot;, &quot;rhodes&quot;, &quot;hotel&quot;, &quot;night&quot;, &quot;location&quot;, &quot;paddington&quot;… ## $ rating &lt;int&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, … glimpse(lda_test) ## Rows: 448,001 ## Columns: 3 ## $ review_id &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … ## $ word &lt;chr&gt; &quot;pleasure&quot;, &quot;stay&quot;, &quot;hotel&quot;, &quot;night&quot;, &quot;recently&quot;, &quot;hotel&quot;, &quot;… ## $ rating &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, … Low frequency tokens impact the topic model analysis. 58% of the data is composed of tokens that appear &lt;=3 times. Just use the high frequency tokens, the ones occurring at least 4 times in the training data. This is (hopefully) enough to learn topics from tokens that occur together in reviews. Nagelkerke (2020) explains that TF-IDF, which combines the within-document token frequency and document frequency, is not always the best way to whittle down the token set. When documents are small, which is common in online reviews, the within document frequency is low and the IDF part is over-weighted. lda_train_0 %&gt;% count(word, name = &quot;token_n&quot;) %&gt;% mutate(token_n = if_else(token_n &gt; 20, 20, token_n)) %&gt;% count(token_n) %&gt;% mutate(pct = n / sum(n), cum_pct = cumsum(pct)) %&gt;% ggplot(aes(x = token_n)) + geom_col(aes(y = pct)) + geom_line(aes(y = cum_pct)) + scale_y_continuous(labels = percent_format(1)) + labs(x = &quot;Token Frequency&quot;, y = &quot;Pct of Tokens&quot;, title = &quot;Token Frequency&quot;) lda_train &lt;- lda_train_0 %&gt;% mutate(.by = word, corpus_token_n = n()) %&gt;% mutate(.by = c(review_id, word), review_token_n = n()) %&gt;% filter(corpus_token_n &gt;= 4) bind_rows( `high freq words` = lda_train %&gt;% summarize(total_words = n(), distinct_words = n_distinct(word)), `low freq words` = anti_join(lda_train_0, lda_train, by = join_by(review_id, word)) %&gt;% summarize(total_words = n(), distinct_words = n_distinct(word)), .id = &quot;partition&quot; ) %&gt;% mutate(total_pct = total_words / sum(total_words) * 100, distinct_pct = distinct_words / sum(distinct_words) * 100) %&gt;% select(partition, total_words, total_pct, distinct_words, distinct_pct) ## # A tibble: 2 × 5 ## partition total_words total_pct distinct_words distinct_pct ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 high freq words 1316274 98.0 9637 41.9 ## 2 low freq words 26368 1.96 13377 58.1 Now we can create a document term matrix (DTM). Again from Nagelkerke (2020), you might expect the document specific term frequency (review_token_n), however I use the overall token frequency corpus_token_n to give more emphasis to terms that are more frequent in general. Reviews are short compared to books or articles, so the probability of a token occurring repeatedly in a review is low. lda_dtm &lt;- cast_dtm(lda_train, document = review_id, term = word, value = corpus_token_n) dim(lda_dtm) ## [1] 16050 9637 # Average token frequency. sum(lda_dtm) / sum(lda_dtm != 0) ## [1] 4337.798 Fit There are several parameters you might tweak for the model fit. The biggest surprise is that you set the number of topics, k. This article explains the harmonic mean method for optimization, but Nagelkerke (2020) suggests sticking with the art vs science method and pick your own k. The model fit took about 3 minutes to run. I ran it once then saved the result. # system.time(lda_fit &lt;- LDA(lda_dtm, k = 3)) # saveRDS(lda_fit, file = &quot;input/lda_fit.RDS&quot;) lda_fit &lt;- readRDS(file = &quot;input/lda_fit.RDS&quot;) The fitted object contains two matrices. The phi matrix is the distribution of tokens (cols) over topics (rows). The theta matrix is the distribution of documents (rows) over topics (cols). The row sum is 1 for each matrix (sum of topic probabilities, some of document probabilities). In each case, the values are probabilities that sum to 1 for each topic. lda_phi &lt;- posterior(lda_fit) %&gt;% pluck(&quot;terms&quot;) %&gt;% as.matrix() dim(lda_phi) ## [1] 3 9526 sum(lda_phi[1, ]) ## [1] 1 lda_theta &lt;- posterior(lda_fit) %&gt;% pluck(&quot;topics&quot;) %&gt;% as.matrix() dim(lda_theta) ## [1] 16032 3 sum(lda_theta[1, ]) ## [1] 1 The tidytext::tidy() function calculates the beta matrix then pivots-longer into a [topic, term, beta] data frame. lda_topics &lt;- tidy(lda_fit) lda_topics %&gt;% mutate(topic = factor(paste(&quot;Topic&quot;, topic))) %&gt;% group_by(topic) %&gt;% slice_max(order_by = beta, n = 10) %&gt;% ungroup() %&gt;% ggplot(aes(x = beta, y = reorder_within(term, by = beta, within = topic))) + geom_col() + scale_y_reordered() + facet_wrap(facets = vars(topic), scales = &quot;free_y&quot;) + labs(y = NULL, title = &quot;LDA Top 10 Terms&quot;) There is a downside to this evaluation. Popular words like room appear at or near the top in all three topics. You might want to look at relative popularity instead: the popularity within the topic divided by overall popularity. That’s problematic too because words that only appear in few reviews will pop to the top. What you want is a combination of both absolute term probability and relative term probability. LDAvis::serVis() can help you do that. Unfortunately, the plot from LDAvis::serVis() is interactive and does not render in the RMarkdown notebook html, so below is just a screenshot of the code chunk output. doc_length &lt;- lda_train %&gt;% count(review_id) %&gt;% pull(n) # vocabulary: unique tokens vocab &lt;- colnames(lda_phi) # overall token frequency term_frequency &lt;- lda_train %&gt;% count(word) %&gt;% arrange(match(word, vocab)) %&gt;% pull(n) # create JSON containing all needed elements json &lt;- LDAvis::createJSON(lda_phi, lda_theta, doc_length, vocab, term_frequency) LDAvis::serVis(json) Held-out Likelihood (discussion of hold-out probability) (Wallach et al., 2009). Semantic Coherence Exclusivity Generally, the greater the number of topics in a model, the lower the quality of the smallest topics. One way around this is simply hiding the low-quality topics. The coherence measure (Mimno et al. 2011) evaluates topics. References "],["stm.html", "2.2 STM", " 2.2 STM STM incorporates arbitrary document metadata into the topic model. Without the inclusion of covariates, STM reduces to a logistic-normal topic model, often called the Correlated Topic Model (CTM) (chapter ??). The goal of STM is to discover topics and estimate their relationship to the metadata. Data Preparation The stm package represents a text corpus as an object with three components: a sparse matrix of counts by document and vocabulary word vector index, the vocabulary word vector, and document metadata. stm::textProcessor() is essentially a wrapper around the tm package. It: * converts words to lowercase, * removes stop words (including custom stop words!), numbers, and punctuation, and * stems words. After processing, stm::prepDocuments() removes infrequently appearing words, and removes any documents that contain no words after processing and removing words. gadarian_dat &lt;- stm::gadarian %&gt;% rename(comment = open.ended.response) %&gt;% select(-MetaID) glimpse(gadarian_dat) ## Rows: 341 ## Columns: 3 ## $ treatment &lt;dbl&gt; 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, … ## $ pid_rep &lt;dbl&gt; 1.00000, 1.00000, 0.33300, 0.50000, 0.66667, 0.00000, 0.8330… ## $ comment &lt;chr&gt; &quot;problems caused by the influx of illegal immigrants who are… gadarian_processed &lt;- textProcessor(gadarian_dat$comment, metadata = gadarian_dat) ## Building corpus... ## Converting to Lower Case... ## Removing punctuation... ## Removing stopwords... ## Removing numbers... ## Stemming... ## Creating Output... plotRemoved(gadarian_processed$documents, lower.thresh = seq(10, 200, by = 10)) Prepare Evaluate Interpret Visualize "],["data-formats.html", "2.3 Data Formats", " 2.3 Data Formats There are five common text mining packages, each with their own format requirements. Whichever package you work in, there is a decent chance you will want to use a function from one of the others, so you need some fluency in them all. tm works with Corpus objects (raw text with document and corpus metadata). Many tm algorithms work with a document-term matrix (DTM), a sparse matrix with one row per document, one column per term, and values equaling the word count or tf-idf. quanteda also works with Corpus objects, but has its own implementation. Many quanteda algorithms work with a document-feature matrix (DFM), again similar to tm’s DTM. tidytext works with tibbles. Many tidytext algorithms work with tibbles with one row per token (usually a word, but possibly a large item of text), a frequency count column, and possibly other metadata columns. qdap works with text fields in a data frame, so it does not require any particular data structure. sentimentr is similar to qdap. Let’s take the sawyer_raw data frame and pre-process it for all three packages. tm Turn the character vector sawyer_raw$text into a text source with VectorSource(), then turn the text source into a corpus with vCorpus(). Clean the corpus with a series of utility functions. One particularly important function, removeWords(), removes stop words, plus any custom stop words. I would normally add “tom” because it is so ubiquitous throughout the text. However, in this case I won’t because stopwords includes valence shifting words like “very” which are used in polarity scoring. I can remove them later for other exercises. # (sawyer_tm &lt;- VCorpus(VectorSource(sawyer$text)) %&gt;% # tm_map(content_transformer(replace_abbreviation)) %&gt;% # tm_map(removePunctuation) %&gt;% # tm_map(removeNumbers) %&gt;% # tm_map(content_transformer(tolower)) %&gt;% # tm_map(removeWords, c(stopwords(&quot;en&quot;), &quot;tom&quot;)) %&gt;% # tm_map(stripWhitespace)) Each document in the sawyer_tm VCorpus is a line of text. Use DocumentTermMaterix() to convert the vCorpus into tm’s bag-of-words format, DTM. # (sawyer_tm_dtm &lt;- DocumentTermMatrix(sawyer_tm)) This is a very sparse (nearly 100% sparse) matrix documents as rows and distinct words as columns. # group_by(chapter) %&gt;% # mutate(text = paste(text, collapse = &quot; &quot;)) %&gt;% # slice_head(n = 1) %&gt;% # select(chapter, text) # # sawyer_sent &lt;- sawyer %&gt;% # sentSplit(&quot;text&quot;) # # skimr::skim(sawyer) quanteda dafdafd tidytext dafdafd "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
