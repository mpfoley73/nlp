[["index.html", "Natural Language Processing in R Intro", " Natural Language Processing in R Michael Foley 2023-11-27 Intro These notes consolidate several resources I’ve encountered while working on text mining projects: Text Mining with R (Silge and Robinson 2017) Introduction to Natural Language Processing in R (DataCamp) Topic Modeling in R (DataCamp) Introduction to Text Analysis in R” (DataCamp) String Manipulation in R with stringr (DataCamp) Text Mining with Bag-of-Words in R (DataCamp) Sentiment Analysis in R (DataCamp) Tidy Sentiment Analysis in R (DataCamp) Julia Silge’s The game is afoot! Topic modeling of Sherlock Holmes stories Julia Silge’s Training, evaluating, and interpreting topic models Toward understanding 17th century English culture: A structural topic model of Francis Bacon’s ideas References "],["data-prep.html", "Chapter 1 Data Preparation", " Chapter 1 Data Preparation This section covers how to prepare a corpus for text analysis. I’ll work with the customer reviews of London-based hotels data set hosted on data.world. hotel_raw contains 27K reviews of the ten most- and ten least-expensive hotels in London. The csv file is located online here. I saved it to my \\inputs directory. library(tidyverse) library(tidytext) library(scales) library(glue) hotel_0 &lt;- read_csv( &quot;input/london_hotel_reviews.csv&quot;, col_types = &quot;cicccc&quot;, col_names = c(&quot;property&quot;, &quot;rating&quot;, &quot;title&quot;, &quot;review&quot;, &quot;reviewer_loc&quot;, &quot;review_dt&quot;), skip = 1 ) %&gt;% mutate(review_id = row_number()) %&gt;% select(review_id, everything()) glimpse(hotel_0) ## Rows: 27,330 ## Columns: 7 ## $ review_id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17… ## $ property &lt;chr&gt; &quot;Apex London Wall Hotel&quot;, &quot;Corinthia Hotel London&quot;, &quot;The … ## $ rating &lt;int&gt; 5, 5, 5, 4, 5, 1, 5, 5, 5, 5, 5, 4, 2, 4, 5, 5, 5, 5, 5, … ## $ title &lt;chr&gt; &quot;Ottima qualità prezzo&quot;, &quot;By far, my best hotel in the wo… ## $ review &lt;chr&gt; &quot;Siamo stati a Londra per un week end ed abbiamo alloggia… ## $ reviewer_loc &lt;chr&gt; &quot;Casale Monferrato, Italy&quot;, &quot;Savannah, Georgia&quot;, &quot;London&quot;… ## $ review_dt &lt;chr&gt; &quot;10/20/2012&quot;, &quot;3/23/2016&quot;, &quot;7/30/2013&quot;, &quot;6/2/2012&quot;, &quot;11/2… "],["scrub.html", "1.1 Scrub", " 1.1 Scrub The data needs to be cleaned. I’ll follow some of the techniques used by Nagelkerke (2020a). One issue is tags like &lt;e9&gt; and unicode characters like &lt;U+0440&gt;. One way to get rid of unicode characters is to convert them to ASCII tags with iconv() and then remove the ASCII tags with str_remove(). E.g., iconv() converts &lt;U+0093&gt; to &lt;93&gt; which you can remove with regex \"\\\\&lt;[:alnum]+\\\\&gt;]\".1 There are also some reviews in other languages that I’ll just drop. And some hotel names are pretty long, so I’ll abbreviate them. hotel_1 &lt;- hotel_0 %&gt;% mutate( # Create ASCII bytes review = iconv(review, from = &quot;&quot;, to = &quot;ASCII&quot;, sub = &quot;byte&quot;), # Remove &lt;..&gt; review = str_remove_all(review, &quot;\\\\&lt;[[:alnum:]]+\\\\&gt;&quot;), # Remove &lt;U+....&gt; review = str_remove_all(review, &quot;\\\\&lt;U\\\\+[[:alnum:]]{4}\\\\&gt;&quot;), # Lots of pipes? review = str_remove_all(review, &quot;(\\\\|)&quot;), review = str_squish(review), # Shorten some of the hotel names. property = factor(str_remove_all( property, &quot;( - .*)|(, .*)|( Hotel)|( London)|(The )|( at .*)|( Hyde .*)|( Knights.*)&quot; )), # Interesting metadata chr_cnt = str_length(review) ) %&gt;% # Exclude reviews written in a foreign language. One heuristic to handle this # is to look for words common in other languages that do not also occur in English. filter( !str_detect(review, &quot;( das )|( der )|( und )|( en )&quot;), # German !str_detect(review, &quot;( et )|( de )|( le )|( les )&quot;), # French !str_detect(review, &quot;( di )|( e )|( la )&quot;), # Italian !str_detect(review, &quot;( un )|( y )&quot;), # Spanish str_length(review) &gt; 0 ) That might be enough. Let’s explore the data. # 90% of reviews rate the property a 4 or 5. hotel_1 %&gt;% janitor::tabyl(rating) ## rating n percent ## 1 498 0.02097990 ## 2 587 0.02472933 ## 3 1378 0.05805283 ## 4 4852 0.20440662 ## 5 16422 0.69183132 # Some reviews are as small as 1 character, but they can get quite large. hotel_1 %&gt;% select(chr_cnt) %&gt;% summary() ## chr_cnt ## Min. : 1.0 ## 1st Qu.: 306.0 ## Median : 512.0 ## Mean : 710.6 ## 3rd Qu.: 866.0 ## Max. :30498.0 # A few reviews hotel_1 %&gt;% sample_n(3, seed = 12345) %&gt;% select(chr_cnt, review) %&gt;% flextable::flextable() %&gt;% flextable::valign(j = 1, valign = &quot;top&quot;) %&gt;% flextable::autofit() .cl-675d2482{}.cl-67566d9a{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-67566dc2{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-6759029e{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-675902b2{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-675902b3{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-67591432{width:0.787in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-6759143c{width:108.859in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-67591446{width:0.787in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-67591447{width:108.859in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-67591448{width:0.787in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-67591450{width:108.859in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-67591451{width:0.787in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-6759145a{width:108.859in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}chr_cntreview553True British charm, and fully modern amenities -- closets you can actually walk into, spacious bathrooms with full sized tubs, rooms the size of, well, traditional American hotel rooms. The hotel has an almost secret outdoor (but covered) courtyard, that might be the last place in Britain you can smoke a cigar. The breakfast buffet is amazing, and the food service in both restaurants is excellent. Location in Charing Cross, right at the emergingly hip Embankment, an excellent place. I have stayed here many times, and find it consistently wonderful1,822Our trip to the Apex London Wall was absolutely fantastic! The room was very clean, the bed was probably the most comfortable hotel bed I've ever slept in, and the bathroom... Oh my word, it was amazing!! This hotel is literally (we timed it) 7 minutes from the time you step foot off the tube in Moorgate station to the moment you walk into the front door of the hotel, and we were walking kinda slow. It's a little hidden... Make sure to look up directions beforehand, but once you know where it is, it's so easy to find. Customer service was great; our adapter plug didn't work so they let us borrow one of their's. There aren't a ton of restaurants nearby (with the exception of the very fancy, slightly expensive hotel restaurant -- which was very nice, but since I had a 12-year-old with me... it was just too fancy for us), but there is a McDonalds right around the corner and two Starbucks', one that is right across the street. The area is very, very nice. It's in the business district, I think. Everyone was in a rush and all dressed up, but if you're lost (long story short, we got extrememly lost when we first got there), people are willing to help. If I had to \"ding\" them for one thing, it would be room checks at 11am. We're from the U.S. and we never fully adjusted to London time, so we slept in most days. Every day at around 11am, we got a knock on the door and a \"hello?\" from a staff member letting us know they were there to check our mini bar. At most, it was an annoyance. Really not a huge deal at all, especially if you're not going to sleep in as much as we did. Oh! And it's really not far from all the sites. I think it only took us maybe 12-15 minutes to get to all the main stuff. This is probably a once-in-a-lifetime trip for me, but if I ever go back again... This is where I'm staying.339The Rembrandt hotel was fully renovated and is now a most lovely and convenient place to stay in London \"away from the madding crowds\"... (standard) rooms are small but very clean and comfortable, very friendly and most helpful staff, excellent breakfast and a prime location in REAL London - close to museums, parks and easy to get around Nagelkerke (2020a) recommends removing punctuation to focus on the entire text rather than the sentences within. Nagelkerke also suggests removing very short (&lt;= 3 chars) for anything other than sentiment analysis. I’m going to keep punctuation and short reviews for now. Some of those extremely short reviews are gibberish, but tokenizing will filter out some of that. References "],["tokenize.html", "1.2 Tokenize", " 1.2 Tokenize Tokenize the reviews. Even if you want bigrams, it is often helpful to tokenize into unigrams first to clean and regularize. token_0 &lt;- hotel_1 %&gt;% select(review_id, review) %&gt;% unnest_tokens(&quot;word&quot;, review) %&gt;% mutate(.by = review_id, n = n()) %&gt;% # Short reviews are mostly gibberish. Require at least 10 words. filter(n &gt;= 10) # Attach word counts back to main data frame. Inner join side effect is removal # of some bogus reviews. hotel_2 &lt;- token_0 %&gt;% count(review_id, name = &quot;word_cnt&quot;) %&gt;% inner_join(hotel_1, by = &quot;review_id&quot;) %&gt;% relocate(word_cnt, .after = chr_cnt) hotel_2 %&gt;% select(chr_cnt, word_cnt) %&gt;% summary() ## chr_cnt word_cnt ## Min. : 51.0 Min. : 10.0 ## 1st Qu.: 314.0 1st Qu.: 56.0 ## Median : 520.0 Median : 93.0 ## Mean : 721.1 Mean : 130.9 ## 3rd Qu.: 875.0 3rd Qu.: 159.0 ## Max. :30498.0 Max. :5712.0 Nagelkerke (2020a) recommends discarding reviews with few (&lt;=50) tokens on the grounds that short reviews will not add much in terms of different topics within reviews.2. That’s about 25% of this sample. I set a lower limit of 10 tokens. References "],["spell-check.html", "1.3 Spell-check", " 1.3 Spell-check Run a spell-check to regularize the data. Its possible to land on the wrong correction, but there is probably more to gain than lose. Only a very small fraction of the tokens were misspellings. # There are multiple possible right spellings, so just choose one. spell_check &lt;- fuzzyjoin::misspellings %&gt;% distinct(misspelling, .keep_all = TRUE) token_1 &lt;- token_0 %&gt;% left_join(spell_check, by = join_by(word == misspelling)) %&gt;% mutate(word = coalesce(correct, word)) %&gt;% select(-correct) # Only .09% of words were misspelled. mean(token_0$word != token_1$word) ## [1] 0.0009311988 # Examples. tibble(before = token_0$word, after = token_1$word) %&gt;% filter(before != after) %&gt;% head() ## # A tibble: 6 × 2 ## before after ## &lt;chr&gt; &lt;chr&gt; ## 1 resturant restaurant ## 2 excellance excellence ## 3 reccomend recommend ## 4 reccommend recommend ## 5 accomodating accommodating ## 6 ther there "],["remove-stop-words.html", "1.4 Remove Stop Words", " 1.4 Remove Stop Words Stop words usually add no value, but you should pay attention to what you are dropping. Be ready to add pertinent words back and perhaps drop others. # Start with a standard list. stop &lt;- tidytext::stop_words %&gt;% # Don&#39;t remove potentially useful words. filter(!word %in% c(&quot;appreciate&quot;, &quot;room&quot;)) %&gt;% # But also exclude these words. bind_rows(tibble(word = c(&quot;hotel&quot;))) token_2 &lt;- token_1 %&gt;% anti_join(stop, by = &quot;word&quot;) %&gt;% # also exclude words that are, or contain, numbers. filter(!str_detect(word, &quot;[0-9]&quot;)) # Most frequently removed words token_1 %&gt;% anti_join(token_2, by = join_by(review_id, word)) %&gt;% count(word, sort = TRUE) ## # A tibble: 2,490 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 the 196439 ## 2 and 117631 ## 3 a 89194 ## 4 to 78025 ## 5 was 64152 ## 6 in 54398 ## 7 we 47724 ## 8 of 44773 ## 9 i 43742 ## 10 for 38260 ## # ℹ 2,480 more rows "],["lemmatize.html", "1.5 Lemmatize", " 1.5 Lemmatize Stemming and lemmatizing are ways to convert word variations like “staying”, “stayed”, and “stay” into a generic form: “stay”. Stemming tends to chop off endings to create a root word, but the stem is often not a word itself. E.g., “staying” becomes “stai”. Lemmatize gives you the more natural “stay”. token &lt;- token_2 %&gt;% mutate(word = textstem::lemmatize_words(word)) # Examples. tibble(before = token_2$word, after = token$word) %&gt;% filter(before != after) %&gt;% count(before, after, sort = TRUE) ## # A tibble: 8,054 × 3 ## before after n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 stayed stay 8785 ## 2 hotels hotel 4169 ## 3 amazing amaze 4017 ## 4 booked book 3041 ## 5 nights night 2948 ## 6 arrived arrive 2517 ## 7 bit bite 2473 ## 8 restaurants restaurant 2398 ## 9 staying stay 2313 ## 10 minutes minute 2180 ## # ℹ 8,044 more rows # A typical review, before and after bind_cols( review_id = 2:4, before = hotel_2 %&gt;% filter(review_id %in% 2:4) %&gt;% pull(review), after = token %&gt;% filter(review_id %in% 2:4) %&gt;% summarize(.by = review_id, x = paste(word, collapse = &quot; &quot;)) %&gt;% pull(x) ) %&gt;% flextable::flextable() %&gt;% flextable::valign(valign = &quot;top&quot;) %&gt;% flextable::autofit() .cl-6a29adca{}.cl-6a215a9e{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-6a215aa8{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-6a262376{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-6a262380{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-6a262381{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-6a262382{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-6a2631ea{width:0.918in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-6a2631f4{width:87.136in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-6a2631f5{width:42.66in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-6a2631fe{width:0.918in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-6a2631ff{width:87.136in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-6a263200{width:42.66in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-6a263208{width:0.918in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-6a263209{width:87.136in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-6a263212{width:42.66in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-6a263213{width:0.918in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-6a263214{width:87.136in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-6a26321c{width:42.66in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}review_idbeforeafter2I had a pleasure of staying in this hotel for 7 nights recently. This hotel was perfect in every way. Communication with the hotel before staying was prompt, and very efficient. Checking in was a breeze. You go through the spectacular lobby with modern glass chandeliers and take the elevator to your room. My room, they gave me an upgrade to junior suite, was spectacular. We had a walk-in closet of the size where you could have put a small bed in there; it served us nicely for the seven day stay. The decor was very refined, and oh the bathroom! Carrera marble floor was heated throughout, rain shower was to die for! Location, as it turned out, was as good as it can be. We were 5 minutes walk to Trafalgar Square, but it was very quiet. Right outside was Embankment tube stop. We would walk to theater area and to numerous restaurants, and many major sites, such as London Eye or Westminster Abbey were within walking distance. We had buffet breakfast or room service every morning. It was pricy, but my rate included it. Couple of nights, we had glass of wine sitting in front of fire place in the lobby. I used the spa, which is included in the room rate, almost every night. After a windchill day of sightseeing, the steam sauna and jacuzzi would soften my weary muscles. I have stayed in many 5 star hotels around the world, but this hotel tops it. I would return here in a heartbeat next time I am in London.pleasure stay night recently perfect communication stay prompt efficient check breeze spectacular lobby modern glass chandelier elevator room room upgrade junior suite spectacular walk closet size bed serve nicely day stay decor refine bathroom carrera marble floor heat rain shower die location minute walk trafalgar square quiet embankment tube stop walk theater numerous restaurant major site london eye westminster abbey walk distance buffet breakfast room service morning pricy rate include couple night glass wine sit front fire lobby spa include room rate night windchill day sightsee steam sauna jacuzzi soften weary muscle stay star hotel world top return heartbeat time london3A very lovely first visit to this iconic hotel bar! Wonderful service, without being intrusive at all! Very delicious cocktails and just generally all round, a very indulgent experience. Well worth visiting only for that 'once in a lifetime' experience, though do make sure you are feeling 'flush' it doesn't come cheap!lovely visit iconic bar wonderful service intrusive delicious cocktail round indulgent experience worth visit lifetime experience feel flush cheap43 of us stayed at the Rhodes Hotel for 4 nights, its a great location for taking the Paddington Express from Heathrow. We like the location clost to the partk and in walking distance of most locations. The room and bath were small compared to American Hotels but very clean. We enjoyed the free WIFI. The owners and the staff were very friendly and helpful with taxi's and resturant recomendations. We would stay there again.stay rhodes night location take paddington express heathrow location clost partk walk distance location room bath compare american hotel clean enjoy free wifi owner staff friendly helpful taxi's restaurant recomendations stay At this point, you might decide to throw out smaller reviews because they are unlikely to identify multiple topics (Gils 2020). References "],["bigrams.html", "1.6 Bigrams", " 1.6 Bigrams Bigrams should not contain stop words. However, they should be adjacent words, so tokenize into bigrams, split into words an then filter out rows where one or both words is stop word. bigram_0 &lt;- # start with the tokenized data _prior_ to removing stop words. token_2 %&gt;% summarize(.by = review_id, reconstructed = paste(word, collapse = &quot; &quot;)) %&gt;% unnest_tokens(&quot;bigram&quot;, reconstructed, token = &quot;ngrams&quot;, n = 2) # Remove bigrams where one or both words are stop words. bigram &lt;- bigram_0 %&gt;% separate(bigram, into = c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% anti_join(stop, by = join_by(word1 == word)) %&gt;% anti_join(stop, by = join_by(word2 == word)) %&gt;% filter(!str_detect(word1, &quot;[0-9]&quot;), !str_detect(word2, &quot;[0-9]&quot;)) %&gt;% mutate(bigram = paste(word1, word2)) %&gt;% select(review_id, bigram) # Example bind_cols( hotel_2 %&gt;% filter(review_id == 3) %&gt;% select(review), bigram %&gt;% filter(review_id == 3) %&gt;% summarize(bigrams = paste(bigram, collapse = &quot;\\n&quot;)) ) %&gt;% flextable::flextable() %&gt;% flextable::autofit() %&gt;% flextable::width(j = 1, width = 4.5, unit = &quot;in&quot;) %&gt;% flextable::width(j = 2, width = 1.5, unit = &quot;in&quot;) %&gt;% flextable::valign(valign = &quot;top&quot;) .cl-7140e06a{}.cl-713b2102{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-713b210c{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-713d6fe8{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-713d6ff2{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-713d7c86{width:4.5in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-713d7c90{width:1.5in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-713d7c91{width:4.5in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-713d7c9a{width:1.5in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}reviewbigramsA very lovely first visit to this iconic hotel bar! Wonderful service, without being intrusive at all! Very delicious cocktails and just generally all round, a very indulgent experience. Well worth visiting only for that 'once in a lifetime' experience, though do make sure you are feeling 'flush' it doesn't come cheap!lovely visitvisit iconiciconic barbar wonderfulwonderful serviceservice intrusiveintrusive deliciousdelicious cocktailscocktails roundround indulgentindulgent experienceexperience worthworth visitingvisiting lifetimelifetime experienceexperience feelingfeeling flushflush cheap "],["save.html", "1.7 Save", " 1.7 Save Combine the tokens and bigrams into cleaner reviews. Save the cleaned data for other analyses like topic modeling and sentiment analysis. review_words &lt;- token %&gt;% summarize(.by = review_id, review_words = paste(word, collapse = &quot; &quot;)) review_bigrams &lt;- bigram %&gt;% summarize(.by = review_id, review_bigrams = paste(bigram, collapse = &quot; &quot;)) hotel_prep &lt;- hotel_2 %&gt;% inner_join(review_words, by = join_by(review_id)) %&gt;% inner_join(review_bigrams, by = join_by(review_id)) save(hotel_prep, token, bigram, file = &quot;input/hotel_prepped.Rdata&quot;) "],["topicmodeling.html", "Chapter 2 Topic Modeling", " Chapter 2 Topic Modeling Topic models are unsupervised ML models that identify topics as clusters of words with an associated probability distribution, and a probability distribution of topics within each document. There are two commonly used models: LDA and STM. LDA, implemented in the topicmodels package, is the simpler model. STM, implemented in the stm package, incorporates document metadata into the model. Both topic models are generative models of word counts. That means they assume there is some process that generates text which is a mixture of topics composed of words which occur with varying probabilities. Think of the observed text document as the product of an algorithm that selected each word in two stages: 1) it sampled a topic from a probability distribution, then 2) it sampled a word from the topic’s word probability distribution. The object in topic modeling is to tune the hyperparameters that define those probability distributions. In a way, topic models do the opposite of what you might expect. They are not estimating the probability that each document is one of those topics. They assume all topics contribute to each document and instead estimate their relative contributions. More concisely, the models treat documents as a mixture of topics, and the topics as a mixture of words, and both the topics and topic words have probability distributions. The sum of topic proportions in document is one; the sum of word probabilities in a topic is one. This leads to two frameworks for thinking about topics. A topic’s prevalance measures the proportion of the document generated by it. The topic’s content is the probability distribution of words associated with it. LDA and STM differ only in how they handle these frameworks. STM defines covariates associated with prevalence and content while sTM does not.3 This section continues with the hotel data from Chapter 1 and follows the ideas from Nagelkerke (2020b) and the stm package vignette package. library(tidyverse) library(tidymodels) library(topicmodels) library(tidytext) library(stm) # library(tm) # generic text mining utilities. Great for DTMs. library(scales) library(glue) load(&quot;input/hotel_prepped.Rdata&quot;) glimpse(token) ## Rows: 1,112,307 ## Columns: 3 ## $ review_id &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … ## $ word &lt;chr&gt; &quot;pleasure&quot;, &quot;stay&quot;, &quot;night&quot;, &quot;recently&quot;, &quot;perfect&quot;, &quot;communi… ## $ n &lt;int&gt; 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, … References "],["lda.html", "2.1 LDA", " 2.1 LDA Latent Dirichlet allocation (LDA) is an instance of a general family of mixed membership models that decompose data into latent components. Latent refers to unidentified topics and Dirichlet refers to the type of distribution followed by the words in the the topics and by the topics in the documents. Algorithm LDA assumes each document is created by a generative process where topics are included according to probabilities and words are included in the topics according to probabilities. The LDA algorithm determines what those probabilities are. The algorithm is: For each document \\(d_i\\), randomly assign each word \\(w_j\\) to one of the \\(K\\) topics. Note that \\(w_j\\) may be assigned to a different topic in different documents. For each document, tabulate the number of words in each topic, a \\(d \\times K\\) matrix. For each word, tabulate the sum of occurrences across all documents, a \\(w \\times K\\) matrix. Resample a single instance of a word from the corpus and remove it from the analysis, decrementing the document’s topic count and the word’s topic count. Calculate the gamma matrix, \\(\\gamma\\), and the beta matrix, \\(\\beta\\). the gamma matrix, aka the topical prevalence, is the probability distribution of topics for each document, \\[p(t_k|d_i) = \\frac{n_{ik} + \\alpha}{N_i + K \\alpha}\\] were \\(n_{ik}\\) is the number of words in document \\(i\\) for topic \\(k\\), \\(N_i\\) is the total number of words in \\(i\\), and \\(\\alpha\\) is a hyperparameter. For each \\(d_i\\), \\(\\sum_{k \\in K} \\gamma_{ik} = 1\\). the beta matrix, aka the topical content, is the probability distribution of words for each topic, \\[p(w_j|t_k) = \\frac{m_{j,k} + \\beta}{\\sum_{j \\in V}m_{j,k} + V\\beta}\\] where \\(m_{j,k}\\) is the corpus-wide frequency count of word \\(w_j\\) to topic \\(k\\), \\(V\\) is the number of distinct words in the corpus, and \\(\\beta\\) is a hyperparameter. For each \\(t_k\\), \\(\\sum_{j \\in V} \\beta_{kj} = 1\\). Perform Gibbs sampling. Calculate the joint probability distribution of words for each document and topic, \\(p(w_j|t_k,d_i) = p(t_k|d_i)p(w_j|t_k)\\). Assign each word, \\(w_j\\), to the topic with the maximum joint probability. Repeat steps 3-6 for all of the words in all of the documents. Repeat steps 3-7 for a pre-determined number of iterations. LDA thus has 3 hyperparameters: the document-topic density factor, \\(\\alpha\\), the topic-word density factor, \\(\\beta\\), and the topic count, \\(K\\). \\(\\alpha\\) controls the number of topics expected per document (large \\(\\alpha\\) = more topics). \\(\\beta\\) controls the distribution of words per topic (large \\(\\beta\\) = more words). Ideally, you want a few topics per document and a few words per topic, so \\(\\alpha\\) and \\(\\beta\\) are typically set below one. \\(K\\) is set using a combination of domain knowledge, coherence, and exclusivity. Notice that LDA is a “bag of words” method. It does not consider the order of the tokens in the text, so where tokens are located what other tokens are nearby do not factor into the output. Data Preparation In addition to the cleaned text produced in Chapter 1, there are a few more data preparation tasks for LDA. Create a bag of words from the union of the word and bigram reviews to get all terms. Keep only the decent sized reviews (&gt;= 25 words). lda_dat &lt;- hotel_prep %&gt;% mutate(combined = paste(review_words, review_bigrams)) %&gt;% select(review_id, combined) %&gt;% unnest_tokens(output = &quot;word&quot;, input = combined) %&gt;% mutate(.by = review_id, n = n()) %&gt;% filter(n &gt;= 25) %&gt;% select(-n) lda_dat %&gt;% glimpse() ## Rows: 3,288,371 ## Columns: 2 ## $ review_id &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … ## $ word &lt;chr&gt; &quot;pleasure&quot;, &quot;stay&quot;, &quot;night&quot;, &quot;recently&quot;, &quot;perfect&quot;, &quot;communi… The next step is optional. If this is a predictive model, create a train/test split. You might even weight the splitting by rating (if that is the outcome variable) to ensure proportional coverage. hotel_split &lt;- rsample::initial_split(hotel_prep, prop = 3/4, strata = review_id) lda_train_0 &lt;- lda_dat %&gt;% inner_join(training(hotel_split) %&gt;% select(review_id, rating), by = join_by(review_id)) lda_test &lt;- lda_dat %&gt;% inner_join(testing(hotel_split) %&gt;% select(review_id, rating), by = join_by(review_id)) glimpse(lda_train_0) ## Rows: 2,478,759 ## Columns: 3 ## $ review_id &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ word &lt;chr&gt; &quot;lovely&quot;, &quot;visit&quot;, &quot;iconic&quot;, &quot;bar&quot;, &quot;wonderful&quot;, &quot;service&quot;, … ## $ rating &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, … glimpse(lda_test) ## Rows: 809,612 ## Columns: 3 ## $ review_id &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … ## $ word &lt;chr&gt; &quot;pleasure&quot;, &quot;stay&quot;, &quot;night&quot;, &quot;recently&quot;, &quot;perfect&quot;, &quot;communi… ## $ rating &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, … Low frequency tokens impact the topic model analysis. 58% of the data is composed of tokens that appear &lt;=3 times. Just use the high frequency tokens, the ones occurring at least 4 times in the training data. This is (hopefully) enough to learn topics from tokens that occur together in reviews. Nagelkerke (2020b) explains that TF-IDF, which combines the within-document token frequency and document frequency, is not always the best way to whittle down the token set. When documents are small, which is common in online reviews, the within document frequency is low and the IDF part is over-weighted. lda_train_0 %&gt;% count(word, name = &quot;token_n&quot;) %&gt;% mutate(token_n = if_else(token_n &gt; 20, 20, token_n)) %&gt;% count(token_n) %&gt;% mutate(pct = n / sum(n), cum_pct = cumsum(pct)) %&gt;% ggplot(aes(x = token_n)) + geom_col(aes(y = pct)) + geom_line(aes(y = cum_pct)) + scale_y_continuous(labels = percent_format(1)) + labs(x = &quot;Token Frequency&quot;, y = &quot;Pct of Tokens&quot;, title = &quot;Token Frequency&quot;) lda_train &lt;- lda_train_0 %&gt;% mutate(.by = word, corpus_token_n = n()) %&gt;% mutate(.by = c(review_id, word), review_token_n = n()) %&gt;% filter(corpus_token_n &gt;= 4) bind_rows( `high freq words` = lda_train %&gt;% summarize(total_words = n(), distinct_words = n_distinct(word)), `low freq words` = anti_join(lda_train_0, lda_train, by = join_by(review_id, word)) %&gt;% summarize(total_words = n(), distinct_words = n_distinct(word)), .id = &quot;partition&quot; ) %&gt;% mutate(total_pct = total_words / sum(total_words) * 100, distinct_pct = distinct_words / sum(distinct_words) * 100) %&gt;% select(partition, total_words, total_pct, distinct_words, distinct_pct) ## # A tibble: 2 × 5 ## partition total_words total_pct distinct_words distinct_pct ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 high freq words 2438254 98.4 15745 51.2 ## 2 low freq words 40505 1.63 15032 48.8 Now we can create a document term matrix (DTM). Again from Nagelkerke (2020b), you might expect the document specific term frequency (review_token_n), however I use the overall token frequency corpus_token_n to give more emphasis to terms that are more frequent in general. Reviews are short compared to books or articles, so the probability of a token occurring repeatedly in a review is low. lda_dtm &lt;- cast_dtm(lda_train, document = review_id, term = word, value = corpus_token_n) dim(lda_dtm) ## [1] 17458 15745 # Average token frequency. sum(lda_dtm) / sum(lda_dtm != 0) ## [1] 5023.017 Fit There are several parameters you might tweak for the model fit. The biggest surprise is that you set the number of topics, k. This article explains the harmonic mean method for optimization, but Nagelkerke (2020b) suggests sticking with the art vs science method and pick your own k. The model fit took about 3 minutes to run. I ran it once then saved the result. lda_fit &lt;- LDA(lda_dtm, k = 3) saveRDS(lda_fit, file = &quot;input/lda_fit.RDS&quot;) lda_fit &lt;- readRDS(file = &quot;input/lda_fit.RDS&quot;) The fitted object contains two matrices. The phi matrix is the distribution of tokens (cols) over topics (rows). The theta matrix is the distribution of documents (rows) over topics (cols). The row sum is 1 for each matrix (sum of topic probabilities, some of document probabilities). In each case, the values are probabilities that sum to 1 for each topic. lda_phi &lt;- posterior(lda_fit) %&gt;% pluck(&quot;terms&quot;) %&gt;% as.matrix() dim(lda_phi) ## [1] 3 9681 sum(lda_phi[1, ]) ## [1] 1 lda_theta &lt;- posterior(lda_fit) %&gt;% pluck(&quot;topics&quot;) %&gt;% as.matrix() dim(lda_theta) ## [1] 15671 3 sum(lda_theta[1, ]) ## [1] 1 The tidytext::tidy() function calculates the beta matrix then pivots-longer into a [topic, term, beta] data frame. lda_tidy &lt;- tidy(lda_fit) lda_top_tokens &lt;- lda_tidy %&gt;% mutate(topic = factor(paste(&quot;Topic&quot;, topic))) %&gt;% group_by(topic) %&gt;% slice_max(order_by = beta, n = 10) %&gt;% ungroup() lda_top_tokens %&gt;% ggplot(aes(x = beta, y = reorder_within(term, by = beta, within = topic))) + geom_col() + scale_y_reordered() + facet_wrap(facets = vars(topic), scales = &quot;free_y&quot;) + labs(y = NULL, title = &quot;LDA Top 10 Terms&quot;) There is a downside to this evaluation. Popular words like room appear at or near the top in all three topics. You might want to look at relative popularity instead: the popularity within the topic divided by overall popularity. That’s problematic too because words that only appear in few reviews will pop to the top. What you want is a combination of both absolute term probability and relative term probability. LDAvis::serVis() can help you do that. Unfortunately, the plot from LDAvis::serVis() is interactive and does not render in the RMarkdown notebook html, so below is just a screenshot of the code chunk output. The left side shows the topic sizes (documents) and topic distances. The right side shows the most important tokens. doc_length &lt;- lda_train %&gt;% count(review_id) %&gt;% pull(n) # vocabulary: unique tokens vocab &lt;- colnames(lda_phi) # overall token frequency term_frequency &lt;- lda_train %&gt;% count(word) %&gt;% arrange(match(word, vocab)) %&gt;% pull(n) # create JSON containing all needed elements json &lt;- LDAvis::createJSON(lda_phi, lda_theta, doc_length, vocab, term_frequency) LDAvis::serVis(json) Iterate through the model by tweaking k, excluding words that dominate and suppress more interesting subdomains, and/or changing the minimal token frequency to focus on more/less dominant tokens. You can also change the document sampling strategy to promote interesting domains, like we did when we over sampled the low hotel ratings. Topic Labeling with ChatGPT To OpenAI’s ChatGPT API service requires an API token. I created one at https://platform.openai.com/api-keys and saved it to .Renviron. See usethis::edit_r_environ(). library(httr2) library(jsonlite) # Create a function to send each list of topic words to Open AI as a separate request. get_topic_from_openai &lt;- function(prompt) { my_resp &lt;- request(&quot;https://api.openai.com/v1/chat/completions&quot;) %&gt;% req_headers(Authorization = paste(&quot;Bearer&quot;, Sys.getenv(&quot;OPENAI_API_KEY&quot;))) %&gt;% req_body_json(list( model = &quot;gpt-3.5-turbo&quot;, # Temperature [0,2] controls creativity, predictable -&gt; variable. temperature = 1, messages = list( # System prompt sets repeated context. It is prefixed to prompts. list( role = &quot;system&quot;, content = paste(&quot;You are a topic modeling assistant. You accept lists &quot;, &quot;of words in a topic and summarizes them into a salient &quot;, &quot;topic label of five words or less. How would you &quot;, &quot;summarize the following list? Return just the topic &quot;, &quot;label and nothing else.&quot;) ), list( role = &quot;user&quot;, content = prompt ) ) )) %&gt;% req_perform() %&gt;% resp_body_json() %&gt;% pluck(&quot;choices&quot;, 1, &quot;message&quot;, &quot;content&quot;) } lda_topics &lt;- lda_top_tokens %&gt;% nest(data = term, .by = topic) %&gt;% mutate( token_str = map(data, ~paste(.$term, collapse = &quot;, &quot;)), topic_lbl = map_chr(token_str, get_topic_from_openai), topic_lbl = str_remove_all(topic_lbl, &#39;\\\\&quot;&#39;), topic_lbl = snakecase::to_any_case(topic_lbl, &quot;title&quot;) ) %&gt;% select(-data) # Save to file system to avoid regenerating. saveRDS(lda_topics, file = &quot;input/lda_topics.RDS&quot;) lda_topics &lt;- readRDS(file = &quot;input/lda_topics.RDS&quot;) Let’s see the topic summary with the newly generated labels. lda_top_tokens %&gt;% inner_join(lda_topics, by = join_by(topic)) %&gt;% mutate(topic_lbl = str_wrap(topic_lbl, 25)) %&gt;% ggplot(aes(x = beta, y = reorder_within(term, by = beta, within = topic_lbl))) + geom_col() + scale_y_reordered() + facet_wrap(facets = vars(topic_lbl), scales = &quot;free_y&quot;) + labs(y = NULL, title = &quot;LDA Top 10 Terms&quot;) TODO I still need to learn more about Held-out Likelihood (Wallach et al., 2009). Semantic Coherence Exclusivity. Generally, the greater the number of topics in a model, the lower the quality of the smallest topics. One way around this is simply hiding the low-quality topics. The coherence measure evaluates topics. References "],["stm.html", "2.2 STM", " 2.2 STM STM incorporates arbitrary document metadata into the topic model. The goal of STM is to discover topics and estimate their relationship to the metadata. If how a topic is discussed depends on the metadata features, control for them in the topical content part of the model (the beta matrix). E.g., children and adults will discuss hotels differently. If what topics are discussed depends on the metadata features, control for them in the topical prevalence model (the gamma matrix). E.g., negative hotel reviews might focus on different topics than positive reviews. Algorithm STM is similar to LDA in that it assumes each document is created by a generative process where topics are included according to probabilities (topical prevalence) and words are included in the topics (topical content) according to probabilities. STM adds the possibility of including topical prevalence covariates, and topical content covariates. Data Preparation Chapter 1 prepped the data by correcting misspellings, lemmatizing words, and removing stop words The stm package represents a text corpus as an object with three components: a sparse matrix of counts by document and vocabulary word vector index, the vocabulary word vector, and document metadata. I used STM for my Battle of the Bands project. stm::textProcessor() is essentially a wrapper around the tm package. It produces a list object with three main components: vocab, a named vocabulary vector, one element per distinct word. documents, a list of matrices, one per document. Each matrix has 2 rows of integers. The first row is indices from the vocabulary vector; the second is their associated word counts. This is a concise representation of a document term matrix. The processing step sometimes removes a few documents if they are empty after removing stopwords, numbers, est. meta, a metadata data frame, one row per document containing the feature cols. The model fit took about 3 minutes to run. I ran it once then saved the result. stm_processed &lt;- stm::textProcessor( documents = hotel_prep$review_words, metadata = hotel_prep %&gt;% select(rating, property), lowercase = FALSE, removestopwords = FALSE, removenumbers = FALSE, removepunctuation = FALSE, stem = FALSE ) saveRDS(stm_processed, file = &quot;input/stm_processed.RDS&quot;) stm_processed &lt;- readRDS(file = &quot;input/stm_processed.RDS&quot;) After processing, prepare the corpus by removing infrequently used words. stm::prepDocuments() removes infrequently appearing words, and removes any documents that contain no words after processing and removing words. 1% (about 230) is a conservative threshold. The plot below shows that removing even a few words will remove some documents, but you can still retain most document plotRemoved(stm_processed$documents, lower.thresh = seq(100, 4000, by = 100)) stm_prepared &lt;- stm::prepDocuments( stm_processed$documents, stm_processed$vocab, stm_processed$meta, lower.thresh = length(stm_processed$documents) * .01 ) ## Removing 26136 of 26886 terms (239369 of 943763 tokens) due to frequency ## Removing 21 Documents with No Words ## Your corpus now has 23352 documents, 750 terms and 704394 tokens. Fit The stm package allows you to either specify the number of topics (K) to identify, or it can choose an optimal number by setting parameter K = 0. The resulting probability distribution of topic words (beta matrix) will be a K x rlength(stm_prepared$vocab)matrix. The probability distribution of topics (gamma matrix, theta in the stm package) will be a 23,352 x K matrix. I expect topics to correlate with the review rating, soratingis a prevalence covariate, and I expect word usage to correlate with the hotel, soproperty` is a topical content covariate. set.seed(1234) stm_fitted &lt;- stm::stm( stm_prepared$documents, stm_prepared$vocab, K = 4, prevalence = ~ rating, content = ~ property, data = stm_prepared$meta, init.type = &quot;Spectral&quot;, verbose = FALSE ) saveRDS(stm_fitted, file = &quot;input/stm_fitted.RDS&quot;) stm_fitted &lt;- readRDS(file = &quot;input/stm_fitted.RDS&quot;) summary(stm_fitted) ## A topic model with 4 topics, 23352 documents and a 750 word dictionary. ## Topic Words: ## Topic 1: pleasure, home, hospitality, professional, outstanding, accommodation, impeccable ## Topic 2: park, min, shop, locate, ride, train, distance ## Topic 3: tea, lunch, wine, sweet, tasty, bread, delicious ## Topic 4: curtain, carpet, low, window, toilet, poor, mirror ## ## Covariate Words: ## Group 45 Park Lane: lane, cut, dorchester, deco, boutique, host, screen ## Group A To Z: cheap, continental, transport, owner, basic, toilet, cup ## Group Apex Wall: apex, district, bean, office, friday, city, balcony ## Group Bulgari: swim, pool, knightsbridge, italian, dark, massage, treatment ## Group City View: basic, cereal, fridge, cup, machine, roll, toilet ## Group Corinthia: corinthia, sauna, courtyard, flower, massage, rain, spa ## Group Dorchester: promenade, dorchester, grill, flower, famous, waiter, ritz ## Group Hartley: smell, budget, smoke, carpet, sink, chair, true ## Group Hotel Xenia: xenia, kensington, italian, twin, toiletry, balcony, spotlessly ## Group Lanesborough: butler, piano, magnificent, lounge, garden, spa, traditional ## Group London Guest House: continental, adequate, fridge, mile, clean, transport, budget ## Group Mandarin Oriental: mandarin, oriental, renovation, spa, renovate, pool, junior ## Group Marble Arch: executive, tasty, box, oxford, stair, sight, complain ## Group Mondrian: mondrian, river, paul&#39;s, container, sea, thames, rooftop ## Group Newham: girl, budget, bad, reasonable, corridor, furniture, buffet ## Group Rembrandt: rembrandt, jacuzzi, swim, club, museum, harrods, pool ## Group Rhodes: rhodes, paddington, jacuzzi, continental, fridge, stair, carry ## Group Ridgemount: ridgemount, owner, share, basic, bus, advice, budget ## Group Savoy: savoy, beaufort, thames, river, grill, american, deco ## Group Wellesley: butler, roll, deco, knightsbridge, terrace, boutique, lounge ## ## Topic-Covariate Interactions: ## Topic 1, Group 45 Park Lane: park, victoria, private, balcony, perfection, cut, interior ## Topic 1, Group A To Z: budget, clean, accommodation ## Topic 1, Group Apex Wall: wall, apex, court, district, taxi, pool, equip ## Topic 1, Group Bulgari: sauna, amp, treatment, spa, word, luxury, perfection ## Topic 1, Group City View: ## Topic 1, Group Corinthia: gym, spa, corinthia, wait, treatment, executive, mandarin ## Topic 1, Group Dorchester: theatre, christmas, talk, establishment, perfection, life, magnificent ## Topic 1, Group Hartley: ## Topic 1, Group Hotel Xenia: boutique, west, underground, xenia, clean, concierge, balcony ## Topic 1, Group Lanesborough: renovation, renovate, refurbish, easy, garden, greet, club ## Topic 1, Group London Guest House: house, guest, continental, safe, owner, breakfast, spotlessly ## Topic 1, Group Mandarin Oriental: oriental, classic, mandarin, complete, property, pool, type ## Topic 1, Group Marble Arch: gym ## Topic 1, Group Mondrian: balcony, view, lounge, roof, cool, lane, cocktail ## Topic 1, Group Newham: cheap, bad, money, family ## Topic 1, Group Rembrandt: attraction, west, theater, shop, nearby, tourist, theatre ## Topic 1, Group Rhodes: read, rhodes, convenient, host, owner, money, run ## Topic 1, Group Ridgemount: morning, fill, phone, bus, run, locate, basic ## Topic 1, Group Savoy: wed, junior, anniversary, surrounding, executive, view, history ## Topic 1, Group Wellesley: slipper, marble, roll, complimentary, touch, car, perfection ## ## Topic 2, Group 45 Park Lane: suite, butler, separate, dorchester, hyde, view, gym ## Topic 2, Group A To Z: towel, wifi, noise, main, train, toilet, line ## Topic 2, Group Apex Wall: moorgate, paul&#39;s, saturday, bank, surrounding, weekend, lunch ## Topic 2, Group Bulgari: harrods, black, marble, oriental, mandarin, boutique, brand ## Topic 2, Group City View: ## Topic 2, Group Corinthia: embankment, trafalgar, square, eye, thames, river, theater ## Topic 2, Group Dorchester: chair, executive, son, american, star, lane, spa ## Topic 2, Group Hartley: clean, free, breakfast ## Topic 2, Group Hotel Xenia: court, albert, natural, museum, history, victoria, hall ## Topic 2, Group Lanesborough: piano, beautifully, gym, corner, luxurious, hyde, excellent ## Topic 2, Group London Guest House: bus, budget, free, paddington, basic, line, stop ## Topic 2, Group Mandarin Oriental: knightsbridge, balcony, magnificent, harrods, hyde, overlook, interior ## Topic 2, Group Marble Arch: marble, oxford, street ## Topic 2, Group Mondrian: blackfriars, tate, bank, eye, bridge, south, promenade ## Topic 2, Group Newham: toast, bean, read, sausage, pay, cereal, iron ## Topic 2, Group Rembrandt: albert, kensington, victoria, south, natural, history, directly ## Topic 2, Group Rhodes: express, heathrow, money, internet, kensington, paddington, hyde ## Topic 2, Group Ridgemount: british, square, court, museum, trafalgar, theater, covent ## Topic 2, Group Savoy: pool, strand, embankment, covent, swim, sauna, christmas ## Topic 2, Group Wellesley: waiter, complimentary, harrods, hyde, junior, heat, corner ## ## Topic 3, Group 45 Park Lane: eye, son, amaze, guy, sunday, arrange, cocktail ## Topic 3, Group A To Z: toast, coffee, type, juice, amp, breakfast, tea ## Topic 3, Group Apex Wall: junior, suite, upgrade, towel, executive, card, bed ## Topic 3, Group Bulgari: website, classy, bad, expectation, toilet, scone, cut ## Topic 3, Group City View: ## Topic 3, Group Corinthia: promenade, chair, lounge, pianist, piano, music, wow ## Topic 3, Group Dorchester: pianist, piano, scone, afternoon, sandwich, surrounding, highly ## Topic 3, Group Hartley: ## Topic 3, Group Hotel Xenia: executive, continental, doorman, break, class, lounge, private ## Topic 3, Group Lanesborough: pianist, guess, scone, sandwich, afternoon, enter, type ## Topic 3, Group London Guest House: cereal, bean, toast, juice, coffee, fruit, plenty ## Topic 3, Group Mandarin Oriental: swim, treatment, pool, massage, spa, suite, club ## Topic 3, Group Marble Arch: menu ## Topic 3, Group Mondrian: spa, treatment, massage, share, world, window, concierge ## Topic 3, Group Newham: amp, egg, breakfast, bread, fruit, juice, tea ## Topic 3, Group Rembrandt: gym, spa, opposite, robe, chair, cereal, cold ## Topic 3, Group Rhodes: cereal, basic, toast, cheese, terrace, coffee, fruit ## Topic 3, Group Ridgemount: machine, internet, bacon, cereal, sausage, bean, free ## Topic 3, Group Savoy: pianist, piano, american, beaufort, scone, play, foyer ## Topic 3, Group Wellesley: feature, piano, pianist, music, play, italian, theatre ## ## Topic 4, Group 45 Park Lane: surrounding, host, wife, system, escort, executive, touch ## Topic 4, Group A To Z: break, smell, refurbish, carpet, fridge, fan, curtain ## Topic 4, Group Apex Wall: embankment, lounge, snack, robe, tea, iron, huge ## Topic 4, Group Bulgari: jacuzzi, breakfast, product, treatment, concierge, massage, bill ## Topic 4, Group City View: bad ## Topic 4, Group Corinthia: jacuzzi, perfection, swim, pool, chocolate, fruit, executive ## Topic 4, Group Dorchester: treatment, cup, escort, spa, airport, suit, wait ## Topic 4, Group Hartley: opposite, eye, read, door, bad, bed, carpet ## Topic 4, Group Hotel Xenia: classic, gym, tiny, pianist, stylish, large, terrace ## Topic 4, Group Lanesborough: amp, treatment, massage, butler, executive, junior, suite ## Topic 4, Group London Guest House: cheese, traffic, bank, single, twin, tiny, fit ## Topic 4, Group Mandarin Oriental: express, smoke, courtyard, tire, party, internet, star ## Topic 4, Group Marble Arch: upgrade, pay ## Topic 4, Group Mondrian: rumpus, play, grind, selection, cocktail, mini, superior ## Topic 4, Group Newham: block, flat, share, event, smell, carpet, partner ## Topic 4, Group Rembrandt: executive, gym, american, renovate, location, train, sauna ## Topic 4, Group Rhodes: elegant, build, single, street, interior, main, tiny ## Topic 4, Group Ridgemount: garden, street, traffic, single, face, stair, grind ## Topic 4, Group Savoy: butler, team, anniversary, club, treatment, attention, package ## Topic 4, Group Wellesley: gym, stun, oriental, smell, smoke, lobby, luxury ## Evaluate Interpret Visualize "],["data-formats.html", "2.3 Data Formats", " 2.3 Data Formats There are five common text mining packages, each with their own format requirements. Whichever package you work in, there is a decent chance you will want to use a function from one of the others, so you need some fluency in them all. tm works with Corpus objects (raw text with document and corpus metadata). Many tm algorithms work with a document-term matrix (DTM), a sparse matrix with one row per document, one column per term, and values equaling the word count or tf-idf. quanteda also works with Corpus objects, but has its own implementation. Many quanteda algorithms work with a document-feature matrix (DFM), again similar to tm’s DTM. tidytext works with tibbles. Many tidytext algorithms work with tibbles with one row per token (usually a word, but possibly a large item of text), a frequency count column, and possibly other metadata columns. qdap works with text fields in a data frame, so it does not require any particular data structure. sentimentr is similar to qdap. Let’s take the sawyer_raw data frame and pre-process it for all three packages. tm Turn the character vector sawyer_raw$text into a text source with VectorSource(), then turn the text source into a corpus with vCorpus(). Clean the corpus with a series of utility functions. One particularly important function, removeWords(), removes stop words, plus any custom stop words. I would normally add “tom” because it is so ubiquitous throughout the text. However, in this case I won’t because stopwords includes valence shifting words like “very” which are used in polarity scoring. I can remove them later for other exercises. # (sawyer_tm &lt;- VCorpus(VectorSource(sawyer$text)) %&gt;% # tm_map(content_transformer(replace_abbreviation)) %&gt;% # tm_map(removePunctuation) %&gt;% # tm_map(removeNumbers) %&gt;% # tm_map(content_transformer(tolower)) %&gt;% # tm_map(removeWords, c(stopwords(&quot;en&quot;), &quot;tom&quot;)) %&gt;% # tm_map(stripWhitespace)) Each document in the sawyer_tm VCorpus is a line of text. Use DocumentTermMaterix() to convert the vCorpus into tm’s bag-of-words format, DTM. # (sawyer_tm_dtm &lt;- DocumentTermMatrix(sawyer_tm)) This is a very sparse (nearly 100% sparse) matrix documents as rows and distinct words as columns. # group_by(chapter) %&gt;% # mutate(text = paste(text, collapse = &quot; &quot;)) %&gt;% # slice_head(n = 1) %&gt;% # select(chapter, text) # # sawyer_sent &lt;- sawyer %&gt;% # sentSplit(&quot;text&quot;) # # skimr::skim(sawyer) quanteda dafdafd tidytext dafdafd "],["sentimentanalysis.html", "Chapter 3 Sentiment Analysis", " Chapter 3 Sentiment Analysis Sentiment analysis is the extraction of the emotional intent of text. You can classify the polarity (positive | negative) or sentiment (angry | sad | happy | …) at the document, sentence, or feature level. This section continues with the hotel data from 1. load(&quot;input/hotel_prepped.Rdata&quot;) hotel_0 &lt;- hotel_prep glimpse(token) ## Rows: 1,112,307 ## Columns: 3 ## $ review_id &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … ## $ word &lt;chr&gt; &quot;pleasure&quot;, &quot;stay&quot;, &quot;night&quot;, &quot;recently&quot;, &quot;perfect&quot;, &quot;communi… ## $ n &lt;int&gt; 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, … "],["subjectivity-lexicons.html", "3.1 Subjectivity Lexicons", " 3.1 Subjectivity Lexicons A subjectivity lexicon is a predefined list of words associated with emotional context such as positive/negative. Subjectivity lexicons are typically short (a few thousand words), but work because of Zipf’s law which holds that the nth-ranked item in a frequency table has a frequency count equal to 1/n of the top-ranked item. So infrequently used words are used very infrequently. There are three common sentiment lexicons. Bing is common for polarity scoring, AFINN for emotion classification. NRC is a less common option for emotion classification. Bing classifies words as positive or negative. bing &lt;- tidytext::get_sentiments(&quot;bing&quot;) %&gt;% # remove dups filter(!word %in% c(&quot;envious&quot;, &quot;enviously&quot;, &quot;enviousness&quot;)) bing %&gt;% count(sentiment) %&gt;% adorn_totals() %&gt;% flextable::flextable() %&gt;% flextable::autofit() .cl-80e9df8a{}.cl-80e4214e{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-80e42158{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-80e66ce2{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-80e66cec{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-80e67a5c{width:0.941in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-80e67a66{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-80e67a67{width:0.941in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-80e67a70{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-80e67a7a{width:0.941in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-80e67a7b{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-80e67a7c{width:0.941in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-80e67a84{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}sentimentnnegative4,778positive2,002Total6,780 AFINN, by Finn Arup Nielsen, associates words with a manually rated valence integer between -5 (negative) and +5 (positive). afinn &lt;- tidytext::get_sentiments(&quot;afinn&quot;) afinn %&gt;% count(value) %&gt;% adorn_totals() %&gt;% flextable::flextable() %&gt;% flextable::autofit() .cl-811016a0{}.cl-810a15c0{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-810a15ca{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-810c6636{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-810c664a{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-810c78c4{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-810c78ce{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-810c78d8{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-810c78e2{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-810c78e3{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-810c78ec{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-810c78ed{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-810c78ee{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-810c78f6{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-810c78f7{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-810c78f8{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-810c7900{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-810c790a{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-810c790b{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}valuen-516-443-3264-2966-13090112082448317244555Total2,477 NRC lexicon associates words with eight emotions corresponding to the second level of Plutchik’s Wheel of Emotions and two sentiments (negative and positive). NRC was created by manual annotation on a crowd sourcing platform (see this). nrc &lt;- tidytext::get_sentiments(&quot;nrc&quot;) nrc %&gt;% count(sentiment) %&gt;% adorn_totals() %&gt;% flextable::flextable() %&gt;% flextable::autofit() .cl-81255e98{}.cl-811e641c{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-811e6430{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-81216b62{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-81216b76{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-81217b7a{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-81217b84{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-81217b85{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-81217b86{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-81217b8e{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-81217b98{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-81217b99{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-81217b9a{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-81217ba2{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-81217ba3{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-81217bac{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-81217bb6{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}sentimentnanger1,245anticipation837disgust1,056fear1,474joy687negative3,316positive2,308sadness1,187surprise532trust1,230Total13,872 "],["polarity-scoring.html", "3.2 Polarity Scoring", " 3.2 Polarity Scoring Two packages measure text polarity. The simpler one tidytext: unnest tokens, join to the Bing lexicon, and calculate the net of positive minus negative polarity counts. sentimentr is more sophisticated in that it takes into account valence shifters, surrounding words that change the intensity of a sentiment (e.g., “very”) or switch its direction (e.g., “not”).4 3.2.1 tidytext The tidytext way to score polarity is to tag words as “positive” and “negative” using the Bing lexicon, then calculate the difference in counts. The qdap and sentimentr packages correct for text length by dividing by \\(\\sqrt{n}\\). It is useful to capture the positive and negative words back in the main data frame for explaining how the polarity score was calculated. polarity_bing &lt;- token %&gt;% left_join(bing, by = &quot;word&quot;, relationship = &quot;many-to-one&quot;) %&gt;% summarize(.by = c(review_id, sentiment), n = n(), words = list(word)) %&gt;% pivot_wider(names_from = sentiment, values_from = c(n, words), values_fill = list(n = 0)) %&gt;% select(-c(n_NA, words_NA)) %&gt;% inner_join(hotel_0 %&gt;% select(review_id, word_cnt), by = &quot;review_id&quot;) %&gt;% mutate( polarity = (n_positive - n_negative) / sqrt(word_cnt), polarity_desc = if_else(polarity &gt;= 0, &quot;Positive&quot;, &quot;Negative&quot;) ) polarity_bing %&gt;% filter(review_id == 520) ## # A tibble: 1 × 8 ## review_id n_positive n_negative words_positive words_negative word_cnt ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;list&gt; &lt;list&gt; &lt;int&gt; ## 1 520 0 0 &lt;NULL&gt; &lt;NULL&gt; 47 ## # ℹ 2 more variables: polarity &lt;dbl&gt;, polarity_desc &lt;chr&gt; polarity_afinn &lt;- token %&gt;% inner_join(afinn, by = &quot;word&quot;, relationship = &quot;many-to-one&quot;) %&gt;% summarize(.by = review_id, sentiment = sum(value), words = list(word)) %&gt;% inner_join(hotel_0 %&gt;% select(review_id, word_cnt), by = &quot;review_id&quot;) %&gt;% mutate( polarity = sentiment / sqrt(word_cnt), polarity_desc = if_else(polarity &gt;= 0, &quot;Positive&quot;, &quot;Negative&quot;) ) # Attach to main data frame hotel_1 &lt;- hotel_0 %&gt;% left_join(polarity_bing %&gt;% select(review_id, polarity, words_positive, words_negative) %&gt;% rename_with(~paste0(&quot;bing_&quot;, .x)), by = join_by(review_id == bing_review_id)) %&gt;% left_join(polarity_afinn %&gt;% select(review_id, polarity) %&gt;% rename_with(~paste0(&quot;afinn_&quot;, .x)), by = join_by(review_id == afinn_review_id)) Let’s see how the polarity scores compare. hotel_1 %&gt;% pivot_longer(cols = c(bing_polarity, afinn_polarity), names_to = &quot;lexicon&quot;, values_to = &quot;polarity&quot;) %&gt;% filter(!is.na(polarity)) %&gt;% ggplot(aes(x = polarity, y = fct_rev(property))) + geom_boxplot() + facet_wrap(facets = vars(lexicon)) + labs(title = &quot;Review polarity&quot;, x = NULL, y = NULL, caption = glue::glue(&quot;Bing Polarity = (n_pos - n_neg) / sqrt(n_words)\\n&quot;, &quot;AFINN Polarity = sentiment / sqrt(n_words)&quot;)) The two lexicons are similar. The data set includes a numeric rating review_rating (1-5). I’ll stick with Bing going forward for convenience. The polarity score should correlate with the numeric rating. hotel_1 %&gt;% filter(!is.na(bing_polarity)) %&gt;% ggplot(aes(x = as_factor(rating), y = bing_polarity)) + geom_jitter(width = 0.2, alpha = 0.3, color = &quot;#5DA5DA&quot;, size = 1) + geom_boxplot(alpha = 0) + theme_minimal() + labs(title = &quot;Polarity is associated with overall Likert score&quot;, x = &quot;Overall Likert Rating&quot;, y = &quot;Polarity Score&quot;) Sentiment increases with Likert rating, but there are many reviews with a rating of 5 and a polarity score &lt;0. In some cases this is because the reviewer interpreted the scale incorrectly. You can use polarity scores to identify problematic reviews like these. hotel_1 %&gt;% mutate( problematic = case_when( (rating == 1 &amp; bing_polarity &gt; 0.5) ~ &quot;Too Low&quot;, (rating == 5 &amp; bing_polarity &lt; -.5) ~ &quot;Too High&quot;, TRUE ~ &quot;Other&quot; ) ) %&gt;% filter(problematic %in% c(&quot;Too High&quot;, &quot;Too Low&quot;)) %&gt;% group_by(problematic) %&gt;% slice_max(order_by = abs(bing_polarity), n = 1) %&gt;% select(problematic, rating, bing_polarity, review) %&gt;% flextable::flextable() %&gt;% flextable::autofit() %&gt;% flextable::valign(valign = &quot;top&quot;) .cl-82fb89c2{}.cl-82f58946{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-82f58950{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-82f7d6e2{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-82f7d6e3{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-82f7d6ec{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-82f7d6ed{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-82f7e45c{width:1.072in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-82f7e466{width:0.671in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-82f7e470{width:1.157in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-82f7e471{width:32.807in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-82f7e47a{width:1.072in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-82f7e484{width:0.671in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-82f7e48e{width:1.157in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-82f7e48f{width:32.807in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-82f7e490{width:1.072in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-82f7e498{width:0.671in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-82f7e4a2{width:1.157in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-82f7e4ac{width:32.807in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}problematicratingbing_polarityreviewToo High5-0.9801961Some design faults in the bathroom - no stool, misplaced grab handles and vanity mirror. Very disappointing experience in Savoy Grill. Good quality ingredients but poorly presented and tasteless. Numerous mistakes in service including charging for expensive drinks which we did not have. Service charge revoked and booking for following night cancelled.Too Low10.8728716Stayed here many times both for business and pleasure, alone, with my wife and even with extended family and children. It is in fact impossible to seperate the business and pleasure stays because everytime was a breathtakingly delectable pleasure. Meeting friends and colleagues whether in the lobby one of the resturants always left an impressive memorable impression. Alas, I haven't been able to visit again for sometime - either fully booked or are not able to guarantee convenient parking for my personal chauffeur and car. The polarity words can help explain why some hotels rated poor or excellent. token %&gt;% inner_join(hotel_1 %&gt;% filter(rating %in% c(1, 5)), by = join_by(review_id)) %&gt;% filter(!word %in% c(&quot;hotel&quot;, &quot;stay&quot;, &quot;night&quot;)) %&gt;% filter((rating == 5 &amp; bing_polarity &gt; 0) | (rating == 1 &amp; bing_polarity &lt; 0)) %&gt;% count(rating, word) %&gt;% mutate(.by = rating, pct = n / sum(n)) %&gt;% group_by(rating) %&gt;% slice_max(order_by = pct, n = 10) %&gt;% ggplot(aes(x = pct, y = reorder_within(word, by = pct, within = rating))) + geom_col() + scale_y_reordered() + scale_x_continuous(labels = percent_format(1)) + labs(y = NULL, x = NULL) + facet_wrap(facets = vars(rating), scales = &quot;free_y&quot;) Word clouds are a nice way to get an overview of the data. token %&gt;% inner_join(hotel_1 %&gt;% filter(rating %in% c(1, 5)), by = join_by(review_id)) %&gt;% filter(!word %in% c(&quot;hotel&quot;, &quot;stay&quot;, &quot;night&quot;, &quot;london&quot;), !is.na(bing_polarity)) %&gt;% mutate(polarity_desc = if_else(bing_polarity &gt; 0, &quot;Positive&quot;, &quot;Negative&quot;)) %&gt;% count(word, polarity_desc, wt = word_cnt) %&gt;% pivot_wider(names_from = polarity_desc, values_from = n, values_fill = 0) %&gt;% data.table::data.table() %&gt;% as.matrix(rownames = &quot;word&quot;) %&gt;% wordcloud::comparison.cloud(max.words = 30, title.size = 1.5, scale = c(1, 3.5)) 3.2.2 sentimentr sentimentr calculates polarity at the sentence level. It improves on tidytext in that it takes into account the context in which the sentiment words occur by incorporating valence shifters. A negator flips the direction of a polarizing word (e.g., “I do not like it.”). lexicon::hash_valence_shifters[y==1]. An amplifier intensifies the impact (e.g., “I really like it.”). lexicon::hash_valence_shifters[y==2]. A de-amplifier (downtoner) reduces the impact (e.g., “I hardly like it.”). lexicon::hash_valence_shifters[y==3]. An adversative conjunction overrules the previous clause containing a polarized word (e.g., “I like it but it’s not worth it.”). lexicon::hash_valence_shifters[y==4]. sentimentr uses a lexicon package combined from the syuzhet and lexicon packages. Positive words are scored +1 and negative words are scored -1. sentimentr identifies clusters of words within sentences of the text. The 4 words before and 2 words after are candidate valence shifters. Polarized words are weighted by the valence shifter weights: negators = -1; amplifiers and de-amplifiers = 1.8; adversative conjunctions decrease the value of the prior cluster and increase the value of the following cluster. Neutral words hold no value, but do affect the word count. hotel_sentimentr &lt;- sentimentr::get_sentences(hotel_1$review) %&gt;% sentimentr::sentiment() %&gt;% summarize(.by = element_id, sentimentr_polarity = mean(sentiment)) hotel_2 &lt;- hotel_1 %&gt;% mutate(element_id = row_number()) %&gt;% inner_join(hotel_sentimentr, by = join_by(element_id)) %&gt;% select(-element_id) Let’s see a few examples where sentimentr differed from tidytext. Looks like bing did a better job on the first one, but sentimentr was better on the next two. hotel_2 %&gt;% filter((bing_polarity &gt; 0.2 &amp; sentimentr_polarity &lt; -0.2) | (bing_polarity &lt; -0.2 &amp; sentimentr_polarity &gt; 0.2)) %&gt;% select(review, bing_polarity, sentimentr_polarity) %&gt;% head(3) %&gt;% flextable::flextable() %&gt;% flextable::autofit() .cl-aabc50a4{}.cl-aab5f9ca{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-aab5f9d4{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-aab8a3dc{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-aab8a3e6{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-aab8b476{width:56.161in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aab8b480{width:1.157in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aab8b48a{width:1.574in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aab8b48b{width:56.161in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aab8b494{width:1.157in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aab8b495{width:1.574in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aab8b496{width:56.161in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aab8b49e{width:1.157in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aab8b4a8{width:1.574in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aab8b4a9{width:56.161in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aab8b4b2{width:1.157in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aab8b4b3{width:1.574in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}reviewbing_polaritysentimentr_polarityWe were locked out of our room twice in a 3 day stay due to the Savoy not being able to track our reservation - the excuses included that we were responsible until they finally admitted their system is fouled up-0.31622780.2161730For many years I had fantasised about staying at The Dorchester so when the opportunity arose I jumped at it. Needless to say, as one of the great British institutions, it's a lovely hotel however it is not fabulous. It has two lovely dining areas /restaurants albeit pricey. I was extremely disappointed by the rooms! The corridors were creaky and narrow and the rooms pokey. There's no doubt it was tasteful decorated, however I was noisy -if the internal door wasn't shut and space was tight with a capital T. The double bed was very compact for 2, there was hardly any space to walk around it! Very claustrophobic! On a positive note the bathroom was spacious with delicious toiletries and good high pressure hot showers - and it you are a lover of hotel slippers- this one was good quality. All in all - charming but not worth the price or reputation- that is in the standard rooms. A word of advise- don't take your car!0.4670994-0.2377933Wonderful. Far better than The Ritz (slow and stiff) and Harvey Nichols (noisy and showy). The service is unsurpassable. Extensive range of teas. Enough food to make dinner unnecessary. Relaxed and unhurried atmosphere. I'm already looking forward to coming back.-0.47434160.2145884 There is a third package called qdap, but the sentimentr Read Me explains sentimentr is an improved version that better balances accuracy and speed.↩︎ "],["statistical-test.html", "3.3 Statistical Test", " 3.3 Statistical Test You can fit an ordinal logistic regression model to predict the rating based on the review sentiment. Which performs better, tidytext or sentimentr? Start with an intercept-only model for a baseline and review of ordinal logistic regression. # Limit to cases where both methods were able to score the review. mdl_dat &lt;- hotel_2 %&gt;% mutate(rating_fct = factor(rating, ordered = TRUE)) fit_intercept &lt;- ordinal::clm(rating_fct ~ 1, data = mdl_dat) summary(fit_intercept) ## formula: rating_fct ~ 1 ## data: mdl_dat ## ## link threshold nobs logLik AIC niter max.grad cond.H ## logit flexible 23373 -21405.88 42819.76 6(0) 1.79e-11 2.3e+01 ## ## Threshold coefficients: ## Estimate Std. Error z value ## 1|2 -3.84374 0.04566 -84.19 ## 2|3 -3.04000 0.03134 -97.01 ## 3|4 -2.15898 0.02147 -100.54 ## 4|5 -0.81248 0.01418 -57.31 The threshold coefficients in the summary table are the log-odds of the outcome variable having a level at or below vs above. Below, 10.3% of ratings were &lt;=3 and 89.7% were &gt;=4 for a log-odds of log(.103/.897) = -2.1620836, corresponding to the 3|4 line in the regression summary. mdl_dat %&gt;% tabyl(rating_fct) %&gt;% mutate(cum = cumsum(percent), `1-cum` = 1 - cum) ## rating_fct n percent cum 1-cum ## 1 490 0.02096436 0.02096436 0.9790356 ## 2 577 0.02468660 0.04565096 0.9543490 ## 3 1352 0.05784452 0.10349549 0.8965045 ## 4 4765 0.20386771 0.30736320 0.6926368 ## 5 16189 0.69263680 1.00000000 0.0000000 Now fit the bing and sentimentr models. The bing model has the lowest log-likelihood. fit_bing &lt;- ordinal::clm(rating_fct ~ bing_polarity, data = mdl_dat) fit_sentimentr &lt;- ordinal::clm(rating_fct ~ sentimentr_polarity, data = mdl_dat) anova(fit_bing, fit_sentimentr, fit_intercept) ## Likelihood ratio tests of cumulative link models: ## ## formula: link: threshold: ## fit_intercept rating_fct ~ 1 logit flexible ## fit_bing rating_fct ~ bing_polarity logit flexible ## fit_sentimentr rating_fct ~ sentimentr_polarity logit flexible ## ## no.par AIC logLik LR.stat df Pr(&gt;Chisq) ## fit_intercept 4 42820 -21406 ## fit_bing 5 37716 -18853 5106.0 1 &lt; 2.2e-16 *** ## fit_sentimentr 5 38521 -19256 -805.5 0 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 How about predictive performance? They both peformed about the same. (bing_conf &lt;- fit_bing %&gt;% augment(type = &quot;class&quot;) %&gt;% conf_mat(truth = rating_fct, estimate = .fitted)) ## Truth ## Prediction 1 2 3 4 5 ## 1 71 19 11 6 2 ## 2 0 0 0 0 0 ## 3 35 29 18 8 4 ## 4 304 343 477 521 681 ## 5 80 186 846 4230 15502 summary(bing_conf) ## # A tibble: 13 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.689 ## 2 kap multiclass 0.141 ## 3 sens macro 0.245 ## 4 spec macro 0.831 ## 5 ppv macro NA ## 6 npv macro 0.885 ## 7 mcc multiclass 0.168 ## 8 j_index macro 0.0759 ## 9 bal_accuracy macro 0.538 ## 10 detection_prevalence macro 0.2 ## 11 precision macro 0.453 ## 12 recall macro 0.245 ## 13 f_meas macro 0.312 (sentimentr_conf &lt;- fit_sentimentr %&gt;% augment(type = &quot;class&quot;) %&gt;% conf_mat(truth = rating_fct, estimate = .fitted)) ## Truth ## Prediction 1 2 3 4 5 ## 1 34 18 14 2 6 ## 2 0 0 0 0 0 ## 3 20 9 7 6 1 ## 4 362 345 485 321 330 ## 5 74 205 846 4436 15852 summary(sentimentr_conf) ## # A tibble: 13 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.694 ## 2 kap multiclass 0.123 ## 3 sens macro 0.224 ## 4 spec macro 0.828 ## 5 ppv macro NA ## 6 npv macro 0.904 ## 7 mcc multiclass 0.159 ## 8 j_index macro 0.0524 ## 9 bal_accuracy macro 0.526 ## 10 detection_prevalence macro 0.2 ## 11 precision macro 0.384 ## 12 recall macro 0.224 ## 13 f_meas macro 0.268 "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
