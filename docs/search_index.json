[["index.html", "Natural Language Processing in R Intro", " Natural Language Processing in R Michael Foley 2023-12-07 Intro Text mining identifies patterns in text. Text mining is commonly used for topic modeling, sentiment analysis, and classification modeling. There are two main approaches to text mining. Semantic parsing identifies words by type and order (sentences, phrases, nouns/verbs, proper nouns, etc.). Bag of words simply treats words as attributes of the document. "],["data-prep.html", "Chapter 1 Data Preparation", " Chapter 1 Data Preparation This section covers how to prepare a corpus for text analysis. I’ll work with the customer reviews of London-based hotels data set hosted on data.world. hotel_raw contains 27K reviews of the ten most- and ten least-expensive hotels in London. The csv file is located online here. I saved it to my \\inputs directory. library(tidyverse) library(tidytext) library(janitor) library(scales) library(glue) hotel_0 &lt;- read_csv( &quot;input/london_hotel_reviews.csv&quot;, col_types = &quot;cicccc&quot;, col_names = c(&quot;hotel&quot;, &quot;rating&quot;, &quot;title&quot;, &quot;review&quot;, &quot;reviewer_loc&quot;, &quot;review_dt&quot;), skip = 1 ) %&gt;% mutate(review_id = row_number()) %&gt;% select(review_id, everything(), -c(title, review_dt)) glimpse(hotel_0) ## Rows: 27,330 ## Columns: 5 ## $ review_id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17… ## $ hotel &lt;chr&gt; &quot;Apex London Wall Hotel&quot;, &quot;Corinthia Hotel London&quot;, &quot;The … ## $ rating &lt;int&gt; 5, 5, 5, 4, 5, 1, 5, 5, 5, 5, 5, 4, 2, 4, 5, 5, 5, 5, 5, … ## $ review &lt;chr&gt; &quot;Siamo stati a Londra per un week end ed abbiamo alloggia… ## $ reviewer_loc &lt;chr&gt; &quot;Casale Monferrato, Italy&quot;, &quot;Savannah, Georgia&quot;, &quot;London&quot;… "],["scrub.html", "1.1 Scrub", " 1.1 Scrub The data needs to be cleaned. I’ll follow some of the techniques used by Nagelkerke (2020a). One issue is tags like &lt;e9&gt; and unicode characters like &lt;U+0440&gt;. One way to get rid of unicode characters is to convert them to ASCII tags with iconv() and then remove the ASCII tags with str_remove(). E.g., iconv() converts &lt;U+0093&gt; to &lt;93&gt; which you can remove with regex \"\\\\&lt;[:alnum]+\\\\&gt;]\".1 There are also some reviews in other languages that I’ll just drop. And some hotel names are pretty long, so I’ll abbreviate them. hotel_1 &lt;- hotel_0 %&gt;% mutate( # Create ASCII bytes review = iconv(review, from = &quot;&quot;, to = &quot;ASCII&quot;, sub = &quot;byte&quot;), # Remove &lt;..&gt; review = str_remove_all(review, &quot;\\\\&lt;[[:alnum:]]+\\\\&gt;&quot;), # Remove &lt;U+....&gt; review = str_remove_all(review, &quot;\\\\&lt;U\\\\+[[:alnum:]]{4}\\\\&gt;&quot;), # Only keep letters, numbers, and apostrophes. review = str_remove_all(review, &quot;[^[:alnum:][\\\\s][\\\\&#39;]]&quot;), review = str_squish(review), # Shorten some of the hotel names. hotel = str_remove_all( hotel, &quot;( - .*)|(, .*)|( Hotel)|( London)|(The )|( at .*)|( Hyde .*)|( Knights.*)&quot; ), hotel = factor(hotel, ordered = TRUE), # Reducing number of hotels for modeling simplicity. hotel = fct_lump_prop(hotel, prop = .05), # Bin common locations, reviewer_loc = factor(case_when( str_detect(reviewer_loc, &quot;(London)|(United Kingdom)|(UK)&quot;) ~ &quot;United Kingdom&quot;, str_detect(reviewer_loc, &quot;(New York)|(California)&quot;) ~ &quot;United States&quot;, TRUE ~ &quot;Other&quot; )), # Low ratings are so rare, lump the bottom two. rating = fct_collapse(as.character(rating), `1-2` = c(&quot;1&quot;, &quot;2&quot;)), # Interesting metadata raw_chrcnt = str_length(review) ) %&gt;% # Exclude reviews written in a foreign language. One heuristic to handle this # is to look for words common in other languages that do not also occur in English. filter( !str_detect(review, &quot;( das )|( der )|( und )|( en )&quot;), # German !str_detect(review, &quot;( et )|( de )|( le )|( les )&quot;), # French !str_detect(review, &quot;( di )|( e )|( la )&quot;), # Italian !str_detect(review, &quot;( un )|( y )&quot;), # Spanish raw_chrcnt &gt; 0 ) That might be enough. Let’s explore the data. We have 9 hotels. Reviewers are binned into 3 locations. 90% of reviews rate the property a 4 or 5. Some reviews are as small as 1 character, but they can get quite large. skimr::skim(hotel_1) Table 1.1: Data summary Name hotel_1 Number of rows 23611 Number of columns 6 _______________________ Column type frequency: character 1 factor 3 numeric 2 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace review 0 1 1 29798 0 23572 0 Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts hotel 0 1 TRUE 9 Sav: 5054, Mon: 3933, Oth: 3770, Rem: 2553 rating 0 1 FALSE 4 5: 16333, 4: 4827, 3: 1368, 1-2: 1083 reviewer_loc 0 1 FALSE 3 Oth: 13958, Uni: 8552, Uni: 1101 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist review_id 0 1 13655.3 7875.48 2 6830.5 13666 20464.5 27330 ▇▇▇▇▇ raw_chrcnt 0 1 696.4 676.68 1 301.0 502 848.0 29798 ▇▁▁▁▁ Nagelkerke (2020a) recommends removing punctuation to focus on the entire text rather than the sentences within. Nagelkerke also suggests removing very short (&lt;= 3 chars) for anything other than sentiment analysis. I’m going to keep punctuation and short reviews for now even though some of those extremely short reviews are gibberish. References "],["tokenize.html", "1.2 Tokenize", " 1.2 Tokenize Most models are based on tokenized text. Even if you are interested in working with bigrams, tokenize into words to clean and regularize the data first. token_0 &lt;- hotel_1 %&gt;% select(review_id, review) %&gt;% unnest_tokens(&quot;word&quot;, review) # Attach word counts back to main data frame, just to aid understanding. hotel_2 &lt;- hotel_1 %&gt;% inner_join(count(token_0, review_id, name = &quot;raw_wordcnt&quot;), by = join_by(review_id)) hotel_2 %&gt;% select(raw_chrcnt, raw_wordcnt) %&gt;% summary() ## raw_chrcnt raw_wordcnt ## Min. : 1.0 Min. : 1.0 ## 1st Qu.: 301.0 1st Qu.: 55.0 ## Median : 502.0 Median : 92.0 ## Mean : 696.4 Mean : 128.8 ## 3rd Qu.: 848.0 3rd Qu.: 157.0 ## Max. :29798.0 Max. :5657.0 "],["spell-check.html", "1.3 Spell-check", " 1.3 Spell-check Run a spell-check to regularize the data. It’s possible to land on the wrong correction, but there is probably more to gain than lose. Only a very small fraction of these tokens were misspellings. # There are multiple possible right spellings, so just choose one. spell_check &lt;- fuzzyjoin::misspellings %&gt;% distinct(misspelling, .keep_all = TRUE) token_1 &lt;- token_0 %&gt;% left_join(spell_check, by = join_by(word == misspelling)) %&gt;% mutate(word = coalesce(correct, word)) %&gt;% select(-correct) # Only .09% of words were misspelled. mean(token_0$word != token_1$word) ## [1] 0.0009456979 # Examples. tibble(before = token_0$word, after = token_1$word) %&gt;% filter(before != after) %&gt;% count(before, after, sort = TRUE) ## # A tibble: 435 × 3 ## before after n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 didnt didn&#39;t 481 ## 2 wasnt wasn&#39;t 183 ## 3 definately definitely 180 ## 4 helpfull helpful 97 ## 5 accomodating accommodating 59 ## 6 doesnt doesn&#39;t 54 ## 7 isnt isn&#39;t 54 ## 8 seperate separate 52 ## 9 alot a lot 50 ## 10 accomodation accommodation 44 ## # ℹ 425 more rows "],["remove-stop-words.html", "1.4 Remove Stop Words", " 1.4 Remove Stop Words Stop words usually add no value, but you should pay attention to what you are dropping. Be ready to add pertinent words back and perhaps drop others. # Start with a standard list. stop &lt;- tidytext::stop_words %&gt;% # Remove potentially useful words from stop list. filter(!word %in% c(&quot;appreciate&quot;, &quot;room&quot;, &quot;first&quot;)) %&gt;% # Add custom stop words. bind_rows(tibble(word = c(&quot;hotel&quot;))) token_2 &lt;- anti_join(token_1, stop, by = &quot;word&quot;) # Most frequently removed words token_1 %&gt;% anti_join(token_2, by = join_by(review_id, word)) %&gt;% count(word, sort = TRUE) ## # A tibble: 695 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 the 195922 ## 2 and 117258 ## 3 a 88453 ## 4 to 77843 ## 5 was 64089 ## 6 in 52906 ## 7 we 47505 ## 8 of 44723 ## 9 i 43501 ## 10 for 38187 ## # ℹ 685 more rows "],["lemmatize.html", "1.5 Lemmatize", " 1.5 Lemmatize Stemming and lemmatizing convert word variations like “staying”, “stayed”, and “stay” into a generic form: “stay”. Stemming tends to chop off endings to create a root word, but the stem is often not a word itself. E.g., “staying” becomes “stai”. Lemmatize gives you the more natural “stay”. token &lt;- token_2 %&gt;% mutate(word = textstem::lemmatize_words(word)) tibble(before = token_2$word, after = token$word) %&gt;% filter(before != after) %&gt;% count(before, after, sort = TRUE) ## # A tibble: 8,030 × 3 ## before after n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 stayed stay 8770 ## 2 hotels hotel 4135 ## 3 amazing amaze 3969 ## 4 booked book 3013 ## 5 nights night 2923 ## 6 arrived arrive 2510 ## 7 bit bite 2470 ## 8 restaurants restaurant 2343 ## 9 staying stay 2311 ## 10 minutes minute 2168 ## # ℹ 8,020 more rows "],["prepped-data.html", "1.6 Prepped Data", " 1.6 Prepped Data Now that the data is prepped, created a stylized review text. prepped_hotel &lt;- token %&gt;% summarize( .by = review_id, prepped_review = paste(word, collapse = &quot; &quot;), prepped_wrdcnt = n() ) %&gt;% inner_join(hotel_2, by = join_by(review_id)) %&gt;% relocate(prepped_review, prepped_wrdcnt, .after = last_col()) prepped_hotel %&gt;% select(review_id, review, prepped_review) %&gt;% filter(review_id == 3) %&gt;% flextable::flextable() %&gt;% flextable::valign(valign = &quot;top&quot;) %&gt;% flextable::autofit() .cl-1c35f8a4{}.cl-1c2d17e8{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-1c2d181a{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-1c31d2d8{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1c31d2e2{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1c31d2ec{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1c31d2ed{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1c31e30e{width:0.918in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c31e318{width:18.629in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c31e319{width:9.147in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c31e31a{width:0.918in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c31e322{width:18.629in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c31e323{width:9.147in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}review_idreviewprepped_review3A very lovely first visit to this iconic hotel bar Wonderful service without being intrusive at all Very delicious cocktails and just generally all round a very indulgent experience Well worth visiting only for that 'once in a lifetime' experience though do make sure you are feeling 'flush' it doesn't come cheaplovely first visit iconic bar wonderful service intrusive delicious cocktail round indulgent experience worth visit lifetime experience feel flush cheap "],["bigrams.html", "1.7 Bigrams", " 1.7 Bigrams If you intend to present bigrams, don’t simply tokenize the raw or prepped text into bigrams because you don’t want stop words in bigram, nor do you want words that aren’t actually adjacent because you’ve removed stop words. Instead, tokenize into bigrams, split the bigrams into words, and filter out rows where one or both words is stop word. # Reassemble token_1 into text and re-tokenize so you get the spalling corrections. bigram_0 &lt;- token_1 %&gt;% summarize(.by = review_id, reconstructed = paste(word, collapse = &quot; &quot;)) %&gt;% unnest_tokens(&quot;bigram&quot;, reconstructed, token = &quot;ngrams&quot;, n = 2) # Remove bigrams where one or both words are stop words. bigram &lt;- bigram_0 %&gt;% separate(bigram, into = c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;% anti_join(stop, by = join_by(word1 == word)) %&gt;% anti_join(stop, by = join_by(word2 == word)) %&gt;% mutate(bigram = paste(word1, word2)) %&gt;% select(review_id, bigram) # Example bind_cols( hotel_2 %&gt;% filter(review_id == 3) %&gt;% select(review), bigram %&gt;% filter(review_id == 3) %&gt;% summarize(bigrams = paste(bigram, collapse = &quot;\\n&quot;)) ) %&gt;% flextable::flextable() %&gt;% flextable::autofit() %&gt;% flextable::width(j = 1, width = 4.5, unit = &quot;in&quot;) %&gt;% flextable::width(j = 2, width = 1.5, unit = &quot;in&quot;) %&gt;% flextable::valign(valign = &quot;top&quot;) .cl-2d7b0000{}.cl-2d75700e{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-2d757018{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-2d77a93c{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-2d77a950{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-2d77b63e{width:4.5in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2d77b648{width:1.5in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2d77b652{width:4.5in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2d77b653{width:1.5in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}reviewbigramsA very lovely first visit to this iconic hotel bar Wonderful service without being intrusive at all Very delicious cocktails and just generally all round a very indulgent experience Well worth visiting only for that 'once in a lifetime' experience though do make sure you are feeling 'flush' it doesn't come cheaplovely firstfirst visitbar wonderfulwonderful servicedelicious cocktailsindulgent experienceworth visitinglifetime experiencefeeling flush "],["save.html", "1.8 Save", " 1.8 Save Save the cleaned data for other analyses like topic modeling and sentiment analysis. save(prepped_hotel, token, bigram, file = &quot;input/hotel_prepped.Rdata&quot;) "],["topicmodeling.html", "Chapter 2 Topic Modeling", " Chapter 2 Topic Modeling Topic models are generative probabilistic models that identify topics as clusters of words with an associated probability distribution and a probability distribution of topics within each document. Topic models such as Latent Dirichlet Allocation (LDA) and Structural Topic Modeling (STM) treat documents within a corpora as a “bags of words” and identifies groups of words that tend to co-occur. The groups are the topics, formally conceptualized as probability distributions over vocabulary. LDA and STM are generative models of word counts, meaning they model a process that generates text which is a mixture of topics composed of words both of which follow probability distributions. Think of documents as the product of an algorithm that selects each word in two stages: 1) sample a topic, then 2) sample a word given that topic. The task in topic modeling is to tune the hyperparameters that define the probability distributions. In a way, topic models do the opposite of what you might expect. They do not estimate the probability that document x is about topic y. Rather, they estimate the contribution of all Y topics to document x. This leads to two frameworks for thinking about topics. Prevalance estimates the proportions of the document generated by the topic. Content is the probability distribution of words within the topic. LDA and STM differ only in how they handle these frameworks. STM controls for covariates associated with prevalence and content while LDA does not. LDA is implemented in the topicmodels package and STM is implemented in the stm package. Whether you use LDA or STM, you start by creating a bag-of-words representation of the data. This was done in Chapter 1. This chapter continues from there and follows the ideas from Nagelkerke (2020b), Meaney (2022), and the stm package vignette package. library(tidyverse) library(tidymodels) library(topicmodels) library(tidytext) library(stm) library(scales) library(glue) library(httr2) library(jsonlite) load(&quot;input/hotel_prepped.Rdata&quot;) glimpse(token) ## Rows: 1,136,694 ## Columns: 2 ## $ review_id &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … ## $ word &lt;chr&gt; &quot;pleasure&quot;, &quot;stay&quot;, &quot;7&quot;, &quot;night&quot;, &quot;recently&quot;, &quot;perfect&quot;, &quot;co… References "],["lda.html", "2.1 LDA", " 2.1 LDA Latent Dirichlet allocation (LDA) is one of a family of mixed membership models that decompose data into latent components. Latent means unidentified topics and Dirichlet is the distribution followed by the words in topics and by topics in documents. LDA presumes each document is created by a generative process in which topics are selected from a probability distribution and then words from that topic are selected from another distribution. LDA is an optimization algorithm to estimate those distributions by performing a random search through the parameter space to find the model with the largest log-likelihood. There are multiple search algorithms, but the preferred one appears to be Gibbs sampling, a type of Monte Carlo Markov Chain (MCMC) algorithm. The algorithm is: For each document \\(d = 1 \\ldots D\\), randomly assign each word \\(w = 1 \\ldots W\\) to one of \\(k = 1\\ldots K\\) topics. Tabulate the number of words in each document and topic, a \\(D \\times K\\) matrix, and tabulate the number of occurrences of each word in each document, a \\(W \\times D\\) matrix. Resample to remove a single instance of a word from the corpus, decrementing the document’s topic count and the word’s topic count. Calculate the gamma matrix, \\(\\gamma\\), and the beta matrix, \\(\\beta\\). the gamma matrix (aka theta) is the topical prevalence, the probability distribution of topics for each document, \\[p(k|d) = \\frac{n_{dk} + \\alpha}{N_i + K \\alpha}\\] were \\(n_{dk}\\) is the number of words in document \\(d\\) for topic \\(k\\), \\(N_d\\) is the total number of words in \\(d\\), and \\(\\alpha\\) is a hyperparameter. For each \\(d\\), \\(\\sum_{k \\in K} \\gamma_{dk} = 1\\). the beta matrix (aka phi), is the topical content, the probability distribution of words for each topic, \\[p(w|k) = \\frac{m_{w,k} + \\beta}{\\sum_{w \\in W}m_{d,k} + W\\beta}\\] where \\(m_{w,k}\\) is the corpus-wide frequency count of word \\(w\\) to topic \\(k\\), \\(W\\) is the number of distinct words in the corpus, and \\(\\beta\\) is a hyperparameter. For each \\(k\\), \\(\\sum_{w \\in W} \\beta_{kw} = 1\\). Calculate the joint probability distribution of words for each document and topic, \\(p(w|k,d) = p(k|d)p(w|k)\\). Assign each word, \\(w\\), to the topic with the maximum joint probability. Repeat steps 3-6 for all of the words in all of the documents. Repeat steps 3-7 for a pre-determined number of iterations. LDA thus has 3 hyperparameters: the document-topic density factor, \\(\\alpha\\), the topic-word density factor, \\(\\beta\\), and the topic count, \\(K\\). \\(\\alpha\\) controls the number of topics expected per document (large \\(\\alpha\\) = more topics). \\(\\beta\\) controls the distribution of words per topic (large \\(\\beta\\) = more words). Ideally, you want a few topics per document and a few words per topic, so \\(\\alpha\\) and \\(\\beta\\) are typically set below one. \\(K\\) is set using a combination of domain knowledge, coherence, and exclusivity. Notice that LDA is a “bag of words” method. It does not consider the order of the tokens in the text, so where tokens are located what other tokens are nearby do not factor into the output. Preprocessing We already cleaned the text in Chapter 1. The next step is to create a document-term matrix (DTM). A DTM has one row per per document, one column per term, and the cells are frequencies. The DTM contains mostly unhelpful infrequently used terms, so the pre-processing step removes sparse terms. Keep only the decent sized reviews, ones with at least 25 words. If this is a predictive model, now is the time to create a train/test split. Consider weighting the split by the outcome variable (rating in this case) to ensure proportional coverage. set.seed(12345) hotel_lda &lt;- prepped_hotel %&gt;% filter(prepped_wrdcnt &gt;= 25) # Parameter `strata` ensures proportional coverage of ratings. lda_split &lt;- rsample::initial_split(hotel_lda, prop = 3/4, strata = rating) lda_train &lt;- token %&gt;% semi_join(training(lda_split), by = join_by(review_id)) lda_test &lt;- token %&gt;% semi_join(testing(lda_split), by = join_by(review_id)) Most words add little value to a topic model because they appear infrequently or too frequently. Including them only wastes computing resources. The most common metric for removing sparse terms is the term frequency-inverse document frequency (TF-IDF). TF(t,d) is term t’s usage proportion in document d. IDF(t) the log of the inverse of the term t’s proportion of documents it appears in. For example, “savoy” appears in n = 1,873 of the N = 12,121 training documents. Its IDF is log(N/n) = 1.87. “savoy” appears in review #5 in 2 of 30 (6.67%) of the terms. The TF-IDF score is the product of the two numbers. Here is that prepped review. form moment arrive leave experience absolute perfection service excellence savoy staff famous personalise service rich heritage savoy win hotel world stay eat dine woud highly recommend kaspers restaurant wait return Nagelkerke (2020b) suggests another route. You’ve already removed stop words, so you needn’t worry about the over-used words. The TF-IDF approach was developed for long documents. Smaller documents like online reviews have little TF variation (most words are used only once or twice in a review), and the IDF ends up dominating. Nagelkerke suggests using the overall word frequency instead. In the end, you need to experiment to find the right cutoff. The elbow in the TD-IDF plot below is around .2. Using that as a threshold would throw out about about 90% of the vocabulary. The corpus frequency plot after that has an elbow around 5 occurrences. Using that as a threshold would retain about 20% of the vocabulary. lda_word_stats &lt;- lda_train %&gt;% count(review_id, word, name = &quot;doc_freq&quot;) %&gt;% bind_tf_idf(word, review_id, doc_freq) %&gt;% mutate(.by = word, corp_freq = sum(doc_freq)) %&gt;% mutate(corp_pct = corp_freq / sum(doc_freq)) lda_word_stats %&gt;% mutate(tf_idf_bin = cut(tf_idf, breaks = 50)) %&gt;% summarize(.by = tf_idf_bin, vocab = n_distinct(word)) %&gt;% arrange(tf_idf_bin) %&gt;% mutate(pct = vocab / sum(vocab), cumpct = cumsum(pct)) %&gt;% ggplot(aes(x = tf_idf_bin)) + geom_col(aes(y = pct)) + geom_line(aes(y = cumpct, group = 1)) + geom_vline(xintercept = 9, linetype = 2) + labs(y = &quot;vocabulary&quot;, title = &quot;TF-IDF Method&quot;) + scale_y_continuous(breaks = seq(0, 1, .1), labels = percent_format(1)) + theme(axis.text.x = element_text(angle = 90, vjust = .5)) lda_word_stats %&gt;% mutate( corp_freq_bin = if_else(corp_freq &gt; 19, &quot;20+&quot;, as.character(corp_freq)), corp_freq_bin = factor(corp_freq_bin, levels = c(as.character(1:19), &quot;20+&quot;)) ) %&gt;% # mutate(corp_pct_bin = cut(corp_pct, breaks = 100)) %&gt;% summarize(.by = corp_freq_bin, vocab = n_distinct(word)) %&gt;% arrange(corp_freq_bin) %&gt;% mutate(pct = vocab / sum(vocab), cumpct = cumsum(pct)) %&gt;% ggplot(aes(x = corp_freq_bin, y = cumpct)) + geom_col(aes(y = pct)) + geom_line(aes(y = cumpct, group = 1)) + geom_vline(xintercept = 5, linetype = 2) + labs(y = &quot;vocabulary&quot;, title = &quot;Corpus Frequency Method&quot;) + scale_y_continuous(breaks = seq(0, 1, .1), labels = percent_format(1)) + theme(axis.text.x = element_text(angle = 90, vjust = .5)) Below is the DTM associated with both methods. The TF-IDF method removed half the documents while retaining 11K terms. The corpus frequency method retained all of the documents while reducing the vocabulary to 6K terms. Corpus frequency does indeed seem superior in this case, so I’ll move forward with it. (lda_dtm_tfidf &lt;- lda_word_stats %&gt;% filter(tf_idf &gt; .2) %&gt;% cast_dtm(document = review_id, term = word, value = doc_freq)) ## &lt;&lt;DocumentTermMatrix (documents: 5623, terms: 10922)&gt;&gt; ## Non-/sparse entries: 15782/61398624 ## Sparsity : 100% ## Maximal term length: 37 ## Weighting : term frequency (tf) (lda_dtm_corpfreq &lt;- lda_word_stats %&gt;% filter(corp_freq &gt; 5) %&gt;% cast_dtm(document = review_id, term = word, value = doc_freq)) ## &lt;&lt;DocumentTermMatrix (documents: 12121, terms: 5821)&gt;&gt; ## Non-/sparse entries: 606488/69949853 ## Sparsity : 99% ## Maximal term length: 16 ## Weighting : term frequency (tf) lda_dtm &lt;- lda_dtm_corpfreq The pre-processing step sure pared down the corpus. The high frequency terms comprise only 20% of the vocabulary, but are still 95% of the total word usage. bind_rows( `high freq words` = lda_word_stats %&gt;% filter(corp_freq &gt; 5) %&gt;% summarize(distinct_words = n_distinct(word), total_words = sum(doc_freq)), `low freq words` = lda_word_stats %&gt;% filter(corp_freq &lt;= 5) %&gt;% summarize(distinct_words = n_distinct(word), total_words = sum(doc_freq)), .id = &quot;partition&quot; ) %&gt;% mutate(total_pct = total_words / sum(total_words) * 100, distinct_pct = distinct_words / sum(distinct_words) * 100) %&gt;% select(partition, distinct_words, distinct_pct, total_words, total_pct) %&gt;% janitor::adorn_totals() ## partition distinct_words distinct_pct total_words total_pct ## high freq words 5821 20.18377 723170 95.396707 ## low freq words 23019 79.81623 34896 4.603293 ## Total 28840 100.00000 758066 100.000000 Fit There are several parameters you might tweak for the model fit. The biggest surprise is that you set the number of topics, k. In general, you only want as many topics as are clearly distinct and that you can easily communicate to others. knowledgeR explains the harmonic mean method for optimization, but Nagelkerke (2020b) suggests sticking with the art vs science approach and picking your own k. Another option is to use the perplexity statistic to help identify a good value for k. Perplexity is a measure of how well a probability model fits a new set of data. Look for the elbow in a scree plot. The code below does that, but it runs forever, especially as you get to higher values of k. I abandoned it. set.seed(12345) train_ind &lt;- sample(nrow(lda_dtm), floor(0.75*nrow(lda_dtm))) k_train &lt;- lda_dtm[train_ind, ] k_test &lt;- lda_dtm[-train_ind, ] k = c(seq(from = 2, to = 5, by = 1)) perp &lt;- k %&gt;% map(~ LDA(k_train, k = .x)) %&gt;% map(~ perplexity(.x, newdata = k_test)) %&gt;% as.numeric() data.frame(k = k, perplexity = perp) %&gt;% ggplot(aes(x = k, y = perplexity)) + geom_point() + geom_smooth(method = &quot;loess&quot;, se = FALSE) + labs(title = &quot;Perplexity Plot for LDM model&quot;) I will stick with a simple k = 3 model. The model fit took a minute to run, so I ran it once then saved the result. lda_fit &lt;- LDA(lda_dtm, k = 3) saveRDS(lda_fit, file = &quot;input/lda_fit.RDS&quot;) lda_fit &lt;- readRDS(file = &quot;input/lda_fit.RDS&quot;) The fitted object contains two matrices. The beta (aka “phi”) matrix is the distribution of tokens (cols) over topics (rows). The gamma (aka “theta”) matrix is the distribution of documents (rows) over topics (cols). The row sum is 1 for each matrix (sum of topic probabilities, some of document probabilities). lda_beta_mtrx &lt;- posterior(lda_fit) %&gt;% pluck(&quot;terms&quot;) %&gt;% as.matrix() # One row per topic, one col per token. dim(lda_beta_mtrx) ## [1] 3 5821 # Word probability distribution sums to 1 for each topic. sum(lda_beta_mtrx[1, ]) ## [1] 1 lda_gamma_mtrx &lt;- posterior(lda_fit) %&gt;% pluck(&quot;topics&quot;) %&gt;% as.matrix() # One row per document, one col per topic dim(lda_gamma_mtrx) ## [1] 12121 3 # Topic probability distribution sums to 1 for each document. sum(lda_gamma_mtrx[1, ]) ## [1] 1 tidytext::tidy() pivots the beta matrix into a [topic, term, beta] data frame. lda_beta &lt;- tidy(lda_fit, matrix = &quot;beta&quot;) lda_top_tokens &lt;- lda_beta %&gt;% mutate(topic = factor(paste(&quot;Topic&quot;, topic))) %&gt;% group_by(topic) %&gt;% slice_max(order_by = beta, n = 10) %&gt;% ungroup() lda_top_tokens %&gt;% ggplot(aes(x = beta, y = reorder_within(term, by = beta, within = topic))) + geom_col() + scale_y_reordered() + facet_wrap(facets = vars(topic), scales = &quot;free_y&quot;) + labs(y = NULL, title = &quot;LDA Top 10 Terms&quot;) Word clouds tell you more or less the same thing. colors6 &lt;- RColorBrewer::brewer.pal(n = 3, name = &quot;Set2&quot;) x &lt;- map( c(1:3), ~ with(lda_beta %&gt;% filter(topic == .x), wordcloud::wordcloud( term, beta, max.words = 20, colors = colors6[.x] )) ) There is a downside to this evaluation. Popular words like “room” and “stay” appear at or near the top in all three topics. You might want to look at relative popularity instead: the popularity within the topic divided by overall popularity. That’s problematic too because words that only appear in few reviews will pop to the top. What you want is a combination of both absolute term probability and relative term probability. LDAvis::serVis() can help you do that. Unfortunately, the plot from LDAvis::serVis() is interactive and does not render in the RMarkdown notebook html, so below is just a screenshot of the code chunk output. The left side shows the topic sizes (documents) and topic distances. The right side shows the most important tokens. # word count for each document doc_length &lt;- lda_word_stats %&gt;% filter(corp_freq &gt; 5) %&gt;% summarize(.by = review_id, n = sum(doc_freq)) %&gt;% pull(n) # vocabulary: unique tokens vocab &lt;- colnames(lda_beta_mtrx) # overall token frequency term_frequency &lt;- lda_word_stats %&gt;% filter(corp_freq &gt; 5) %&gt;% summarize(.by = word, n = sum(doc_freq)) %&gt;% arrange(match(word, vocab)) %&gt;% pull(n) # create JSON containing all needed elements json &lt;- LDAvis::createJSON(lda_beta_mtrx, lda_gamma_mtrx, doc_length, vocab, term_frequency) LDAvis::serVis(json) The gamma matrix shows topic distributions. You can use it to see if topics vary by a covariate. tidy(lda_fit, matrix = &quot;gamma&quot;) %&gt;% mutate(document = as.numeric(document), topic = factor(topic)) %&gt;% inner_join(prepped_hotel, by = join_by(document == review_id)) %&gt;% summarize(.by = c(hotel, topic), gamma = mean(gamma)) %&gt;% ggplot(aes(x = gamma, y = fct_rev(hotel), fill = topic)) + geom_col() + labs(y = NULL, title = &quot;LDA Topic Distribution&quot;) Iterate through the model by tweaking k, excluding words that dominate and suppress the more interesting subdomains, and/or changing the minimal token frequency to focus on more/less dominant tokens. You can also change the document sampling strategy to promote interesting domains, like we did when we over sampled the low hotel ratings. Topic Labeling with ChatGPT To OpenAI’s ChatGPT API service requires an API token. I created one at https://platform.openai.com/api-keys and saved it to .Renviron. See usethis::edit_r_environ(). # Create a function to send each list of topic words to Open AI as a separate request. get_topic_from_openai &lt;- function(prompt) { my_resp &lt;- request(&quot;https://api.openai.com/v1/chat/completions&quot;) %&gt;% req_headers(Authorization = paste(&quot;Bearer&quot;, Sys.getenv(&quot;OPENAI_API_KEY&quot;))) %&gt;% req_body_json(list( model = &quot;gpt-3.5-turbo&quot;, # Temperature [0,2] controls creativity, predictable -&gt; variable. temperature = 1, messages = list( # System prompt sets repeated context. It is prefixed to prompts. list( role = &quot;system&quot;, content = paste(&quot;You are a topic modeling assistant. You accept lists &quot;, &quot;of words in a topic and summarizes them into a salient &quot;, &quot;topic label of five words or less. How would you &quot;, &quot;summarize the following list? The list is in descending &quot;, &quot;order of importance, so the first term in the list is most &quot;, &quot;strongly tied to the topic. Return just the topic label and &quot;, &quot;nothing else.&quot;) ), list( role = &quot;user&quot;, content = prompt ) ) )) %&gt;% req_perform() %&gt;% resp_body_json() %&gt;% pluck(&quot;choices&quot;, 1, &quot;message&quot;, &quot;content&quot;) } lda_topics &lt;- lda_top_tokens %&gt;% nest(data = term, .by = topic) %&gt;% mutate( token_str = map(data, ~paste(.$term, collapse = &quot;, &quot;)), topic_lbl = map_chr(token_str, get_topic_from_openai), topic_lbl = str_remove_all(topic_lbl, &#39;\\\\&quot;&#39;), topic_lbl = snakecase::to_any_case(topic_lbl, &quot;title&quot;) ) %&gt;% select(-data) # Save to file system to avoid regenerating. saveRDS(lda_topics, file = &quot;input/lda_topics.RDS&quot;) lda_topics &lt;- readRDS(file = &quot;input/lda_topics.RDS&quot;) Let’s see the topic summary with the newly generated labels. lda_top_tokens %&gt;% inner_join(lda_topics, by = join_by(topic)) %&gt;% mutate(topic_lbl = str_wrap(topic_lbl, 25)) %&gt;% ggplot(aes(x = beta, y = reorder_within(term, by = beta, within = topic_lbl))) + geom_col() + scale_y_reordered() + facet_wrap(facets = vars(topic_lbl), scales = &quot;free_y&quot;) + labs(y = NULL, title = &quot;LDA Top 10 Terms&quot;) TODO I still need to learn more about Held-out Likelihood (Wallach et al., 2009). Semantic Coherence. The coherence measure evaluates topics. Exclusivity. Generally, the greater the number of topics in a model, the lower the quality of the smallest topics. One way around this is hiding the low-quality topics. References "],["stm.html", "2.2 STM", " 2.2 STM STM incorporates arbitrary document metadata into the topic model. The goal of STM is to discover topics and estimate their relationship to the metadata. Topical prevalence. If what topics are discussed depends on the metadata features, control for them in the prevalence (the gamma matrix). E.g., negative hotel reviews might focus on different topics than positive reviews. Topical content. If how a topic is discussed depends on metadata features, control for them in the content (the beta matrix). E.g., visitors from the US may discuss hotels differently than visitors from the UK. Algorithm STM is similar to LDA in that it assumes each document is created by a generative process where topics are included according to probabilities (topical prevalence) and words are included in the topics (topical content) according to probabilities. STM adds the possibility of including topical prevalence covariates, and topical content covariates. Data Preparation Chapter 1 prepped the data by correcting misspellings, lemmatizing words, and removing stop words The stm package represents a text corpus as an object with three components: a sparse matrix of counts by document and vocabulary word vector index, the vocabulary word vector, and document metadata. I used STM for my Battle of the Bands project. stm::textProcessor() is essentially a wrapper around the tm package. It produces a list object with three main components: vocab, a named vocabulary vector, one element per distinct word. documents, a list of matrices, one per document. Each matrix has 2 rows of integers. The first row is indices from the vocabulary vector; the second is their associated word counts. This is a concise representation of a document term matrix. The processing step sometimes removes a few documents if they are empty after removing stopwords, numbers, est. meta, a metadata data frame, one row per document containing the feature cols. This step took about 3 minutes to run, so I ran it once then saved the result. stm_processed &lt;- stm::textProcessor( documents = prepped_hotel$review_words, metadata = prepped_hotel %&gt;% select(rating, reviewer_loc, review), lowercase = FALSE, removestopwords = FALSE, removenumbers = FALSE, removepunctuation = FALSE, stem = FALSE ) saveRDS(stm_processed, file = &quot;input/stm_processed.RDS&quot;) stm_processed &lt;- readRDS(file = &quot;input/stm_processed.RDS&quot;) After processing, prepare the corpus by removing infrequently used words. stm::prepDocuments() removes infrequently appearing words, and removes any documents that contain no words after processing and removing words. 1% (about 230) is a conservative threshold. The plot below shows that removing even a few words will remove some documents, but you can still retain most document plotRemoved(stm_processed$documents, lower.thresh = seq(100, 4000, by = 100)) stm_prepared &lt;- stm::prepDocuments( stm_processed$documents, stm_processed$vocab, stm_processed$meta, lower.thresh = length(stm_processed$documents) * .01 ) ## Removing 26136 of 26886 terms (239369 of 943763 tokens) due to frequency ## Removing 21 Documents with No Words ## Your corpus now has 23352 documents, 750 terms and 704394 tokens. Fit The stm package allows you to either specify the number of topics (K) to identify, or it can choose an optimal number by setting parameter K = 0. The resulting probability distribution of topic words (beta matrix) will be a K x rlength(stm_prepared$vocab)matrix. The probability distribution of topics (gamma matrix, theta in the stm package) will be a 23,352 x K matrix. I expect topics to correlate with the review rating, soratingis a prevalence covariate, and I expect word usage to correlate with the reviewer location, soreviewer_loc` is a topical content covariate. Running the model with K = 3 threw an error! Error: chol(): decomposition failed I set it to K = 4 and it worked. It took a couple minutes to run, so I ran it once then saved the result. set.seed(1234) stm_fit &lt;- stm::stm( stm_prepared$documents, stm_prepared$vocab, K = 4, prevalence = ~ rating, content = ~ reviewer_loc, data = stm_prepared$meta, init.type = &quot;Spectral&quot;, verbose = FALSE ) saveRDS(stm_fit, file = &quot;input/stm_fit.RDS&quot;) stm_fit &lt;- readRDS(file = &quot;input/stm_fit.RDS&quot;) stm_heldout &lt;- stm::make.heldout(stm_prepared$documents, vocab = stm_prepared$vocab) stm_fit2 &lt;- stm::stm( stm_heldout$documents, stm_heldout$vocab, K = 4, prevalence = ~ rating, content = ~ reviewer_loc, data = stm_prepared$meta, init.type = &quot;Spectral&quot;, verbose = FALSE ) # stm_fit2 %&gt;% stm::exclusivity() # stm_fit2 %&gt;% stm::semanticCoherence(documents = stm_heldout$documents) Interpret The fit summary has three sections showing the tops words. The first section shows the prevalence model; the second shows the topical content model; and the third shows their interaction. summary(stm_fit) ## A topic model with 4 topics, 23352 documents and a 750 word dictionary. ## Topic Words: ## Topic 1: favorite, notch, corinthia, awesome, property, exceed, mandarin ## Topic 2: neighborhood, convenient, victoria, heathrow, ride, underground, rembrandt ## Topic 3: promenade, beaufort, celebration, celebrate, savoy, surrounding, american ## Topic 4: closet, clothe, move, desk, smell, light, tub ## ## Covariate Words: ## Group Other: renovation, smile, renovate, direct, miss, europe, directly ## Group United Kingdom: partner, whilst, saturday, sunday, party, fab, round ## Group United States: hotel&#39;s, spectacular, elegant, accommodate, perfection, tate, renovation ## ## Topic-Covariate Interactions: ## Topic 1, Group Other: bridge, theater, boutique, blackfriars, lane, modern, pool ## Topic 1, Group United Kingdom: wed, fault, trouble, anniversary, brilliant, penny, faultless ## Topic 1, Group United States: exquisite, club, square, trafalgar, city, gorgeous, tate ## ## Topic 2, Group Other: british, share, block, host, store, ridgemount, advice ## Topic 2, Group United Kingdom: apex, appoint, comfy, blackfriars, toiletry, spacious, club ## Topic 2, Group United States: theater, phone, wonderful, tour, bridge, express, block ## ## Topic 3, Group Other: wed, anniversary, fab, attend, partner, wow, round ## Topic 3, Group United Kingdom: daughter, cake, rush, pianist, ritz, birthday, pass ## Topic 3, Group United States: classic, housekeeping, amenity, kid, speak, miss, level ## ## Topic 4, Group Other: club, pillow, executive, cold, fall, type, fan ## Topic 4, Group United Kingdom: lift, downstairs, miss, rumpus, bottle, car, complimentary ## Topic 4, Group United States: stylish, feature, renovation, black, elevator, boutique, smoke ## If this was just a regular topic model, or a prevalence or content model, we’d see top words by 4 metrics: highest probability, FREX, lift, and score. Highest probability weights words by their overall frequency. FREX weights words by their overall frequency and how exclusive they are to the topic. Lift weights words by dividing by their frequency in other topics, therefore giving higher weight to words that appear less frequently in other topics. Score divides the log frequency of the word in the topic by the log frequency of the word in other topics. Let’s fit a new model just to show that. set.seed(1234) stm_fit_simple &lt;- stm::stm( stm_prepared$documents, stm_prepared$vocab, K = 4, # prevalence = ~ rating, # content = ~ reviewer_loc, data = stm_prepared$meta, init.type = &quot;Spectral&quot;, verbose = FALSE ) saveRDS(stm_fit_simple, file = &quot;input/stm_fit_simple.RDS&quot;) stm_fit_simple &lt;- readRDS(file = &quot;input/stm_fit_simple.RDS&quot;) stm::labelTopics(stm_fit_simple) ## Topic 1 Top Words: ## Highest Prob: stay, london, staff, service, excellent, restaurant, location ## FREX: spa, corinthia, concierge, luxury, mondrian, love, pool ## Lift: personal, oriental, mandarin, notch, corinthia, pleasure, property ## Score: personal, london, service, stay, spa, corinthia, location ## Topic 2 Top Words: ## Highest Prob: breakfast, walk, room, stay, location, clean, london ## FREX: tube, station, museum, hyde, street, bus, walk ## Lift: hyde, paddington, albert, ridgemount, bus, kensington, rhodes ## Score: hyde, tube, station, museum, bus, walk, clean ## Topic 3 Top Words: ## Highest Prob: savoy, bar, tea, lovely, staff, special, birthday ## FREX: afternoon, birthday, savoy, cocktail, cake, american, treat ## Lift: scone, pianist, cake, piano, beaufort, afternoon, sandwich ## Score: scone, savoy, afternoon, birthday, tea, cake, cocktail ## Topic 4 Top Words: ## Highest Prob: room, night, check, stay, bed, book, bathroom ## FREX: check, charge, bath, call, floor, pay, issue ## Lift: rumpus, robe, mirror, wake, smell, curtain, corridor ## Score: rumpus, room, check, shower, bed, bathroom, floor It is interesting that the top terms for UK did not include “restaurant” or “location”. The top terms for the US did not include “excellent” or “amaze”, but did include “love”. stm_tidy &lt;- tidy(stm_fit) stm_top_tokens &lt;- stm_tidy %&gt;% mutate(topic = factor(paste(&quot;Topic&quot;, topic))) %&gt;% group_by(topic, y.level) %&gt;% slice_max(order_by = beta, n = 10) %&gt;% ungroup() stm_top_tokens %&gt;% filter(topic == &quot;Topic 1&quot;) %&gt;% ggplot(aes(x = beta, y = reorder_within(term, by = beta, within = topic))) + geom_col() + scale_y_reordered() + facet_wrap(facets = vars(y.level), scales = &quot;free_x&quot;, ncol = 3) + labs(y = NULL, title = &quot;STM Top 10 Terms for Topic 1&quot;) As we did with the LDA model, we can assign topic labels with Open AI. stm_topics &lt;- stm_top_tokens %&gt;% nest(data = term, .by = topic) %&gt;% mutate( token_str = map(data, ~paste(.$term, collapse = &quot;, &quot;)), topic_lbl = map_chr(token_str, get_topic_from_openai), topic_lbl = str_remove_all(topic_lbl, &#39;\\\\&quot;&#39;), topic_lbl = snakecase::to_any_case(topic_lbl, &quot;title&quot;) ) %&gt;% select(-data) # Save to file system to avoid regenerating. saveRDS(stm_topics, file = &quot;input/stm_topics.RDS&quot;) stm_topics &lt;- readRDS(file = &quot;input/stm_topics.RDS&quot;) stm_topics ## # A tibble: 4 × 3 ## topic token_str topic_lbl ## &lt;fct&gt; &lt;list&gt; &lt;chr&gt; ## 1 Topic 1 &lt;chr [1]&gt; Luxury Stay in London ## 2 Topic 2 &lt;chr [1]&gt; Comfortable Stay Near London ## 3 Topic 3 &lt;chr [1]&gt; Afternoon Tea at the Savoy ## 4 Topic 4 &lt;chr [1]&gt; Hotel Stay Another way to evaluate the model is to print reviews that are most representative of the topic. Topic 1 stm::findThoughts( stm_fit, n = 3, texts = stm_prepared$meta$review, topics = 1, meta = stm_prepared$meta ) ## ## Topic 1: ## To those who have stayed here in the past and have said anything negative of this hotel and services are foolish beyond belief. This hotel receives a five star plus recognition in my book of first class travels. It is one on my top ten list of first class hotel experiences. In fact I would confidently say that this hotel and it&#39;s staff ranks as the top three luxury hotel experiences that I have had so far. First and foremost I must say that the Fine Hotels and Resorts Pkg. is well worth it at this hotel. They truly went above and beyond to recognize our patronage. Second, this hotel went above and beyond to recognize our anniversary. Third, this hotel boasts the most superior, thoughtful and careful staff service that I have ever experienced in a first class hotel. While the hotel itself has an old world charm to it, it never underestimates the luxury offerings for it&#39;s guests. There is absolutely nothing that I can find fault during our week stay at is hotel. I would highly recommend this hotel for anyone looking for luxury and service. If in London again, I would seriously consider staying here again. I must be honest when I say that two years ago I tried the Mandarin hotel chain for my very first time in NYC and was disappointed. Since then I have tried the Mandarin Las Vegas and now London and both times I have been very impressed. I am confident to say that Mandarin Oriental Hotels is now my new favorite first class hotel chain. If you like first class hotels and luxury this hotel is for you. Enjoy it as much that I did. Bravo to the entire management team at the Mandarin Hyde Park London for maintaining such an impeccable property and focus on customer satisfaction. Every staff member at this hotel must receive a huge appreciation and thanks on behalf of our entire family for giving us a ver memorable and stellar holiday stay. Thank you! ## This was our third trip, the Corinthia London is a perfect 5 star and could be rated higher, the service is impeccable which is sometimes difficult to find in London, the food is fantastic and all the hotel has to offer is top notch with an a beautiful Spa in a prime location, it actually has everything you could want, the rooms and suites are beautifully appointed and very comfortable, it is currently our favourite hotel in central London and we will back and recommend to all, it&#39;s also very family friendly as well as being an ideal romantic stay or a luxurious base to experience the city, world class! ## I stayed at the Corinthia for three nights during a business trip and REALLY liked this hotel. I&#39;ve stayed at a number of other luxury 5-star properties in London and this is by far my favorite. The rooms are beautiful and well appointed and the spa is just amazing. More importantly, the service is outstanding and it feels like everyone really cares and is genuinely concerned about making sure you are enjoying your visit. Highly recommend staying here. "],["sentimentanalysis.html", "Chapter 3 Sentiment Analysis", " Chapter 3 Sentiment Analysis Sentiment analysis is the extraction of the emotional intent of text. You can classify the polarity (positive | negative) or sentiment (angry | sad | happy | …) at the document, sentence, or feature level. This section continues with the hotel data from 1. load(&quot;input/hotel_prepped.Rdata&quot;) hotel_0 &lt;- prepped_hotel glimpse(token) ## Rows: 1,136,694 ## Columns: 2 ## $ review_id &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … ## $ word &lt;chr&gt; &quot;pleasure&quot;, &quot;stay&quot;, &quot;7&quot;, &quot;night&quot;, &quot;recently&quot;, &quot;perfect&quot;, &quot;co… "],["subjectivity-lexicons.html", "3.1 Subjectivity Lexicons", " 3.1 Subjectivity Lexicons A subjectivity lexicon is a predefined list of words associated with emotional context such as positive/negative. Subjectivity lexicons are typically short (a few thousand words), but work because of Zipf’s law which holds that the nth-ranked item in a frequency table has a frequency count equal to 1/n of the top-ranked item. So infrequently used words are used very infrequently. There are three common sentiment lexicons. Bing is common for polarity scoring, AFINN for emotion classification. NRC is a less common option for emotion classification. Bing classifies words as positive or negative. bing &lt;- tidytext::get_sentiments(&quot;bing&quot;) %&gt;% # remove dups filter(!word %in% c(&quot;envious&quot;, &quot;enviously&quot;, &quot;enviousness&quot;)) bing %&gt;% count(sentiment) %&gt;% adorn_totals() %&gt;% flextable::flextable() %&gt;% flextable::autofit() .cl-36fd9d90{}.cl-36f72780{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-36f7279e{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-36f9f49c{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-36f9f4ba{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-36fa0a22{width:0.941in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-36fa0a23{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-36fa0a2c{width:0.941in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-36fa0a36{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-36fa0a40{width:0.941in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-36fa0a41{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-36fa0a4a{width:0.941in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-36fa0a4b{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}sentimentnnegative4,778positive2,002Total6,780 AFINN, by Finn Arup Nielsen, associates words with a manually rated valence integer between -5 (negative) and +5 (positive). afinn &lt;- tidytext::get_sentiments(&quot;afinn&quot;) afinn %&gt;% count(value) %&gt;% adorn_totals() %&gt;% flextable::flextable() %&gt;% flextable::autofit() .cl-3726623e{}.cl-37206ca8{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-37206cb2{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-3722c39a{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3722c3a4{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3722d1c8{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3722d1d2{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3722d1dc{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3722d1dd{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3722d1e6{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3722d1e7{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3722d1f0{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3722d1fa{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3722d204{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3722d205{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3722d206{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3722d20e{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3722d20f{width:0.64in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3722d218{width:0.633in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}valuen-516-443-3264-2966-13090112082448317244555Total2,477 NRC lexicon associates words with eight emotions corresponding to the second level of Plutchik’s Wheel of Emotions and two sentiments (negative and positive). NRC was created by manual annotation on a crowd sourcing platform (see this). nrc &lt;- tidytext::get_sentiments(&quot;nrc&quot;) nrc %&gt;% count(sentiment) %&gt;% adorn_totals() %&gt;% flextable::flextable() %&gt;% flextable::autofit() .cl-373b1454{}.cl-37351108{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-37351112{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-37377510{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-37377524{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-37378712{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3737871c{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3737871d{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-37378726{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-37378727{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-37378730{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-37378731{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3737873a{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3737873b{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-37378744{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3737874e{width:0.988in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3737874f{width:0.71in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}sentimentnanger1,245anticipation837disgust1,056fear1,474joy687negative3,316positive2,308sadness1,187surprise532trust1,230Total13,872 "],["polarity-scoring.html", "3.2 Polarity Scoring", " 3.2 Polarity Scoring Two packages measure text polarity. The simpler one tidytext: unnest tokens, join to the Bing lexicon, and calculate the net of positive minus negative polarity counts. sentimentr is more sophisticated in that it takes into account valence shifters, surrounding words that change the intensity of a sentiment (e.g., “very”) or switch its direction (e.g., “not”).2 tidytext The tidytext way to score polarity is to tag words as “positive” and “negative” using the Bing lexicon, then calculate the difference in counts. The qdap and sentimentr packages correct for text length by dividing by \\(\\sqrt{n}\\). It is useful to capture the positive and negative words back in the main data frame for explaining how the polarity score was calculated. polarity_bing &lt;- token %&gt;% left_join(bing, by = &quot;word&quot;, relationship = &quot;many-to-one&quot;) %&gt;% summarize(.by = c(review_id, sentiment), n = n(), words = list(word)) %&gt;% pivot_wider(names_from = sentiment, values_from = c(n, words), values_fill = list(n = 0)) %&gt;% select(-c(n_NA, words_NA)) %&gt;% inner_join(hotel_0 %&gt;% select(review_id, prepped_wrdcnt), by = &quot;review_id&quot;) %&gt;% mutate( polarity = (n_positive - n_negative) / sqrt(prepped_wrdcnt), polarity_desc = if_else(polarity &gt;= 0, &quot;Positive&quot;, &quot;Negative&quot;) ) polarity_afinn &lt;- token %&gt;% inner_join(afinn, by = &quot;word&quot;, relationship = &quot;many-to-one&quot;) %&gt;% summarize(.by = review_id, sentiment = sum(value), words = list(word)) %&gt;% inner_join(hotel_0 %&gt;% select(review_id, prepped_wrdcnt), by = &quot;review_id&quot;) %&gt;% mutate( polarity = sentiment / sqrt(prepped_wrdcnt), polarity_desc = if_else(polarity &gt;= 0, &quot;Positive&quot;, &quot;Negative&quot;) ) # Attach to main data frame hotel_1 &lt;- hotel_0 %&gt;% left_join(polarity_bing %&gt;% select(review_id, polarity, words_positive, words_negative) %&gt;% rename_with(~paste0(&quot;bing_&quot;, .x)), by = join_by(review_id == bing_review_id)) %&gt;% left_join(polarity_afinn %&gt;% select(review_id, polarity) %&gt;% rename_with(~paste0(&quot;afinn_&quot;, .x)), by = join_by(review_id == afinn_review_id)) Let’s see how the polarity scores compare. hotel_1 %&gt;% pivot_longer(cols = c(bing_polarity, afinn_polarity), names_to = &quot;lexicon&quot;, values_to = &quot;polarity&quot;) %&gt;% filter(!is.na(polarity)) %&gt;% ggplot(aes(x = polarity, y = fct_rev(hotel))) + geom_boxplot() + facet_wrap(facets = vars(lexicon)) + labs(title = &quot;Review polarity&quot;, x = NULL, y = NULL, caption = glue::glue(&quot;Bing Polarity = (n_pos - n_neg) / sqrt(n_words)\\n&quot;, &quot;AFINN Polarity = sentiment / sqrt(n_words)&quot;)) The two lexicons are similar. The data set includes a rating (1-5). I’ll stick with Bing going forward for convenience. The polarity score should correlate with the rating. hotel_1 %&gt;% filter(!is.na(bing_polarity)) %&gt;% ggplot(aes(x = as_factor(rating), y = bing_polarity)) + geom_jitter(width = 0.2, alpha = 0.3, color = &quot;#5DA5DA&quot;, size = 1) + geom_boxplot(alpha = 0) + theme_minimal() + labs(title = &quot;Polarity is associated with overall Likert score&quot;, x = &quot;Overall Likert Rating&quot;, y = &quot;Polarity Score&quot;) Sentiment increases with Likert rating, but there are many reviews with a rating of 5 and a polarity score &lt;0. In some cases this is because the reviewer interpreted the scale incorrectly. You can use polarity scores to identify problematic reviews like these. hotel_1 %&gt;% mutate( problematic = case_when( (rating == &quot;1-2&quot; &amp; bing_polarity &gt; 0.5) ~ &quot;Too Low&quot;, (rating == &quot;5&quot; &amp; bing_polarity &lt; -.5) ~ &quot;Too High&quot;, TRUE ~ &quot;Other&quot; ) ) %&gt;% filter(problematic %in% c(&quot;Too High&quot;, &quot;Too Low&quot;)) %&gt;% group_by(problematic) %&gt;% slice_max(order_by = abs(bing_polarity), n = 1) %&gt;% select(problematic, rating, bing_polarity, review) %&gt;% flextable::flextable() %&gt;% flextable::autofit() %&gt;% flextable::valign(valign = &quot;top&quot;) .cl-391541be{}.cl-390f70ea{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-390f70f4{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-3911c368{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3911c372{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3911c373{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3911c37c{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3911d128{width:1.072in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3911d132{width:0.671in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3911d133{width:1.157in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3911d13c{width:389.236in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3911d146{width:1.072in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3911d147{width:0.671in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3911d150{width:1.157in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3911d151{width:389.236in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3911d15a{width:1.072in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3911d15b{width:0.671in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3911d164{width:1.157in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3911d165{width:389.236in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}problematicratingbing_polarityreviewToo High5-1.278019Some design faults in the bathroom no stool misplaced grab handles and vanity mirror Very disappointing experience in Savoy Grill Good quality ingredients but poorly presented and tasteless Numerous mistakes in service including charging for expensive drinks which we did not have Service charge revoked and booking for following night cancelledToo Low1-21.774014Having stayed at several Morgans Hotels on both sides of the pond I was really looking forward to checking in to the Mondrian London I have to say I was bitterly disappointed and left questioning whether this hotel was actually a Morgans property Normally the second you step into any Morgans property you instantly know youre in a Morgans Hotel They always have the wow factor amp the dcor has such a quirky distinctive style and the service is always exemplary without being stuffy The same cannot be said about Mondrian London we could have been in any generic highend hotel in the world and some of the staff behaviour we witnessed was highly substandard The room was nice but nice is not a word I would normally use to describe a Morgans room its usually amazing iconic incredible It was stylish but had no soul amp dcor wise we could have been in a room in any hotel in the world However the view of the Thames amp St Pauls was fantastic and was a redemptive feature amp the large marble bathroom which is synonymous with Morgans was amazing amp without doubt the highlight of the room The bed was enormous but was so hard it was like sleeping on breeze blocks possibly one of the worst nights sleep Ive ever had had to get up in the night amp go amp sleep on the couch not really what you expect from a 500 room We had aperitifs in the Dandelyan bar again the dcor is completely unremarkable nothing special when compared to the likes of the groups other bars but was comfortable enough We had really high hopes for this bar as it is apparently run by an award winning mixologist so we couldnt wait to sample to cocktails and wait we did while we pored over the enormous incomprehensible menu It was more like a botanical reference book than a cocktail menu gargantuan in size and too wordy with no comprehensible flow People know what they like to drink either in terms of the spirit base of their cocktail vodka rum gin whiskey or the type of drink they prefer martini flute long etc therefore it is a sensible idea to arrange menus in a way that people can easily access the information they are looking for as most high end places do We got so fed up of trying to look for something suitable that we gave up amp ordered a glass of Champagne which was served warm amp flat Quite ironic that these supposed award winning bartenders werent even able to get a glass of Champagne right But the waitress not sure of her name but she was of an eastern heritage was very welcoming and friendly with a constant smile so made up for the poor drinks We went through to dinner and it was fantastic the food was all executed perfectly amp really tasty amp well presented Service was also very good but we found the waiter to be quite formal and stiff which was quite surprising in a Morgans He was very efficient and slick but wasnt very engaging and definitely needed to smile more The dcor of the restaurant was very elegant and had great views across the Thames There was a clear managerial presence on the floor which always instils confidence in you as a diner The Irish manager checked on our table and the other tables around us a touch that was very much appreciated Breakfast was also excellent there I would wholeheartedly recommend the Sea Containers restaurant as a standalone venue We had drinks up at the Rumpus Room amp were welcomed by a very colourful character who instantly put a smile on our faces amp gave us a very warm welcome not sure if he was the manager or host gentleman with a beard amp earrings Again the bar was nice dcor wise it had a classy feel to it amp the views across the Thames were spectacular This time we opted to sit at the bar so we could speak to the bartenders about the drinks amp avoid another negative experience like we had in Dandelyan We were served by great bartender cant recall his name blue eyed chap who I believe told us he was from Hungary He was friendly amp chatty amp made us feel welcome without being intrusive He made us some bespoke cocktails based on our preferences which were exquisite Faith restored we opted to order the next round from the menu big mistake they pre make their cocktails in batches amp literally just pour it out of a bottle onto ice You may expect this kind of cutting corners from a highvolume lowend venue but we certainly did not expect this from a supposed highend cocktail bar in a luxury hotel When you are paying around 15 for a cocktail plus service charge you expect your drinks to be freshly made amp for there to be a bit more service than simply opening a bottle I couldnt believe what I was seeing The bartender made us some more bespoke cocktails instead which again were great if you visit this bar ask for the bartenders to make you something based on your likes amp avoid their menu We moved to a table amp had several different waitresses check on us amp serve us all of whom were friendly amp attentive The ambience was good in the bar amp it is great for people watching but one thing we did notice which we thought was highly inappropriate was the lady with the short hair amp American accent not sure what her role was but she seemed to work there as went behind the bar amp was talking well shouting across the room actually to all the other staff openly imbibing in the bar In our experience of high end hotels the staff dont normally openly drink to the point of belligerence in front of the guests during hours of service so we were quite shocked by this amp didnt think having a drunk member of staff on the floor was acceptable So this amp the pre made cocktails let this bar down amp clouded our otherwise good experience Overall I would say this hotel amp its facilities are mediocre at best They are ok but when you are paying five star prices you expect a five star experience If you are a Morgans aficionado like me then avoid this place and stay at Sanderson or St Martins Lane instead where you will receive the full Morgans experience But I would certainly recommend the Sea Containers restaurant excellent food amp good service in a beautiful location And if you dont mind a drunken member of staff shouting across the room amp having to sit at the bar to order with the bartenders then I would also recommend Rumpus Room The polarity words can help explain why some hotels rated poor or excellent. token %&gt;% inner_join(hotel_1 %&gt;% filter(rating %in% c(&quot;1-2&quot;, &quot;5&quot;)), by = join_by(review_id)) %&gt;% filter(!word %in% c(&quot;hotel&quot;, &quot;stay&quot;, &quot;night&quot;)) %&gt;% filter((rating == &quot;5&quot; &amp; bing_polarity &gt; 0) | (rating == &quot;1-2&quot; &amp; bing_polarity &lt; 0)) %&gt;% count(rating, word) %&gt;% mutate(.by = rating, pct = n / sum(n)) %&gt;% group_by(rating) %&gt;% slice_max(order_by = pct, n = 10) %&gt;% ggplot(aes(x = pct, y = reorder_within(word, by = pct, within = rating))) + geom_col() + scale_y_reordered() + scale_x_continuous(labels = percent_format(1)) + labs(y = NULL, x = NULL) + facet_wrap(facets = vars(rating), scales = &quot;free_y&quot;) Word clouds are a nice way to get an overview of the data. token %&gt;% inner_join(hotel_1 %&gt;% filter(rating %in% c(&quot;1-2&quot;, &quot;5&quot;)), by = join_by(review_id)) %&gt;% filter(!word %in% c(&quot;hotel&quot;, &quot;stay&quot;, &quot;night&quot;, &quot;london&quot;), !is.na(bing_polarity)) %&gt;% mutate(polarity_desc = if_else(bing_polarity &gt; 0, &quot;Positive&quot;, &quot;Negative&quot;)) %&gt;% count(word, polarity_desc, wt = prepped_wrdcnt) %&gt;% pivot_wider(names_from = polarity_desc, values_from = n, values_fill = 0) %&gt;% data.table::data.table() %&gt;% as.matrix(rownames = &quot;word&quot;) %&gt;% wordcloud::comparison.cloud(max.words = 30, title.size = 1.5, scale = c(1, 3.5)) sentimentr sentimentr calculates polarity at the sentence level. It improves on tidytext in that it takes into account the context in which the sentiment words occur by incorporating valence shifters. A negator flips the direction of a polarizing word (e.g., “I do not like it.”). lexicon::hash_valence_shifters[y==1]. An amplifier intensifies the impact (e.g., “I really like it.”). lexicon::hash_valence_shifters[y==2]. A de-amplifier (downtoner) reduces the impact (e.g., “I hardly like it.”). lexicon::hash_valence_shifters[y==3]. An adversative conjunction overrules the previous clause containing a polarized word (e.g., “I like it but it’s not worth it.”). lexicon::hash_valence_shifters[y==4]. sentimentr uses a lexicon package combined from the syuzhet and lexicon packages. Positive words are scored +1 and negative words are scored -1. sentimentr identifies clusters of words within sentences of the text. The 4 words before and 2 words after are candidate valence shifters. Polarized words are weighted by the valence shifter weights: negators = -1; amplifiers and de-amplifiers = 1.8; adversative conjunctions decrease the value of the prior cluster and increase the value of the following cluster. Neutral words hold no value, but do affect the word count. hotel_sentimentr &lt;- sentimentr::get_sentences(hotel_1$review) %&gt;% sentimentr::sentiment() %&gt;% summarize(.by = element_id, sentimentr_polarity = mean(sentiment)) hotel_2 &lt;- hotel_1 %&gt;% mutate(element_id = row_number()) %&gt;% inner_join(hotel_sentimentr, by = join_by(element_id)) %&gt;% select(-element_id) Let’s see a few examples where sentimentr differed from tidytext. Looks like bing did a better job on the first one, but sentimentr was better on the next two. hotel_2 %&gt;% filter((bing_polarity &gt; 0.2 &amp; sentimentr_polarity &lt; -0.2) | (bing_polarity &lt; -0.2 &amp; sentimentr_polarity &gt; 0.2)) %&gt;% select(review, bing_polarity, sentimentr_polarity) %&gt;% head(3) %&gt;% flextable::flextable() %&gt;% flextable::autofit() .cl-5698e6e6{}.cl-56931a2c{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-56931a36{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-56956ca0{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-56956caa{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-56957a88{width:79.799in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-56957a92{width:1.157in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-56957a9c{width:1.574in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-56957a9d{width:79.799in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-56957aa6{width:1.157in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-56957aa7{width:1.574in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-56957ab0{width:79.799in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-56957ab1{width:1.157in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-56957aba{width:1.574in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-56957abb{width:79.799in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-56957abc{width:1.157in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-56957ac4{width:1.574in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}reviewbing_polaritysentimentr_polarityWell I am no strange to London's 5star hotels and when a new one comes along I am eager to try So I heard about the corinthia via some friends who work in the industry and said their bar was really something Until the incident I had stay there a total of 3 times but on the 4th time in June I was on my second of 4 nights and was about get some cash from my room as I was going to exchange money for some dollars as I was flying to the US pretty soon When I came to my money it felt light When I counted it it was almost 1000 down Obviously shocked I contacted reception They started an investigation Well a couple of days later I had to checkout and the matter was not resolved I was made to pay my bill in full Upon my return to London I met up with a manager at the hotel Jean Louis He said the matter was still under investigation and he gave me his card to get in touch Nothing was ever resolved This hotel does not only boast to be one of the best hotels in London but one of the best in the world You pay premium for this reason and you expect a premium service This hotel epically failed me and dented my confidence in the hotel industry I didn't even get a formal apology It's a lovely hotel but I would have to say stay at your own risk but for over 500 a night should there be any risk-0.22792120.2346432was so looking forward to staying here had high expections what a let down it was we had a connecting room as we took the children rooms are small they are clean but for over 1000 a night u expect a bit more there was five of us altogether and felt like we was crammed in staff are rude I've stayed a cheaper hotels and got treated better-0.22941570.3910338The service the Food the room all 100 i simply can't fault The Savoy i have stayed all over the world and paid huge sums The Savoy is in my mind the level that all Hotels should be judged when we stay in the City for work or to just to take in a show we always choose The Savoy-0.21821790.2017928 There is a third package called qdap, but the sentimentr Read Me explains sentimentr is an improved version that better balances accuracy and speed.↩︎ "],["statistical-test.html", "3.3 Statistical Test", " 3.3 Statistical Test You can fit an ordinal logistic regression model to predict the rating based on the review sentiment. Which performs better, tidytext or sentimentr? Start with an intercept-only model for a baseline and review of ordinal logistic regression. fit_intercept &lt;- ordinal::clm(rating ~ 1, data = hotel_2) summary(fit_intercept) ## formula: rating ~ 1 ## data: hotel_2 ## ## link threshold nobs logLik AIC niter max.grad cond.H ## logit flexible 23611 -20916.27 41838.53 6(0) 2.17e-11 1.0e+01 ## ## Threshold coefficients: ## Estimate Std. Error z value ## 1-2|3 -3.03502 0.03111 -97.56 ## 3|4 -2.15562 0.02134 -101.03 ## 4|5 -0.80833 0.01409 -57.35 The threshold coefficients in the summary table are the log-odds of the outcome variable having a level at or below vs above. Below, 10.3% of ratings were &lt;=3 and 89.7% were &gt;3 for a log-odds of log(.103/.897) = -2.1620836, corresponding to the 3|4 line in the regression summary. hotel_2 %&gt;% tabyl(rating) %&gt;% mutate(cum = cumsum(percent), `1-cum` = 1 - cum) ## rating n percent cum 1-cum ## 1-2 1083 0.04586845 0.04586845 0.9541315 ## 3 1368 0.05793910 0.10380755 0.8961925 ## 4 4827 0.20443861 0.30824616 0.6917538 ## 5 16333 0.69175384 1.00000000 0.0000000 Now fit the bing and sentimentr models. The bing model has the higher -2 * log-likelihood. fit_bing &lt;- ordinal::clm(rating ~ bing_polarity, data = hotel_2) fit_sentimentr &lt;- ordinal::clm(rating ~ sentimentr_polarity, data = hotel_2) anova(fit_bing, fit_sentimentr, fit_intercept) ## Likelihood ratio tests of cumulative link models: ## ## formula: link: threshold: ## fit_intercept rating ~ 1 logit flexible ## fit_bing rating ~ bing_polarity logit flexible ## fit_sentimentr rating ~ sentimentr_polarity logit flexible ## ## no.par AIC logLik LR.stat df Pr(&gt;Chisq) ## fit_intercept 3 41839 -20916 ## fit_bing 4 36771 -18382 5069.10 1 &lt; 2.2e-16 *** ## fit_sentimentr 4 37701 -18847 -929.79 0 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 How about predictive performance? They performed about the same. bing_conf &lt;- fit_bing %&gt;% augment(type = &quot;class&quot;) %&gt;% conf_mat(truth = rating, estimate = .fitted) sentimentr_conf &lt;- fit_sentimentr %&gt;% augment(type = &quot;class&quot;) %&gt;% conf_mat(truth = rating, estimate = .fitted) bind_rows( bing = as_tibble(bing_conf$table), sentimentr = as_tibble(sentimentr_conf$table), .id = &quot;lexicon&quot; ) %&gt;% pivot_wider(names_from = Truth, values_from = n) %&gt;% flextable::flextable() %&gt;% flextable::merge_v(j = 1) %&gt;% flextable::valign(j = 1, valign = &quot;top&quot;) .cl-57016f36{}.cl-56fbab1e{font-family:'Arial';font-size:10pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-56fbab28{font-family:'Arial';font-size:10pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-56fded02{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-56fded0c{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-56fded16{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-56fdfa04{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-56fdfa05{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-56fdfa06{width:0.75in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-56fdfa0e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-56fdfa0f{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-56fdfa10{width:0.75in;background-color:transparent;vertical-align: top;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-56fdfa18{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-56fdfa19{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0.75pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-56fdfa1a{width:0.75in;background-color:transparent;vertical-align: top;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-56fdfa22{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-56fdfa23{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0.75pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}lexiconPrediction1-2345bing1-223154311630000458746756781652658474,22915,501sentimentr1-2122227930000458742638149453749204,43915,830 bind_rows( bing = summary(bing_conf), sentimentr = summary(sentimentr_conf), .id = &quot;lexicon&quot; ) %&gt;% pivot_wider(names_from = lexicon, values_from = .estimate) ## # A tibble: 13 × 4 ## .metric .estimator bing sentimentr ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.690 0.692 ## 2 kap multiclass 0.158 0.123 ## 3 sens macro 0.320 0.290 ## 4 spec macro 0.791 0.783 ## 5 ppv macro NA NA ## 6 npv macro 0.851 0.863 ## 7 mcc multiclass 0.184 0.158 ## 8 j_index macro 0.110 0.0728 ## 9 bal_accuracy macro 0.555 0.536 ## 10 detection_prevalence macro 0.25 0.25 ## 11 precision macro 0.557 0.566 ## 12 recall macro 0.320 0.290 ## 13 f_meas macro 0.439 0.382 "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
