# Topic Modeling {#topicmodeling}

Topic models are generative probabilistic models that identify topics as clusters of words with an associated probability distribution and a probability distribution of topics within each document.

Topic models such as Latent Dirichlet Allocation (LDA) and Structural Topic Modeling (STM) treat documents within a corpora as a "bags of words" and identifies groups of words that tend to co-occur. The groups are the topics, formally conceptualized as probability distributions over vocabulary. LDA and STM are _generative models of word counts_, meaning they model a process that generates text which is a mixture of topics composed of words both of which follow probability distributions. Think of documents as the product of an algorithm that selects each word in two stages: 1) sample a topic, then 2) sample a word given that topic. The task in topic modeling is to tune the hyperparameters that define the probability distributions. In a way, topic models do the opposite of what you might expect. They do _not_ estimate the probability that document x is about topic y. Rather, they estimate the contribution of _all_ Y topics to document x.

This leads to two frameworks for thinking about topics. _Prevalance_ estimates the proportions of the document generated by the topic. _Content_ is the probability distribution of words within the topic. LDA and STM differ only in how they handle these frameworks. STM controls for covariates associated with prevalence and content while LDA does not. LDA is implemented in the **topicmodels** package and STM is implemented in the **stm** package. Whether you use LDA or STM, you start by creating a bag-of-words representation of the data. This was done in Chapter \@ref(data-prep). This chapter continues from there and follows the ideas from @Nagelkerke2020b, @Meaney2022, and the **stm** [package vignette](http://www.structuraltopicmodel.com/) package.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(topicmodels)
library(tidytext)
library(stm)
library(scales)
library(glue)
library(httr2)
library(jsonlite)
```

```{r}
load("input/hotel_prepped.Rdata")

glimpse(token)
```

## Pre-processing

Chapter \@ref(data-prep) cleaned the text. The next step is to create a document-term matrix (DTM). A DTM has one row per per document, one column per term, and the cells are frequencies. The cleaned text contains mostly infrequently used terms that will not contribute to topics, so we need to remove the sparse terms first. The pre-processing steps are the same for LDA and STM with the exception of the final DTM object class.

:::rdmnote
The LDA and STM model fitting functions in this chapter use different DTM objects. `topicmodels::LDA()` uses class DocumentTermMatrix from the **tm** package. `stm::stm()` uses class dfm from the **quanteda** package. DFM stands for document feature matrix. DTM and DFM are essentially the same thing.
:::

Keep only the decent sized reviews, ones with at least 25 words. If this is a predictive model, now is the time to create a train/test split. Consider weighting the split by the outcome variable of interest, `rating` in this case, to ensure proportional coverage.

```{r}
set.seed(12345)

hotel_gte25 <- prepped_hotel %>% filter(prepped_wrdcnt >= 25)

# Parameter `strata` ensures proportional coverage of ratings.
hotel_split <- rsample::initial_split(hotel_gte25, prop = 3/4, strata = rating)
hotel_train <- training(hotel_split)
token_test <- testing(hotel_split)

token_train <- token %>% semi_join(training(hotel_split), by = join_by(review_id))

token_test <- token %>% semi_join(testing(hotel_split), by = join_by(review_id))
```

Most words add little value to a topic model because they appear infrequently or too frequently. The most common metric for removing sparse terms is the term frequency-inverse document frequency (TF-IDF). TF(t,d) is term t's usage proportion in document d. IDF(t) the log of the inverse of the term t's proportion of documents it appears. For example, "savoy" appears in n = `r (n <- token_train %>% filter(word == "savoy") %>% select(review_id) %>% n_distinct()) %>% comma(1)` of the N = `r (N <- training(hotel_split) %>% nrow()) %>% comma(1)` training documents. Its IDF is log(N/n) = `r comma(log(N/n), .01)`. "savoy" appears in review #`r (rvw <- token_train %>% filter(word == "savoy") %>% head(1) %>% pull(review_id))` in `r token_train %>% filter(review_id == rvw) %>% summarize(n = sum(word == "savoy"), N = n(), M = mean(word == "savoy")) %>% mutate(str = glue("{n} of {N} ({percent(M, .01)})")) %>% pull(str)` of the terms. The TF-IDF score is the product of the two numbers. Here is that prepped review.

> `r hotel_gte25 %>% filter(review_id == rvw) %>% pull(prepped_review)`

@Nagelkerke2020b suggests another route. You already removed the stop words, so the over-used words are out. The TF-IDF approach was developed for long documents. Smaller documents like online reviews have little TF variation (most words are used only once or twice in a review), and the IDF ends up dominating. All you need to do is filter on each word's corpus frequency.

In the end, you need to experiment to find the right cutoff. Using TF-IDF, the elbow in the plot below is around .15. That threshold would throw out about about 90% of the vocabulary. The corpus frequency plot has an elbow around 4 occurrences. That threshold would throw out 80% of the vocabulary.

```{r}
hotel_word_stats <-
  token_train %>% 
  count(review_id, word, name = "doc_freq") %>% 
  bind_tf_idf(word, review_id, doc_freq) %>%
  mutate(.by = word, corp_freq = sum(doc_freq)) %>%
  mutate(corp_pct = corp_freq / sum(doc_freq))

hotel_word_stats %>% 
  mutate(tf_idf_bin = cut(tf_idf, breaks = 50)) %>%
  summarize(.by = tf_idf_bin, vocab = n_distinct(word)) %>%
  arrange(tf_idf_bin) %>%
  mutate(pct = vocab / sum(vocab), cumpct = cumsum(pct)) %>%
  ggplot(aes(x = tf_idf_bin)) + 
  geom_col(aes(y = pct)) +
  geom_line(aes(y = cumpct, group = 1)) +
  geom_vline(xintercept = 13, linetype = 2) +
  labs(y = "vocabulary", title = "TF-IDF Method") +
  scale_y_continuous(breaks = seq(0, 1, .1), labels = percent_format(1)) +
  theme(axis.text.x = element_text(angle = 90, vjust = .5))

hotel_word_stats %>% 
  mutate(
    corp_freq_bin = if_else(corp_freq > 19, "20+", as.character(corp_freq)),
    corp_freq_bin = factor(corp_freq_bin, levels = c(as.character(1:19), "20+"))
  ) %>%
  # mutate(corp_pct_bin = cut(corp_pct, breaks = 100)) %>%
  summarize(.by = corp_freq_bin, vocab = n_distinct(word)) %>%
  arrange(corp_freq_bin) %>%
  mutate(pct = vocab / sum(vocab), cumpct = cumsum(pct)) %>%
  ggplot(aes(x = corp_freq_bin, y = cumpct)) + 
  geom_col(aes(y = pct)) +
  geom_line(aes(y = cumpct, group = 1)) +
  geom_vline(xintercept = 4, linetype = 2) +
  labs(y = "vocabulary", title = "Corpus Frequency Method") +
  scale_y_continuous(breaks = seq(0, 1, .1), labels = percent_format(1)) +
  theme(axis.text.x = element_text(angle = 90, vjust = .5))
```

Compare the resulting DTMs. The TF-IDF method keeps half the documents and 1,500 terms. The corpus frequency method retains _all_ documents while limiting the vocabulary to 1,200 terms. Corpus frequency does seem superior.

```{r collapse=TRUE}
(dtm_tfidf <-
  hotel_word_stats %>%
  filter(tf_idf > .15) %>%
  cast_dtm(document = review_id, term = word, value = doc_freq))

(dtm_corpfreq <-
  hotel_word_stats %>%
  filter(corp_freq > 5) %>%
  cast_dtm(document = review_id, term = word, value = doc_freq))
```

The pre-processing step sure pares down the corpus. The high frequency terms comprise only 20% of the vocabulary, but are still 83% of the total word usage.

```{r}
bind_rows(
  `high freq words` = hotel_word_stats %>% filter(corp_freq > 5) %>%
    summarize(distinct_words = n_distinct(word), total_words = sum(doc_freq)),
  `low freq words` = hotel_word_stats %>% filter(corp_freq <= 5) %>%
    summarize(distinct_words = n_distinct(word), total_words = sum(doc_freq)),
  .id = "partition"
) %>%
  mutate(total_pct = total_words / sum(total_words) * 100, 
         distinct_pct = distinct_words / sum(distinct_words) * 100) %>%
  select(partition, distinct_words, distinct_pct, total_words, total_pct) %>%
  janitor::adorn_totals()
```

## LDA {#lda}

Latent Dirichlet allocation (LDA) is one of a family of mixed membership models that decompose data into latent components. *Latent* means _unidentified_ topics and *Dirichlet* is the distribution followed by the words in topics and by topics in documents.

LDA presumes each document is created by a generative process in which topics are selected from a probability distribution and then words from that topic are selected from another distribution. LDA optimizes the distributions by performing a random search through the parameter space to find the model with the largest log-likelihood. There are multiple search algorithms, but the preferred one appears to be Gibbs sampling, a type of Monte Carlo Markov Chain (MCMC) algorithm. The [algorithm](https://www.mygreatlearning.com/blog/understanding-latent-dirichlet-allocation/) is:

1) For each document $d = 1 \ldots D$, randomly assign each word $w = 1 \ldots W$ to one of $k = 1\ldots K$ topics.

2) Tabulate the number of words in each document and topic, a $D \times K$ matrix, and tabulate the number of occurrences of each word in each document, a $W \times D$ matrix.

3) Resample to remove a single instance of a word from the corpus, decrementing the document's topic count and the word's topic count.

4) Calculate the gamma matrix and the beta matrix.
    * the gamma matrix (aka theta) is the topical prevalence, the probability distribution of topics for each document, $$p(k|d) = \frac{n_{dk} + \alpha}{N_d + K \alpha}$$ were $n_{dk}$ is the number of words in document $d$ for topic $k$, $N_d$ is the total number of words in $d$, and $\alpha$ is a hyperparameter. For each $d$, $\sum_{k \in K} \gamma_{dk} = 1$.
    * the beta matrix (aka phi), is the topical content, the probability distribution of words for each topic, $$p(w|k) = \frac{m_{w,k} + \beta}{\sum_{w \in W}m_{d,k} + W\beta}$$ where $m_{w,k}$ is the corpus-wide frequency count of word $w$ to topic $k$, $W$ is the number of distinct words in the corpus, and $\beta$ is a hyperparameter. For each $k$, $\sum_{w \in W} \beta_{kw} = 1$.
  
6) Calculate the joint probability distribution of words for each document and topic, $p(w|k,d) = p(k|d)p(w|k)$. Assign each word, $w$, to the topic with the maximum joint probability.

7) Repeat steps 3-6 for all of the words in all of the documents.

8) Repeat steps 3-7 for a pre-determined number of iterations.

LDA thus has 3 hyperparameters: the document-topic density factor, $\alpha$, the topic-word density factor, $\beta$, and the topic count, $K$. $\alpha$ controls the number of topics expected per document (large $\alpha$ = more topics). $\beta$ controls the distribution of words per topic (large $\beta$ = more words). Ideally, you want a few topics per document and a few words per topic, so $\alpha$ and $\beta$ are typically set below one. $K$ is set using a combination of domain knowledge, coherence, and exclusivity.

Notice that LDA is a "bag of words" method. It does not consider the order of the tokens in the text, so where tokens are located what other tokens are nearby do not factor into the output.

### Fit {-}

Fit the LDA model with topicmodels::LDA(). It uses the **tm** DocumentTermMatrix data class, the same as what you just created in the pre-processing step.

```{r}
hotel_dtm <- dtm_corpfreq
```

There are several parameters you might tweak for the model fit. The biggest surprise is that you set the number of topics, `k`. In general, you only want as many topics as are clearly distinct and that you can easily communicate to others. @Nagelkerke2020b suggests sticking with the art vs science approach and picking your own `k`. A common recommendation is the perplexity statistic. Perplexity is a measure of how well a probability model fits a new set of data. As the number of topics increase, the perplexity will generally decrease. That is not the case below.

```{r warning=FALSE}
set.seed(12345)

train_ind <- sample(nrow(hotel_dtm), floor(0.75*nrow(hotel_dtm)))
k_train <- hotel_dtm[train_ind, ]
k_test <- hotel_dtm[-train_ind, ]

k = c(seq(from = 2, to = 10, by = 2))

perp <- k %>% 
  map(~ LDA(k_train, k = .x)) %>%
  map(~ perplexity(.x, newdata = k_test, control = list(estimate.beta = FALSE))) %>%
  as.numeric()

data.frame(k = k, perplexity = perp) %>%
  ggplot(aes(x = k, y = perplexity)) + 
  geom_point() + 
  geom_smooth(method = "loess", se = FALSE, formula = "y~x") +
  labs(title = "Perplexity Plot for LDM model")
```

I will stick with a simple k = 4 model, mostly for convenience since this is just an example.

```{r}
lda_fit <- LDA(hotel_dtm, k = 4)
```

The fitted object contains two matrices. The **beta** (aka "phi") matrix is the distribution of tokens (cols) over topics (rows). The **gamma** (aka "theta") matrix is the distribution of documents (rows) over topics (cols). The row sum is 1 for each matrix (sum of topic probabilities, some of document probabilities).

```{r collapse=TRUE}
lda_beta_mtrx <- posterior(lda_fit) %>% pluck("terms") %>% as.matrix()

# One row per topic, one col per token.
dim(lda_beta_mtrx)

# Word probability distribution sums to 1 for each topic.
sum(lda_beta_mtrx[1, ])

lda_gamma_mtrx <- posterior(lda_fit) %>% pluck("topics") %>% as.matrix()

# One row per document, one col per topic
dim(lda_gamma_mtrx)

# Topic probability distribution sums to 1 for each document.
sum(lda_gamma_mtrx[1, ])
```

`tidytext::tidy()` pivots the beta matrix into a [topic, term, beta] data frame.

```{r}
lda_beta <- tidy(lda_fit, matrix = "beta")

lda_top_tokens <- 
  lda_beta %>%
  mutate(topic = factor(paste("Topic", topic))) %>%
  group_by(topic) %>%
  slice_max(order_by = beta, n = 10) %>%
  ungroup()

lda_top_tokens %>%
  ggplot(aes(x = beta, y = reorder_within(term, by = beta, within = topic))) +
  geom_col() +
  scale_y_reordered() +
  facet_wrap(facets = vars(topic), scales = "free_y") +
  labs(y = NULL, title = "LDA Top 10 Terms")
```

Word clouds tell you more or less the same thing. Here is the code below. I think everyone has pretty much moved past word clouds, so I'm not even wasting space on this.

```{r eval=FALSE}
colors6 <- RColorBrewer::brewer.pal(n = 4, name = "Set2")

x <- map(
  c(1:4), 
  ~ with(lda_beta %>% filter(topic == .x), 
         wordcloud::wordcloud(
           term, 
           beta, 
           max.words = 20,
           colors = colors6[.x]
         ))
)
```

There is a downside to this evaluation. Popular words like "room" appear at or near the top in multiple topics. You might want to look at _relative_ popularity instead: the popularity within the topic divided by overall popularity. That's problematic too because words that only appear in few reviews will pop to the top. What you want is a combination of both absolute term probability and relative term probability. `LDAvis::serVis()` can help you do that. 

:::rmdnote
Unfortunately, the plot from `LDAvis::serVis()` is interactive and does not render in the RMarkdown notebook html, so below is just a screenshot of the code chunk output.
:::

The left side shows the topic sizes (documents) and topic distances. The right side shows the most important tokens.

```{r eval=FALSE}
# word count for each document
doc_length <- hotel_word_stats %>% filter(corp_freq > 5) %>% 
  summarize(.by = review_id, n = sum(doc_freq)) %>% pull(n)

# vocabulary: unique tokens
vocab <- colnames(lda_beta_mtrx) 

# overall token frequency
term_frequency <- hotel_word_stats %>% filter(corp_freq > 5) %>% 
  summarize(.by = word, n = sum(doc_freq)) %>% arrange(match(word, vocab)) %>% pull(n) 

# create JSON containing all needed elements
json <- LDAvis::createJSON(lda_beta_mtrx, lda_gamma_mtrx, doc_length, vocab, term_frequency)

LDAvis::serVis(json)
```

![](images/ldavis.png)

The gamma matrix shows topic distributions. Use it to see if topics vary by a covariate. The topics did not vary by rating

```{r}
tidy(lda_fit, matrix = "gamma") %>% 
  mutate(document = as.numeric(document), topic = factor(topic)) %>%
  inner_join(prepped_hotel, by = join_by(document == review_id)) %>%
  summarize(.by = c(rating, topic), gamma = mean(gamma)) %>%
  ggplot(aes(x = gamma, y = fct_rev(rating), fill = topic)) +
  geom_col() +
  labs(y = NULL, title = "LDA Topic Distribution")
```

Iterate through the model by tweaking `k`, excluding words that suppress interesting subdomains, and/or changing the minimal token frequency to focus on more/less dominant tokens. You can also change the document sampling strategy to promote interesting domains, like we did when we over sampled the low hotel ratings.

### Topic Labeling with ChatGPT {-}

:::rmdnote
An API token is required for OpenAI's ChatGPT API service. I created one at https://platform.openai.com/api-keys and saved it to .Renviron. See `usethis::edit_r_environ()`. 
:::

```{r eval=FALSE}
# Create a function to send each list of topic words to Open AI as a separate request.
get_topic_from_openai <- function(prompt) {
  my_resp <-
    request("https://api.openai.com/v1/chat/completions") %>%
    req_headers(Authorization = paste("Bearer", Sys.getenv("OPENAI_API_KEY"))) %>%
    req_body_json(list(
      model = "gpt-3.5-turbo",
      # Temperature [0,2] controls creativity, predictable -> variable. 
      temperature = 1,
      messages = list(
        # System prompt sets repeated context. It is prefixed to prompts.
        list(
          role = "system",
          content = paste("You are a topic modeling assistant. You accept lists ",
                          "of words in a topic and summarizes them into a salient ",
                          "topic label of five words or less. How would you ",
                          "summarize the following list? The list is in descending ",
                          "order of importance, so the first term in the list is most ",
                          "strongly tied to the topic. Return just the topic label and ",
                          "nothing else.")
        ),
        list(
          role = "user",
          content = prompt
        )
      )
    )) %>%
    req_perform() %>% 
    resp_body_json() %>%
    pluck("choices", 1, "message", "content")
}

lda_topics <- 
  lda_top_tokens %>%
  nest(data = term, .by = topic) %>%
  mutate(
    token_str = map(data, ~paste(.$term, collapse = ", ")),
    topic_lbl = map_chr(token_str, get_topic_from_openai),
    topic_lbl = str_remove_all(topic_lbl, '\\"'),
    topic_lbl = snakecase::to_any_case(topic_lbl, "title")
  ) %>%
  select(-data)

# Save to file system to avoid regenerating.
saveRDS(lda_topics, file = "input/lda_topics.RDS")
```

```{r}
lda_topics <- readRDS(file = "input/lda_topics.RDS")
```

Let's see the topic summary with the newly generated labels.

```{r}
lda_top_tokens %>%
  inner_join(lda_topics, by = join_by(topic)) %>%
  mutate(topic_lbl = str_wrap(topic_lbl, 25)) %>%
  ggplot(aes(x = beta, y = reorder_within(term, by = beta, within = topic_lbl))) +
  geom_col() +
  scale_y_reordered() +
  facet_wrap(facets = vars(topic_lbl), scales = "free_y") +
  labs(y = NULL, title = "LDA Top 10 Terms")
```

### Discussion

LDA has some restrictive assumptions to be aware of.

* The distribution of topics is estimated independently of the documents. If the corpus was about vacation resorts, weather would an important topic in warm equatorial zones where sunny weather was the main selling point. Museums might be more important in historical European cities.

* The distribution of words in a topic is estimated independently of the documents. Two documents could be about the topic "violence during protests", but use different language based on viewpoint "peaceful protesters and brutal police" vs "rioters and law and order". It's the same topic, but the _content_ differs.

### TODO {-}

I still need to learn more about 

* Held-out Likelihood (Wallach et al., 2009).
* Semantic Coherence. The coherence measure evaluates topics.
* Exclusivity. Generally, the greater the number of topics in a model, the lower the quality of the smallest topics. One way around this is hiding the low-quality topics.

## STM {#stm}

STM introduces covariates which "structure" the prior distributions in the topic model @roberts2014structural. Topics can be correlated, each document has its own prior distribution over topics, and word use can vary by covariates. STM models incorporate two effects.

* **Topical prevalence covariate effects**. STM models *prevalence* as a function of the covariates. A survey respondent's party affiliation may affect _which_ topics they discuss in a question about their view on immigration. In our hotel case study, a local (UK) business traveler might care about different hotel qualities than a foreign tourist.
* **Topical content covariate effects**. STM models *content* as a function of the covariates. A survey respondent's party affiliate may affect _how_ they discuss topics in a question about public protests. A conservative might use words like "rioters" and "law and order" for a topic about violent demonstrations while a liberal might use words like "police brutality" and "oppression". In the hotel example, negative reviews might focus on lapses in baseline expectations such as "sheets", while positive reviewers focus on unexpected delights such as "origami" towels.

STM is similar to LDA in that it assumes each document is created by a generative process where topics are included according to probabilities (topical prevalence) and words are included in the topics (topical content) according to probabilities. In LDA, topic prevalence and content came from Dirichlet distributions with hyperparameters set in advance, sometimes referred to as a and b. With STM, the topic prevalence and content come from document metadata. The covariates provide a way of “structuring” the prior distributions in the topic model, injecting valuable information into the inference procedure @tingly2014.

### Fit {-}

Fit the STM model with stm::stm(). It uses the **quanteda** dfm data class.^[I used STM for my [Battle of the Bands](https://mpfoley73.github.io/battle-of-the-bands/) project.]

:::rmdnote
You can cast `hotel_word_stats` as a DFM with `tidytext::cast_dfm()`, but `stm()` returned errors with prevalence and topic covariates. Instead, use `stm::textProcessor()` and `stm::prepDocuments()`.
:::

`stm::textProcessor()` produces a list object containing a vocabulary vector, a list of mini-DTM matrices for each document, and a metadata data frame. `textProcessor()` can remove stop words and change case, etc., but we already did that, so set those parameters to FALSE. `stm::prepDocuments()` removes sparse terms from the matrix.

```{r}
# This produces errors in modeling phase, so don't use it.
hotel_dfm_tidy <-
  hotel_word_stats %>%
  filter(corp_freq > 5) %>%
  cast_dfm(document = review_id, term = word, value = doc_freq)

hotel_processed <-
  stm::textProcessor(
    documents = training(hotel_split) %>% pull(prepped_review),
    metadata = training(hotel_split) %>% select(rating, reviewer_loc),
    lowercase = FALSE,
    removestopwords = FALSE,
    removenumbers = FALSE,
    removepunctuation = FALSE,
    stem = FALSE
  )

hotel_dfm <-
  stm::prepDocuments(
    hotel_processed$documents,
    hotel_processed$vocab,
    hotel_processed$meta,
    lower.thresh = 5
  )
```

What constitutes a "good" model? The topics should be _cohesive_ in the sense that high-probability words tend to co-occur with documents. The topics should also be _exclusive_ in the sense that the top words in topics are unlikely to be shared. To find the best model, generate a candidate set with varying tuning parameters, initialization, and pre-processing. Plot the coherence and exclusivity to identify the one with the best combination.

Take time to validate the model by trying to predict covariate values from the text.



Either specify the number of topics (K) to identify, or let `stm()` choose an optimal number by setting `K = 0`. The resulting probability distribution of topic words (beta matrix) will be a K x `r comma(length(hotel_processed$vocab), 1)` matrix. The probability distribution of topics (gamma matrix, theta in the **stm** package) will be a `r comma(length(hotel_dfm$documents), 1)` x K matrix. I expect topics to correlate with the review rating, so `rating` is a prevalence covariate, and I expect word usage to correlate with the reviewer location, so `reviewer_loc` is a topical content covariate.

```{r eval=FALSE}
# This model fit operation took 15 minutes to run. Run once and save to disk.
set.seed(1234)

stm_fits <- 
  tibble(K = seq(from = 2, to = 10, by = 2)) %>%
  mutate(fit = map(
    K, 
    ~stm::stm(
        documents = hotel_dfm$documents,
        vocab = hotel_dfm$vocab,
        K = .,
        prevalence = ~ rating,
        content = ~ reviewer_loc,
        data = hotel_dfm$meta,
        verbose = FALSE
      )
  ))

stm_fit <- stm::stm(
  documents = hotel_dfm$documents,
  vocab = hotel_dfm$vocab,
  K = 4,
  prevalence = ~ rating,
  content = ~ reviewer_loc,
  data = hotel_dfm$meta,
  verbose = FALSE
)

saveRDS(stm_fit, file = "input/stm_fit.RDS")
```

```{r}
stm_fit <- readRDS(file = "input/stm_fit.RDS")

hotel_dfm %>% summary()
```

Whereas LDA models are optimized using the perplexity statistic, STM offers several options. The most useful are the held-out likelihood and coherence.

```{r eval=FALSE}
stm_heldout <- stm::make.heldout(hotel_dfm$documents, vocab = hotel_dfm$vocab)

stm::semanticCoherence(stm_fit, documents = hotel_dfm$documents)

stm_fits %>%
  mutate(
    semantic_coherence = map(fit, ~semanticCoherence(.x, documents = hotel_dfm$documents))
  ) %>%
  select(K, semantic_coherence) %>%
  unnest(semantic_coherence) %>%
  summarize(.by = K, semantic_coherence = mean(semantic_coherence)) %>%
  ggplot(aes(x = K, y = semantic_coherence)) +
    geom_line()

stm_fit2 <- stm::stm(
  stm_heldout$documents,
  stm_heldout$vocab,
  K = 4,
  prevalence = ~ rating,
  content = ~ reviewer_loc,
  data = hotel_dfm$meta,
  init.type = "Spectral",
  verbose = FALSE
)

# stm_fit2 %>% stm::exclusivity()
# stm_fit2 %>% stm::semanticCoherence(documents = stm_heldout$documents)
```


### Interpret {-}

The fit summary has three sections showing the tops words. The first section shows the prevalence model; the second shows the topical content model; and the third shows their interaction.

```{r}
summary(stm_fit)
```

If this was just a regular topic model, or a prevalence or content model, we'd see top words by 4 metrics: highest probability, FREX, lift, and score.

- **Highest probability** weights words by their overall frequency.
- **FREX** weights words by their overall frequency and how exclusive they are to the topic.
- **Lift** weights words by dividing by their frequency in other topics, therefore giving higher weight to words that appear less frequently in other topics.
- **Score** divides the log frequency of the word in the topic by the log frequency of the word in other topics.

Let's fit a new model just to show that.

```{r eval=FALSE}
set.seed(1234)

stm_fit_simple <- stm::stm(
  hotel_dfm$documents,
  hotel_dfm$vocab,
  K = 4,
  # prevalence = ~ rating,
  # content = ~ reviewer_loc,
  data = hotel_dfm$meta,
  init.type = "Spectral",
  verbose = FALSE
)

saveRDS(stm_fit_simple, file = "input/stm_fit_simple.RDS")
```

```{r}
stm_fit_simple <- readRDS(file = "input/stm_fit_simple.RDS")
```

```{r}
stm::labelTopics(stm_fit_simple)
```

It is interesting that the top terms for UK did not include "restaurant" or "location". The top terms for the US did not include "excellent" or "amaze", but did include "love".

```{r}
stm_tidy <- tidy(stm_fit)

stm_top_tokens <- 
  stm_tidy %>%
  mutate(topic = factor(paste("Topic", topic))) %>%
  group_by(topic, y.level) %>%
  slice_max(order_by = beta, n = 10) %>%
  ungroup()

stm_top_tokens %>%
  filter(topic == "Topic 1") %>%
  ggplot(aes(x = beta, y = reorder_within(term, by = beta, within = topic))) +
  geom_col() +
  scale_y_reordered() +
  facet_wrap(facets = vars(y.level), scales = "free_x", ncol = 3) +
  labs(y = NULL, title = "STM Top 10 Terms for Topic 1")
```


As we did with the LDA model, we can assign topic labels with Open AI.

```{r eval=FALSE}
stm_topics <- 
  stm_top_tokens %>%
  nest(data = term, .by = topic) %>%
  mutate(
    token_str = map(data, ~paste(.$term, collapse = ", ")),
    topic_lbl = map_chr(token_str, get_topic_from_openai),
    topic_lbl = str_remove_all(topic_lbl, '\\"'),
    topic_lbl = snakecase::to_any_case(topic_lbl, "title")
  ) %>%
  select(-data)

# Save to file system to avoid regenerating.
saveRDS(stm_topics, file = "input/stm_topics.RDS")
```

```{r}
stm_topics <- readRDS(file = "input/stm_topics.RDS")

stm_topics
```

Another way to evaluate the model is to print reviews that are most representative of the topic. Topic 1 

```{r}
stm::findThoughts(
  stm_fit, 
  n = 3, 
  texts = hotel_dfm$meta$review, 
  topics = 1, 
  meta = hotel_dfm$meta
)
```

## Prediction
