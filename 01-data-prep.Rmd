# Data Preparation {#data-prep}

This section covers how to prepare a corpus for text analysis. I'll work with the [customer reviews of London-based hotels](https://data.world/promptcloud/customer-of-reviews-of-london-based-hotels) data set hosted on data.world. `hotel_raw` contains 27K reviews of the ten most- and ten least-expensive hotels in London. The csv file is located online [here](https://bhciaaablob.blob.core.windows.net/cmotionsnlpblogs/RestoReviewRawdata.csv). I saved it to my \\inputs directory.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext)
library(scales)
library(glue)
```

```{r message=FALSE}
hotel_0 <- 
  read_csv(
    "input/london_hotel_reviews.csv", 
    col_types = "cicccc",
    col_names = c("property", "rating", "title", "review", "reviewer_loc", "review_dt"),
    skip = 1
  ) %>%
  mutate(review_id = row_number()) %>%
  select(review_id, everything())

glimpse(hotel_0)
```

## Scrub

The data needs to be cleaned. I'll follow some of the techniques used by @Nagelkerke2020a. One issue is tags like *\<e9\>* and unicode characters like *<U+0440>*. [One way](https://stackoverflow.com/questions/36108790/trouble-with-strings-with-u0092-unicode-characters) to get rid of unicode characters is to convert them to ASCII tags with `iconv()` and then remove the ASCII tags with `str_remove()`. E.g., `iconv()` converts <U+0093> to <93> which you can remove with regex `"\\<[:alnum]+\\>]"`.^[More help with regex on [RStudio's cheat sheets](https://rstudio.com/resources/cheatsheets/).] There are also some reviews in other languages that I'll just drop. And some hotel names are pretty long, so I'll abbreviate them.

```{r}
hotel_1 <- hotel_0 %>%
  mutate(
    # Create ASCII bytes
    review = iconv(review, from = "", to = "ASCII", sub = "byte"),
    # Remove <..>
    review = str_remove_all(review, "\\<[[:alnum:]]+\\>"),
    # Remove <U+....>
    review = str_remove_all(review, "\\<U\\+[[:alnum:]]{4}\\>"),
    # Lots of pipes?
    review = str_remove_all(review, "(\\|)"),
    review = str_squish(review),
    # Shorten some of the hotel names.
    property = factor(str_remove_all(
      property, "( - .*)|(, .*)|( Hotel)|( London)|(The )|( at .*)|( Hyde .*)|( Knights.*)"
    )),
    # Interesting metadata
    chr_cnt = str_length(review)
  ) %>%
  # Exclude reviews written in a foreign language. One heuristic to handle this 
  # is to look for words common in other languages that do not also occur in English.
  filter(
    !str_detect(review, "( das )|( der )|( und )|( en )"), # German
    !str_detect(review, "( et )|( de )|( le )|( les )"),   # French
    !str_detect(review, "( di )|( e )|( la )"),            # Italian
    !str_detect(review, "( un )|( y )"),                   # Spanish
    str_length(review) > 0
  ) 
```

That might be enough. Let's explore the data.

```{r}
# 90% of reviews rate the property a 4 or 5.
hotel_1 %>% janitor::tabyl(rating)

# Some reviews are as small as 16 characters, but they can get quite large.
hotel_1 %>% select(chr_cnt) %>% summary()

# A few reviews
hotel_1 %>% 
  sample_n(3, seed = 12345) %>% 
  select(chr_cnt, review) %>% 
  flextable::flextable() %>% 
  flextable::valign(j = 1, valign = "top") %>% 
  flextable::autofit()
```

@Nagelkerke2020a recommends removing punctuation to focus on the entire text rather than the sentences within. Nagelkerke also suggests removing very short (<= 3 chars) for anything other than sentiment analysis. I'm going to keep punctuation and short reviews for now. Some of those extremely short reviews are gibberish, but tokenizing will filter out some of that.

## Tokenize

Tokenize the reviews. Even if you want bigrams, it is often helpful to tokenize into unigrams first to clean and regularize.

```{r}
token_0 <- 
  hotel_1 %>%
  select(review_id, review) %>%
  unnest_tokens("word", review) %>%
  mutate(.by = review_id, n = n()) %>%
  # Short reviews are mostly gibberish. Require at least 10 words.
  filter(n >= 10)

# Attach word counts back to main data frame. Inner joining will remove some
# of the bogus reviews.
hotel_2 <- 
  token_0 %>% 
  count(review_id, name = "word_cnt") %>% 
  inner_join(hotel_1, by = "review_id") %>%
  relocate(word_cnt, .after = chr_cnt)

hotel_2 %>% select(chr_cnt, word_cnt) %>% summary()
```

@Nagelkerke2020a recommends discarding reviews with few (<=50) tokens on the grounds that short reviews will not add much in terms of _different_ topics _within_ reviews.^[TODO: what does Nagelkerke mean - why is this important?]. That's about 25% of this sample. I set a lower limit of 10 tokens.

## Spell-check

Run a spell-check to regularize the data. Its possible to land on the wrong correction, but there is probably more to gain than lose. Only a very small fraction of the tokens were misspellings.

```{r}
# There are multiple possible right spellings, so just choose one.
spell_check <- fuzzyjoin::misspellings %>% distinct(misspelling, .keep_all = TRUE)

token_1 <-
  token_0 %>%
  left_join(spell_check, by = join_by(word == misspelling)) %>%
  mutate(word = coalesce(correct, word)) %>%
  select(-correct)

# Only .09% of words were misspelled.
mean(token_0$word != token_1$word)

# Examples.
tibble(before = token_0$word, after = token_1$word) %>% filter(before != after) %>% head()
```

## Lemmatize

Stemming and lemmatizing are ways to convert word variations like "staying", "stayed", and "stay" into a generic form: "stay". Stemming tends to chop off endings to create a root word, but the stem is often not a word itself. E.g., "staying" becomes "stai". Lemmatize gives you the more natural "stay".

```{r}
token_2 <- token_1 %>% mutate(word = textstem::lemmatize_words(word))

# Examples.
tibble(before = token_1$word, after = token_2$word) %>% filter(before != after) %>% head()
```

## Remove Stop Words

Stop words usually add no value, but you should pay attention to what you are dropping. Be ready to add pertinent words back and perhaps drop others.

```{r}
# Start with a standard list.
stop <- tidytext::stop_words %>%
  # Don't remove potentially useful words.
  filter(!word %in% c("appreciate", "room")) %>%
  # But also exclude these words.
  bind_rows(tibble(word = as.character(0:9)))

token <- token_2 %>% anti_join(stop, by = "word")

# Most frequently removed words
token_2 %>% semi_join(stop, by = "word") %>% count(word, sort = TRUE)

# A typical review, before and after
bind_cols(
  review_id = 2:4,
  before = hotel_2 %>% filter(review_id %in% 2:4) %>% pull(review),
  after = token %>% filter(review_id %in% 2:4) %>% 
    summarize(.by = review_id, x = paste(word, collapse = " ")) %>%
    pull(x)
) %>%
  flextable::flextable() %>% flextable::valign(valign = "top") %>% flextable::autofit()
```

At this point, you might decide to throw out smaller reviews because they are unlikely to identify multiple topics [@VanGils2020]. 

## Bigrams

Bigrams should not contain stop words. However, they should be adjacent words, so tokenize into bigrams, split into words an then filter out rows where one or both words is stop word.

```{r}
bigram_0 <-
  token_2 %>%
  summarize(.by = review_id, reconstructed = paste(word, collapse = " ")) %>%
  unnest_tokens("bigram", reconstructed, token = "ngrams", n = 2)

# Remove the bigrams where one or both words are stop words
bigram <- 
  bigram_0 %>%
  separate(bigram, into = c("word1", "word2"), sep = " ") %>%
  anti_join(stop, by = join_by(word1 == word)) %>%
  anti_join(stop, by = join_by(word2 == word)) %>%
  mutate(bigram = paste(word1, word2)) %>%
  select(review_id, bigram)

# Example
bind_cols(
  hotel_2 %>% filter(review_id == 3) %>% select(review),
  bigram %>% filter(review_id == 3) %>% 
    summarize(bigrams = paste(bigram, collapse = "\n"))
) %>%
  flextable::flextable() %>%
  flextable::autofit() %>%
  flextable::width(j = 1, width = 4.5, unit = "in") %>%
  flextable::width(j = 2, width = 1.5, unit = "in") %>%
  flextable::valign(valign = "top")
```

## Save

Save the cleaned data for other analyses like topic modeling and sentiment analysis.

```{r}
hotel_prep <- hotel_2

save(hotel_prep, token, bigram, file = "input/hotel_prepped.Rdata")
```
