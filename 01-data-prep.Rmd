# Data Preparation {#data-prep}

This section covers how to prepare a corpus for text analysis. I'll work with the [customer reviews of London-based hotels](https://data.world/promptcloud/customer-of-reviews-of-london-based-hotels) data set hosted on data.world. `hotel_raw` contains 27K reviews of the ten most- and ten least-expensive hotels in London. The csv file is located online [here](https://bhciaaablob.blob.core.windows.net/cmotionsnlpblogs/RestoReviewRawdata.csv). I saved it to my \\inputs directory.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext)
library(janitor)
library(scales)
library(glue)
```

```{r message=FALSE}
hotel_0 <- 
  read_csv(
    "input/london_hotel_reviews.csv", 
    col_types = "cicccc",
    col_names = c("hotel", "rating", "title", "review", "reviewer_loc", "review_dt"),
    skip = 1
  ) %>%
  mutate(review_id = row_number()) %>%
  select(review_id, everything(), -c(title, review_dt))

glimpse(hotel_0)
```

## Scrub

The data needs to be cleaned. I'll follow some of the techniques used by @Nagelkerke2020a. One issue is tags like *\<e9\>* and unicode characters like *<U+0440>*. [One way](https://stackoverflow.com/questions/36108790/trouble-with-strings-with-u0092-unicode-characters) to get rid of unicode characters is to convert them to ASCII tags with `iconv()` and then remove the ASCII tags with `str_remove()`. E.g., `iconv()` converts <U+0093> to <93> which you can remove with regex `"\\<[:alnum]+\\>]"`.^[More help with regex on [RStudio's cheat sheets](https://rstudio.com/resources/cheatsheets/).] There are also some reviews in other languages that I'll just drop. And some hotel names are pretty long, so I'll abbreviate them.

```{r}
hotel_1 <- hotel_0 %>%
  mutate(
    # Create ASCII bytes
    review = iconv(review, from = "", to = "ASCII", sub = "byte"),
    # Remove <..>
    review = str_remove_all(review, "\\<[[:alnum:]]+\\>"),
    # Remove <U+....>
    review = str_remove_all(review, "\\<U\\+[[:alnum:]]{4}\\>"),
    # Only keep letters, numbers, and apostrophes.
    review = str_remove_all(review, "[^[:alnum:][\\s][\\']]"),
    review = str_squish(review),
    # Shorten some of the hotel names.
    hotel = str_remove_all(
      hotel, 
      "( - .*)|(, .*)|( Hotel)|( London)|(The )|( at .*)|( Hyde .*)|( Knights.*)"
    ), 
    hotel = factor(hotel, ordered = TRUE),
    # Reducing number of hotels for modeling simplicity.
    hotel = fct_lump_prop(hotel, prop = .05),
    # Bin common locations,
    reviewer_loc = factor(case_when(
      str_detect(reviewer_loc, "(London)|(United Kingdom)|(UK)") ~ "United Kingdom",
      str_detect(reviewer_loc, "(New York)|(California)") ~ "United States",
      TRUE ~ "Other"
    )),
    # Low ratings are so rare, lump the bottom two.
    rating = fct_collapse(as.character(rating), `1-2` = c("1", "2")),
    # Interesting metadata
    raw_chrcnt = str_length(review)
  ) %>%
  # Exclude reviews written in a foreign language. One heuristic to handle this 
  # is to look for words common in other languages that do not also occur in English.
  filter(
    !str_detect(review, "( das )|( der )|( und )|( en )"), # German
    !str_detect(review, "( et )|( de )|( le )|( les )"),   # French
    !str_detect(review, "( di )|( e )|( la )"),            # Italian
    !str_detect(review, "( un )|( y )"),                   # Spanish
    raw_chrcnt > 0
  ) 
```

That might be enough. Let's explore the data. We have 9 hotels. Reviewers are binned into 3 locations. 90% of reviews rate the property a 4 or 5. Some reviews are as small as 1 character, but they can get quite large.

```{r}
skimr::skim(hotel_1)
```

@Nagelkerke2020a recommends removing punctuation to focus on the entire text rather than the sentences within. Nagelkerke also suggests removing very short (<= 3 chars) for anything other than sentiment analysis. I'm going to keep punctuation and short reviews for now even though some of those extremely short reviews are gibberish.

## Tokenize

Most models are based on tokenized text. Even if you are interested in working with bigrams, tokenize into words to clean and regularize the data first.

```{r}
token_0 <- 
  hotel_1 %>%
  select(review_id, review) %>%
  unnest_tokens("word", review)

# Attach word counts back to main data frame, just to aid understanding.
hotel_2 <-
  hotel_1 %>%
  inner_join(count(token_0, review_id, name = "raw_wordcnt"), by = join_by(review_id))

hotel_2 %>% select(raw_chrcnt, raw_wordcnt) %>% summary()
```

## Spell-check

Run a spell-check to regularize the data. It's possible to land on the wrong correction, but there is probably more to gain than lose. Only a very small fraction of these tokens were misspellings.

```{r}
# There are multiple possible right spellings, so just choose one.
spell_check <- fuzzyjoin::misspellings %>% distinct(misspelling, .keep_all = TRUE)

token_1 <-
  token_0 %>%
  left_join(spell_check, by = join_by(word == misspelling)) %>%
  mutate(word = coalesce(correct, word)) %>%
  select(-correct)

# Only .09% of words were misspelled.
mean(token_0$word != token_1$word)

# Examples.
tibble(before = token_0$word, after = token_1$word) %>% filter(before != after) %>% 
  count(before, after, sort = TRUE)
```

## Remove Stop Words

Stop words usually add no value, but you should pay attention to what you are dropping. Be ready to add pertinent words back and perhaps drop others.

```{r}
# Start with a standard list.
stop <- tidytext::stop_words %>%
  # Remove potentially useful words from stop list.
  filter(!word %in% c("appreciate", "room", "first")) %>%
  # Add custom stop words.
  bind_rows(tibble(word = c("hotel")))

token_2 <- anti_join(token_1, stop, by = "word")

# Most frequently removed words
token_1 %>% 
  anti_join(token_2, by = join_by(review_id, word)) %>% 
  count(word, sort = TRUE)
```

## Lemmatize

Stemming and lemmatizing convert word variations like "staying", "stayed", and "stay" into a generic form: "stay". Stemming tends to chop off endings to create a root word, but the stem is often not a word itself. E.g., "staying" becomes "stai". Lemmatize gives you the more natural "stay".

```{r}
token <- token_2 %>% mutate(word = textstem::lemmatize_words(word))

tibble(before = token_2$word, after = token$word) %>% 
  filter(before != after) %>% 
  count(before, after, sort = TRUE)
```

## Prepped Data

Now that the data is prepped, created a stylized review text.

```{r}
prepped_hotel <-
  token %>%
  summarize(
    .by = review_id, 
    prepped_review = paste(word, collapse = " "),
    prepped_wrdcnt = n()
  ) %>%
  inner_join(hotel_2, by = join_by(review_id)) %>%
  relocate(prepped_review, prepped_wrdcnt, .after = last_col())

prepped_hotel %>% 
  select(review_id, review, prepped_review) %>% 
  filter(review_id == 3) %>%
  flextable::flextable() %>% 
  flextable::valign(valign = "top") %>% 
  flextable::autofit()
```

## Bigrams

If you intend to present bigrams, don't simply tokenize the raw or prepped text into bigrams because you don't want stop words in bigram, nor do you want words that aren't actually adjacent because you've removed stop words. Instead, tokenize into bigrams, split the bigrams into words, and filter out rows where one or both words is stop word.

```{r}
# Reassemble token_1 into text and re-tokenize so you get the spalling corrections.
bigram_0 <-
  token_1 %>%
  summarize(.by = review_id, reconstructed = paste(word, collapse = " ")) %>%
  unnest_tokens("bigram", reconstructed, token = "ngrams", n = 2)

# Remove bigrams where one or both words are stop words.
bigram <- 
  bigram_0 %>%
  separate(bigram, into = c("word1", "word2"), sep = " ") %>%
  anti_join(stop, by = join_by(word1 == word)) %>%
  anti_join(stop, by = join_by(word2 == word)) %>%
  mutate(bigram = paste(word1, word2)) %>%
  select(review_id, bigram)

# Example
bind_cols(
  hotel_2 %>% filter(review_id == 3) %>% select(review),
  bigram %>% filter(review_id == 3) %>% 
    summarize(bigrams = paste(bigram, collapse = "\n"))
) %>%
  flextable::flextable() %>%
  flextable::autofit() %>%
  flextable::width(j = 1, width = 4.5, unit = "in") %>%
  flextable::width(j = 2, width = 1.5, unit = "in") %>%
  flextable::valign(valign = "top")
```

## Save

Save the cleaned data for other analyses like topic modeling and sentiment analysis.

```{r}
save(prepped_hotel, token, bigram, file = "input/hotel_prepped.Rdata")
```
